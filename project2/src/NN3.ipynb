{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Machine Learning project CS-433: NMR spectroscopy supervised learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedules:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Week 10 (18-24 November): \n",
    " * Tests of various linear models/simple NN on a 10% subset of data\n",
    "* Week 11 (25-1 December):\n",
    " * Feature selection: being able to come with a good set of features\n",
    "* Week 12 (2-8 December):\n",
    " * Start of big scale analysis with Spark, implementation of the models which perform well at small scale\n",
    "* Week 13 (9-15 December):\n",
    " * Wrapping up\n",
    "* Week 14 (16-22 December): \n",
    " * 19th December: Deadline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Log Book](#log)\n",
    "2. [Pipeline](#pipeline)\n",
    "3. [Data Processing](#data_proc) <br>\n",
    "&emsp;3.1. [Data Vizualisation](#data_viz) <br>\n",
    "&emsp;3.2 [Outliers detection](#outliers) <br>\n",
    "  &emsp;&emsp;3.2.1 [DBSCAN](#dbscan) <br>\n",
    "  &emsp;&emsp;3.2.2 [Inter quantile range method](#iqr) <br>\n",
    "&emsp;3.3 [Scaling](#scaling) <br>\n",
    "&emsp;&emsp;3.3.1 [Min max scaling](#minmax) <br>\n",
    "&emsp;3.4 [Dimensionality reduction](#dim_red) <br>\n",
    "  &emsp;&emsp;3.4.1 [PCA](#pca) <br>\n",
    "&emsp;3.5 [Feature Selection](#feat_sel) <br>\n",
    "  &emsp;&emsp;3.5.1 [Relative importance from linear regression](#rel_imp_lin) <br>\n",
    "  &emsp;&emsp;3.5.2 [Random forest](#rand_for) <br>\n",
    "  &emsp;&emsp;3.5.3 [Univariate feature selection](#un_feat_sel) <br>\n",
    "  &emsp;&emsp;3.5.4 [Recursive feature selection](#rec_feat_sel) <br>\n",
    "  &emsp;&emsp;3.5.5 [Lasso Regression](#lasso) <br>\n",
    "  &emsp;&emsp;3.5.6 [Boruta](#boruta) <br>\n",
    "&emsp;3.6 [Models](#models) <br>\n",
    "  &emsp;&emsp;3.6.1 [Linear Models](#lin_mods) <br>\n",
    "  &emsp;&emsp;3.6.2 [Neural Networks](#NN) <br>\n",
    "4. [Main](#main) <br>\n",
    "   4.1 [ANN implementation](#ann_imp) <br>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import scipy.stats\n",
    "import sklearn.metrics\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm\n",
    "from itertools import combinations\n",
    "\n",
    "#from boruta import BorutaPy\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For neural net part\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "#from keras.callbacks import ModelCheckpoint\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Activation, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_3(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(Net_3, self).__init__()\n",
    "        self.fc1 = nn.Linear(n,100)\n",
    "        self.fc2 = nn.Linear(100,1)\n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN3():\n",
    "    def __init__(self,mod_1,mod_2,mod_3,mini_batch_size = 10 ,\n",
    "                 apply_iqr = True,apply_scaler = False,apply_pca = False,\n",
    "                 assemble_y = 'custom',nb_epochs = 150,normalize = False):\n",
    "        self.mod1 = mod_1\n",
    "        self.mod2 = mod_2\n",
    "        self.mod3 = mod_3\n",
    "        self.minbatchsize = mini_batch_size\n",
    "        self.assemble_y = assemble_y\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.apply_iqr = apply_iqr\n",
    "        self.apply_pca = apply_pca\n",
    "        self.apply_scaler = apply_scaler\n",
    "        \n",
    "    def train_model(self,model, train_input, train_target, monitor_loss=False):\n",
    "        criterion = nn.MSELoss() #regression task\n",
    "        optimizer = optim.Adam(model.parameters(), lr = 1e-4) #1e-4 normalement\n",
    "\n",
    "        # Monitor loss\n",
    "        losses = []\n",
    "\n",
    "        for e in range(self.nb_epochs):\n",
    "            sum_loss = 0\n",
    "            N = train_input.size(0)\n",
    "            for b in range(0, N, self.minbatchsize):\n",
    "                output = model(train_input.narrow(0, b, min(self.minbatchsize,N - b)))\n",
    "                loss = criterion(output, train_target.narrow(0, b, min(self.minbatchsize,N - b)))\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                sum_loss += loss.item() #compute loss for each mini batch for 1 epoch\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "            # Monitor loss\n",
    "            losses.append(sum_loss)\n",
    "\n",
    "            print('[epoch {:d}] loss: {:0.2f}'.format(e+1, sum_loss))\n",
    "\n",
    "        if monitor_loss:\n",
    "            return losses\n",
    "        \n",
    "    def IQR_y_outliers(self,X1,X2,X3,y_data):\n",
    "        ''' aims at removing all rows whose label (i.e. shielding) is considered as outlier.\n",
    "        output:\n",
    "         - X_filtered\n",
    "         - y_filtered\n",
    "        '''\n",
    "        q1, q3 = np.percentile(y_data, [25, 75])\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - (iqr * 1.5)\n",
    "        upper_bound = q3 + (iqr * 1.5)\n",
    "\n",
    "        assert(q1 != q3)\n",
    "\n",
    "        idx = np.where((y_data > lower_bound) & (y_data < upper_bound))\n",
    "        X1, X2, X3 = X1[idx], X2[idx], X3[idx]\n",
    "        y_data = y_data[idx]\n",
    "\n",
    "        assert(X1.shape[0] == y_data.shape[0] and X2.shape[0] == y_data.shape[0] and X3.shape[0] == y_data.shape[0])\n",
    "        return X1, X2, X3, y_data\n",
    "    \n",
    "    def fit(self, X1,X2,X3,y):\n",
    "        if self.apply_iqr:\n",
    "            X1,X2,X3,y = self.IQR_y_outliers(X1,X2,X3,y)\n",
    "        if self.normalize:\n",
    "            self.trans1 = Normalizer().fit(X1)\n",
    "            self.trans2 = Normalizer().fit(X2)\n",
    "            self.tran3 = Normalizer.fit(X3)\n",
    "        X1 = torch.Tensor(X1)\n",
    "        X2 = torch.Tensor(X2)\n",
    "        X3 = torch.Tensor(X3)\n",
    "        y = torch.Tensor(y.reshape(len(y), 1))\n",
    "        print('#' * 30 + 'Training model 1'+ '#' * 30)\n",
    "        loss1 = self.train_model(self.mod1, X1, y, monitor_loss=True)\n",
    "        print('#' * 30 + 'Training model 2'+ '#' * 30)\n",
    "        loss2 = self.train_model(self.mod2, X2, y, monitor_loss=True)\n",
    "        print('#' * 30 + 'Training model 3'+ '#' * 30)\n",
    "        loss3 = self.train_model(self.mod3, X3, y, monitor_loss=True)\n",
    "        print('#' * 30 + 'TRAINING TERMINATED'+ '#' * 30)\n",
    "        \n",
    "    \n",
    "    def droledemean(self,xs):\n",
    "        xs = list(xs)\n",
    "        invs = np.array([[1/np.abs(x -y) for y in xs if y is not x] for x in xs])\n",
    "        tot = np.sum(invs)\n",
    "        weights = np.sum(invs,axis = 1)\n",
    "        return np.sum(weights * xs)/tot\n",
    "        \n",
    "    def assemble_ys(self,y1,y2,y3):\n",
    "        if self.assemble_y == 'mean':\n",
    "            return np.mean([y1,y2,y3],axis = 0)\n",
    "        k = np.array([self.droledemean(i) for i in np.array([y1.reshape(y1.shape[0]),\n",
    "                                                             y2.reshape(y2.shape[0]),\n",
    "                                                             y3.reshape(y3.shape[0])]).T])\n",
    "        return k\n",
    "    \n",
    "    def predict_indep(self,X1,X2,X3):\n",
    "        if self.normalize:\n",
    "            X1 = self.trans1.transform(X1)\n",
    "            X2 = self.trans2.transform(X2)\n",
    "            X3 = self.trans3.transform(X3)\n",
    "        X1 = torch.Tensor(X1)\n",
    "        X2 = torch.Tensor(X2)\n",
    "        X3 = torch.Tensor(X3)\n",
    "        y1_hat = self.mod1(X1).detach().numpy()\n",
    "        y2_hat = self.mod2(X2).detach().numpy()\n",
    "        y3_hat = self.mod3(X3).detach().numpy()\n",
    "        return y1_hat,y2_hat,y3_hat\n",
    "    \n",
    "    def set_mean(self,meth):\n",
    "        assert(meth == 'mean' or meth == 'custom')\n",
    "        self.assemble_y = meth\n",
    "        \n",
    "    def predict(self,X1,X2,X3):\n",
    "        y1_hat,y2_hat,y3_hat = self.predict_indep(X1,X2,X3)\n",
    "        return self.assemble_ys(y1_hat,y2_hat,y3_hat)      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each cell here is meant to do a whole pipeline, from loading a certain number of samples, preprocessing etc. We keep using the R2 score, the MSE and the MAE as our metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(y_actual, y_pred,verbose = False):\n",
    "    mse = mean_squared_error(y_actual, y_pred)\n",
    "    mae = mean_absolute_error(y_actual, y_pred)\n",
    "    if verbose:\n",
    "        print(\"Obtained MSE on test set %2.2f \" % mse)\n",
    "        print(\"Obtained MAE on test set %2.2f \" % mae)\n",
    "    return {'mse':mse,'mae':mae}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFold3(model3,data_X1,data_X2,data_X3,data_y,n_splits = 4,verbose = True):\n",
    "    \"\"\"\n",
    "    perform Kfold cross validation on an NN3 object\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, random_state=14, shuffle=True)\n",
    "    scores_mean = {'mse':[],'mae':[]}\n",
    "    scores_custom = {'mse':[],'mae':[]}\n",
    "    for kindx,(train_index, test_index) in enumerate(kf.split(data_y)):\n",
    "        \n",
    "        print('%i / %i fold' % (kindx+1,n_splits))\n",
    "        X1_train, X1_test = data_X1[train_index],data_X1[test_index]\n",
    "        X2_train, X2_test = data_X2[train_index],data_X2[test_index]\n",
    "        X3_train, X3_test = data_X3[train_index],data_X3[test_index]\n",
    "        y_train, y_test = data_y[train_index], data_y[test_index]\n",
    "        model3.fit(X1_train,X2_train,X3_train,y_train)\n",
    "        \n",
    "        model3.set_mean('mean')\n",
    "        y_hat = model3.predict(X1_test,X2_test,X3_test)\n",
    "        score_mean = compute_score(y_test,y_hat)\n",
    "        scores_mean['mse'].append(score_mean['mse'])\n",
    "        scores_mean['mae'].append(score_mean['mae'])\n",
    "        \n",
    "        model3.set_mean('custom')\n",
    "        y_hat = model3.predict(X1_test,X2_test,X3_test)\n",
    "        score_custom = compute_score(y_test,y_hat)\n",
    "        scores_custom['mse'].append(score_custom['mse'])\n",
    "        scores_custom['mae'].append(score_custom['mae'])\n",
    "        \n",
    "        if verbose:\n",
    "            print('Mean method:{}'.format(score_mean))\n",
    "            print('Custom method:{}'.format(score_custom))\n",
    "    return scores_mean,scores_custom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of samples we take from the datasets\n",
    "n_samples = 15000\n",
    "tot_data_y = np.load('data/CSD-10k_H_chemical_shieldings.npy',mmap_mode='r')\n",
    "mask = np.random.permutation(tot_data_y.shape[0])[:n_samples]\n",
    "data_y = tot_data_y[mask]\n",
    "tot_data_X = np.load('data/CSD-10k_H_fps_1k_MD_n_12_l_9_rc_3.0_gw_0.3_rsr_1.0_rss_2.5_rse_5.npy',mmap_mode='r')\n",
    "data_X1 = tot_data_X[mask]\n",
    "tot_data_X = np.load('data/CSD-10k_H_fps_1k_MD_n_12_l_9_rc_5.0_gw_0.3_rsr_1.0_rss_2.5_rse_5.npy',mmap_mode='r')\n",
    "data_X2 = tot_data_X[mask]\n",
    "tot_data_X = np.load('data/CSD-10k_H_fps_1k_MD_n_12_l_9_rc_7.0_gw_0.3_rsr_1.0_rss_2.5_rse_5.npy',mmap_mode='r')\n",
    "data_X3 = tot_data_X[mask]\n",
    "\n",
    "#instanciation of the bigbibo\n",
    "bibo1 = Net_3(14400)\n",
    "bibo2 = Net_3(14400)\n",
    "bibo3 = Net_3(14400)\n",
    "bigbibo  = NN3(bibo1,bibo2,bibo3,nb_epochs=150,assemble_y='custom')\n",
    "\n",
    "scores_mean,scores_custom = KFold3(bigbibo,data_X1,data_X2,data_X3,data_y,n_splits=5)\n",
    "\n",
    "scores = {'mean_method':scores_mean,'custom_method':scores_custom}\n",
    "\n",
    "with open('kfoldresults.pickle','wb') as f:\n",
    "    pickle.dump(scores,f)\n",
    "\n",
    "print(np.mean(scores_mean['mse']))\n",
    "print(np.mean(scores_custom['mse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single training no kfold\n",
    "def single_test(bibgbibo,data_X1,data_X2,data_X3,data_y):\n",
    "    train_size = 200\n",
    "    mask = np.full(data_y.shape[0], False)\n",
    "    mask[:train_size] = True\n",
    "    np.random.shuffle(mask)\n",
    "\n",
    "    y_train = data_y[mask]\n",
    "    y_test = data_y[~mask]\n",
    "    X1_train = data_X1[mask]\n",
    "    X1_test = data_X1[~mask]\n",
    "    X2_train = data_X2[mask]\n",
    "    X2_test = data_X2[~mask]\n",
    "    X3_train = data_X3[mask]\n",
    "    X3_test = data_X3[~mask]\n",
    "\n",
    "    bigbibo.fit(X1_train,X2_train,X3_train,y_train)\n",
    "    \n",
    "    bigbibo.set_mean('mean')\n",
    "    y_hat = bigbibo.predict(X1_test,X2_test,X3_test)\n",
    "    score_mean = compute_score(y_test,y_hat)\n",
    "    \n",
    "    bigbibo.set_mean('custom')\n",
    "    y_hat = bigbibo.predict(X1_test,X2_test,X3_test)\n",
    "    score_custom = compute_score(y_test,y_hat)\n",
    "    \n",
    "    return score_mean,score_custom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################Training model 1##############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] loss: 12693.66\n",
      "[epoch 2] loss: 12616.09\n",
      "[epoch 3] loss: 12528.58\n",
      "##############################Training model 2##############################\n",
      "[epoch 1] loss: 12666.94\n",
      "[epoch 2] loss: 12610.31\n",
      "[epoch 3] loss: 12545.70\n",
      "##############################Training model 3##############################\n",
      "[epoch 1] loss: 12745.96\n",
      "[epoch 2] loss: 12696.40\n",
      "[epoch 3] loss: 12639.43\n",
      "##############################TRAINING TERMINATED##############################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'mse': 645.6513731255138, 'mae': 25.16708791091442},\n",
       " {'mse': 644.775093078498, 'mae': 25.149303449651683})"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_test(bigbibo,data_X1,data_X2,data_X3,data_y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
