{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Machine Learning project CS-433: NMR spectroscopy supervised learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedules:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Week 10 (18-24 November): \n",
    " * Tests of various linear models/simple NN on a 10% subset of data\n",
    "* Week 11 (25-1 December):\n",
    " * Feature selection: being able to come with a good set of features\n",
    "* Week 12 (2-8 December):\n",
    " * Start of big scale analysis with Spark, implementation of the models which perform well at small scale\n",
    "* Week 13 (9-15 December):\n",
    " * Wrapping up\n",
    "* Week 14 (16-22 December): \n",
    " * 19th December: Deadline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Log Book](#log)\n",
    "2. [Pipeline](#pipeline)\n",
    "3. [Data Processing](#data_proc) <br>\n",
    "&emsp;3.1. [Data Vizualisation](#data_viz) <br>\n",
    "&emsp;3.2 [Outliers detection](#outliers) <br>\n",
    "  &emsp;&emsp;3.2.1 [DBSCAN](#dbscan) <br>\n",
    "  &emsp;&emsp;3.2.2 [Inter quantile range method](#iqr) <br>\n",
    "&emsp;3.3 [Scaling](#scaling) <br>\n",
    "&emsp;&emsp;3.3.1 [Min max scaling](#minmax) <br>\n",
    "&emsp;3.4 [Dimensionality reduction](#dim_red) <br>\n",
    "  &emsp;&emsp;3.4.1 [PCA](#pca) <br>\n",
    "&emsp;3.5 [Feature Selection](#feat_sel) <br>\n",
    "  &emsp;&emsp;3.5.1 [Relative importance from linear regression](#rel_imp_lin) <br>\n",
    "  &emsp;&emsp;3.5.2 [Random forest](#rand_for) <br>\n",
    "  &emsp;&emsp;3.5.3 [Univariate feature selection](#un_feat_sel) <br>\n",
    "  &emsp;&emsp;3.5.4 [Recursive feature selection](#rec_feat_sel) <br>\n",
    "  &emsp;&emsp;3.5.5 [Lasso Regression](#lasso) <br>\n",
    "  &emsp;&emsp;3.5.6 [Boruta](#boruta) <br>\n",
    "&emsp;3.6 [Models](#models) <br>\n",
    "  &emsp;&emsp;3.6.1 [Linear Models](#lin_mods) <br>\n",
    "  &emsp;&emsp;3.6.2 [Neural Networks](#NN) <br>\n",
    "4. [Main](#main) <br>\n",
    "   4.1 [ANN implementation](#ann_imp) <br>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scipy.stats\n",
    "import sklearn.metrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest,f_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso,ElasticNet\n",
    "\n",
    "# For neural net part\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(sys.path[0],'src'))\n",
    "import helpers as hl\n",
    "import data_viz as dv\n",
    "import outliers as out\n",
    "import lin_mods as lm\n",
    "import neural_nets as nrn\n",
    "import NN3 as nn3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import data\n",
    "<a id='import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data'\n",
    "\n",
    "files = os.listdir(data_folder)  \n",
    "X_files = [filename for filename in files if (filename.endswith('.npy') and ('rsr' in filename))]\n",
    "y_files = [filename for filename in files if (filename.endswith('.npy') and ('chemical_shielding' in filename))]\n",
    "\n",
    "\n",
    "tot_data_X = np.load(data_folder + '/' + X_files[1], mmap_mode='r')\n",
    "tot_data_Y = np.load(data_folder + '/' + y_files[0], mmap_mode='r')\n",
    "data_X,data_y = hl.load_data(100,tot_data_X,tot_data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38514, 14400)\n",
      "(38514,)\n"
     ]
    }
   ],
   "source": [
    "print(tot_data_X.shape)\n",
    "print(tot_data_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Vizualisation\n",
    "<a id='data_viz'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_number = [12711, 1457, 4502]\n",
    "\n",
    "feature_0 = X_tot[:, feature_number[0]]\n",
    "feature_1 = X_tot[:, feature_number[1]]\n",
    "feature_2 = X_tot[:, feature_number[2]]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(14,5))\n",
    "\n",
    "# Divide the figure into a 2x1 grid, and give me the first section\n",
    "ax_1 = fig.add_subplot(131)\n",
    "\n",
    "# Divide the figure into a 2x1 grid, and give me the second section\n",
    "ax_2 = fig.add_subplot(132)\n",
    "ax_3 = fig.add_subplot(133)\n",
    "\n",
    "\n",
    "sns.distplot(feature_0, ax=ax_1, kde=False, hist_kws=dict(edgecolor=\"w\", linewidth=1))\n",
    "\n",
    "ax_1.axes.set_title(\"Feature {}\".format(str(feature_number[0]), fontsize=20))\n",
    "ax_1.set_xlabel(\"\", fontsize=17)\n",
    "ax_1.set_ylabel(\"Count\",fontsize=17)\n",
    "\n",
    "sns.distplot(feature_1, ax=ax_2, kde=False, hist_kws=dict(edgecolor=\"w\", linewidth=1))\n",
    "\n",
    "ax_2.axes.set_title(\"Feature {}\".format(str(feature_number[1]), fontsize=20))\n",
    "ax_2.set_xlabel(\"Feature value\", fontsize=17)\n",
    "ax_2.set_ylabel(\"\", fontsize=17)\n",
    "                    \n",
    "sns.distplot(feature_2, ax=ax_3, kde=False, hist_kws=dict(edgecolor=\"w\", linewidth=1))\n",
    "\n",
    "ax_3.axes.set_title(\"Feature {}\".format(str(feature_number[2]), fontsize=20))\n",
    "ax_3.set_xlabel(\"\", fontsize=17)\n",
    "ax_3.set_ylabel(\"\", fontsize=17)\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x792 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dv.plot_distribs(pd.DataFrame(data_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the different features are scaled pretty differently, we might want to scale them beforehand. Since they don' look like following a gaussian, we'll apply min/max scaling: but in order to do so, we first need to get rid of the outliers thanks to one of the following methods\n",
    "* Zscore: not adapted as our data might not be gaussian\n",
    "* DBScan:\n",
    "* Isolation Forest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing\n",
    "<a id='preprocess'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### 2.1 Outliers detection\n",
    "<a id='outliers'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 DBSCAN\n",
    "<a id = 'dbscan'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: computationally too demanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Interquartile range method (IQR)\n",
    "<a id = 'iqr'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consists in considering as outliers all data points that lie in >1.5 interquartile range from the quartiles:\n",
    "\n",
    "* Method 1: removing the samples which are considered as outliers in their label\n",
    "* Method 2: removing the samples which are considered as outliers for any of the x features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X,data_y = hl.load_data(100,tot_data_X,tot_data_Y)\n",
    "lin = LinearRegression()\n",
    "SCORING = ['neg_mean_squared_error', 'neg_mean_absolute_error','r2']\n",
    "\n",
    "#TODO:: make an accurate KFold\n",
    "\n",
    "#method 1\n",
    "out_remover = out.IQR_outlier()\n",
    "out_remover.fit(data_X)\n",
    "data_X1,data_y1 = out_remover.transform(data_X,data_y)\n",
    "scores1 = cross_validate(lin, data_X1, data_y1, cv=10,scoring = SCORING)\n",
    "\n",
    "#method 2\n",
    "data_X2,data_y2 = out.IQR_y_outliers(data_X,data_y)\n",
    "scores2 = cross_validate(lin,data_X,data_y,cv = 10,scoring = SCORING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the 2 methods on a classic linear regression task in order to assess which one performs better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_box_plot(scores1,scores2,metric):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    scores_df = pd.DataFrame({'meth1':scores1[metric],'meth2':scores2[metric]})\n",
    "    sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(scores_df))\n",
    "plot_box_plot(scores1,scores2,'test_neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that removing data with erronate labels is more efficient in terms of MSE: we'll prefer this method over the second one over the project. One still needs to assess its efficiency at a larger scale though, which is done in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO GIANNI: do a function, insert the pickled plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I should do a function but j'ai la flemme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 287us/step - loss: 20.0163 - val_loss: 7.9898\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 2.1726 - val_loss: 1.1877\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 1.0646 - val_loss: 0.9910\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.9258 - val_loss: 0.8876\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.8713 - val_loss: 0.8659\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.8536 - val_loss: 0.8499\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.8411 - val_loss: 0.8589\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.8331 - val_loss: 0.8565\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.8326 - val_loss: 0.8517\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.8289 - val_loss: 0.8619\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.8315 - val_loss: 0.8580\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.8284 - val_loss: 0.8839\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.8276 - val_loss: 0.8578\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 185us/step - loss: 0.8262 - val_loss: 0.8565\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 187us/step - loss: 0.8234 - val_loss: 0.8550\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 185us/step - loss: 0.8218 - val_loss: 0.8637\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.8203 - val_loss: 0.8654\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 195us/step - loss: 0.8249 - val_loss: 0.8537\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 181us/step - loss: 0.8196 - val_loss: 0.8522\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.8175 - val_loss: 0.8494\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 181us/step - loss: 0.8189 - val_loss: 0.8510\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.8130 - val_loss: 0.8459\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.8115 - val_loss: 0.8501\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.8087 - val_loss: 0.8495\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.8069 - val_loss: 0.8427\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.8080 - val_loss: 0.8414\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.8061 - val_loss: 0.8522\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.8016 - val_loss: 0.8409\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7983 - val_loss: 0.8705\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7991 - val_loss: 0.8329\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7937 - val_loss: 0.8308\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7925 - val_loss: 0.8290\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7902 - val_loss: 0.8265\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7882 - val_loss: 0.8243\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7854 - val_loss: 0.8280\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7852 - val_loss: 0.8237\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7822 - val_loss: 0.8217\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7798 - val_loss: 0.8380\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7824 - val_loss: 0.8184\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7768 - val_loss: 0.8152\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7756 - val_loss: 0.8149\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7689 - val_loss: 0.8147\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7663 - val_loss: 0.8097\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7664 - val_loss: 0.8070\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7636 - val_loss: 0.8069\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7603 - val_loss: 0.8057\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7607 - val_loss: 0.8039\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 185us/step - loss: 0.7592 - val_loss: 0.7992\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 194us/step - loss: 0.7594 - val_loss: 0.7998\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7536 - val_loss: 0.8180\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7510 - val_loss: 0.7949\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7557 - val_loss: 0.7951\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7491 - val_loss: 0.8020\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 199us/step - loss: 0.7492 - val_loss: 0.7872\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7476 - val_loss: 0.8102\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7464 - val_loss: 0.7925\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7436 - val_loss: 0.7915\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7381 - val_loss: 0.7885\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7400 - val_loss: 0.7872\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7419 - val_loss: 0.7938\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7356 - val_loss: 0.7853\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7365 - val_loss: 0.7846\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 179us/step - loss: 0.7343 - val_loss: 0.7871\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7346 - val_loss: 0.7882\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7333 - val_loss: 0.7803\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7310 - val_loss: 0.7772\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7293 - val_loss: 0.7733\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7275 - val_loss: 0.7920\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 187us/step - loss: 0.7246 - val_loss: 0.7810\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.7224 - val_loss: 0.7719\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7225 - val_loss: 0.7775\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.7231 - val_loss: 0.7733\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 181us/step - loss: 0.7203 - val_loss: 0.8159\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 200us/step - loss: 0.7181 - val_loss: 0.7725\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 193us/step - loss: 0.7240 - val_loss: 0.7762\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 194us/step - loss: 0.7181 - val_loss: 0.7717\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 242us/step - loss: 0.7193 - val_loss: 0.7742\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7169 - val_loss: 0.7762\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7129 - val_loss: 0.7686\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.7127 - val_loss: 0.7704\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.7105 - val_loss: 0.7700\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7133 - val_loss: 0.7780\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 179us/step - loss: 0.7084 - val_loss: 0.7688\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7110 - val_loss: 0.7686\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7097 - val_loss: 0.7681\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7073 - val_loss: 0.7755\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7085 - val_loss: 0.7650\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7098 - val_loss: 0.7698\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7076 - val_loss: 0.7663\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7056 - val_loss: 0.7771\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7043 - val_loss: 0.7711\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7024 - val_loss: 0.7642\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7011 - val_loss: 0.7725\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7073 - val_loss: 0.7868\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6984 - val_loss: 0.7672\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6978 - val_loss: 0.7613\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7003 - val_loss: 0.7802\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 181us/step - loss: 0.6998 - val_loss: 0.7655\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6970 - val_loss: 0.7676\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6975 - val_loss: 0.7742\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6971 - val_loss: 0.7676\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 290us/step - loss: 0.6976 - val_loss: 0.7651\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6936 - val_loss: 0.7629\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 181us/step - loss: 0.6944 - val_loss: 0.7562\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6928 - val_loss: 0.7575\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6892 - val_loss: 0.7613\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6892 - val_loss: 0.7596\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6894 - val_loss: 0.7568\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6911 - val_loss: 0.7557\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6886 - val_loss: 0.7734\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6874 - val_loss: 0.7612\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6899 - val_loss: 0.7583\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6887 - val_loss: 0.7786\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6887 - val_loss: 0.7578\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6875 - val_loss: 0.7518\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6893 - val_loss: 0.7562\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6857 - val_loss: 0.7545\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6835 - val_loss: 0.7568\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6860 - val_loss: 0.7538\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6793 - val_loss: 0.7582\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6794 - val_loss: 0.7717\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6840 - val_loss: 0.7573\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6808 - val_loss: 0.7567\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6827 - val_loss: 0.7575\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6801 - val_loss: 0.7563\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6802 - val_loss: 0.7571\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6777 - val_loss: 0.7591\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6772 - val_loss: 0.7639\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6757 - val_loss: 0.7538\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6734 - val_loss: 0.7735\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6756 - val_loss: 0.7574\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6735 - val_loss: 0.7533\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6786 - val_loss: 0.7531\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6747 - val_loss: 0.7676\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6720 - val_loss: 0.7564\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6738 - val_loss: 0.7556\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6726 - val_loss: 0.7531\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6784 - val_loss: 0.7657\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6740 - val_loss: 0.7525\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6715 - val_loss: 0.7631\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6683 - val_loss: 0.7489\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6728 - val_loss: 0.7548\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6671 - val_loss: 0.7529\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6688 - val_loss: 0.7497\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6678 - val_loss: 0.7816\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6677 - val_loss: 0.7643\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6688 - val_loss: 0.7523\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6664 - val_loss: 0.7489\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6676 - val_loss: 0.7442\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6698 - val_loss: 0.7546\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 283us/step - loss: 19.3701 - val_loss: 6.9150\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 1.9228 - val_loss: 1.1749\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.9935 - val_loss: 0.9364\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.8667 - val_loss: 0.8616\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.8181 - val_loss: 0.8320\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.8010 - val_loss: 0.8137\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7988 - val_loss: 0.8149\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7931 - val_loss: 0.7984\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7862 - val_loss: 0.7959\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7832 - val_loss: 0.7950\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7842 - val_loss: 0.7864\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7836 - val_loss: 0.8022\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7811 - val_loss: 0.7964\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7806 - val_loss: 0.7806\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7789 - val_loss: 0.7863\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7792 - val_loss: 0.7783\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7791 - val_loss: 0.7799\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7734 - val_loss: 0.7794\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7730 - val_loss: 0.7960\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7726 - val_loss: 0.7797\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7689 - val_loss: 0.7761\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7668 - val_loss: 0.7899\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7674 - val_loss: 0.7779\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7643 - val_loss: 0.7777\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7626 - val_loss: 0.7904\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7618 - val_loss: 0.7931\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7587 - val_loss: 0.7688\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7585 - val_loss: 0.7702\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7534 - val_loss: 0.7835\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7512 - val_loss: 0.7709\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7492 - val_loss: 0.7689\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7480 - val_loss: 0.7678\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7433 - val_loss: 0.7657\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7455 - val_loss: 0.7666\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7446 - val_loss: 0.7634\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7410 - val_loss: 0.7612\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7428 - val_loss: 0.7678\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7379 - val_loss: 0.7668\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7348 - val_loss: 0.7871\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7332 - val_loss: 0.7616\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7339 - val_loss: 0.7652\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7306 - val_loss: 0.7578\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7289 - val_loss: 0.7963\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7244 - val_loss: 0.7657\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7252 - val_loss: 0.7548\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7216 - val_loss: 0.7577\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7184 - val_loss: 0.7582\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7198 - val_loss: 0.7561\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7203 - val_loss: 0.7525\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7170 - val_loss: 0.7527\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7104 - val_loss: 0.7541\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7128 - val_loss: 0.7531\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7093 - val_loss: 0.7517\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7079 - val_loss: 0.7461\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7096 - val_loss: 0.7571\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7073 - val_loss: 0.7503\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7075 - val_loss: 0.7561\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7059 - val_loss: 0.7535\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7004 - val_loss: 0.7467\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7027 - val_loss: 0.7602\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6979 - val_loss: 0.7496\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6977 - val_loss: 0.7556\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7021 - val_loss: 0.7689\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6972 - val_loss: 0.7480\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6977 - val_loss: 0.7519\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6946 - val_loss: 0.7658\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6931 - val_loss: 0.7476\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6903 - val_loss: 0.7632\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6906 - val_loss: 0.7381\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6894 - val_loss: 0.7357\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6864 - val_loss: 0.7491\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6889 - val_loss: 0.7496\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6861 - val_loss: 0.7377\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6919 - val_loss: 0.7333\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6831 - val_loss: 0.7348\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6826 - val_loss: 0.7397\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6812 - val_loss: 0.7363\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6822 - val_loss: 0.7403\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6843 - val_loss: 0.7367\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6813 - val_loss: 0.7324\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6813 - val_loss: 0.7370\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6753 - val_loss: 0.7468\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6811 - val_loss: 0.7401\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6757 - val_loss: 0.7393\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6739 - val_loss: 0.7640\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6788 - val_loss: 0.7538\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6697 - val_loss: 0.7385\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6766 - val_loss: 0.7312\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6704 - val_loss: 0.7349\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6713 - val_loss: 0.7531\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6700 - val_loss: 0.7435\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6670 - val_loss: 0.7422\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6674 - val_loss: 0.7719\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6683 - val_loss: 0.7400\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6687 - val_loss: 0.7332\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6666 - val_loss: 0.7624\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6684 - val_loss: 0.7370\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6634 - val_loss: 0.7358\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6681 - val_loss: 0.7452\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6612 - val_loss: 0.7359\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6605 - val_loss: 0.7346\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6655 - val_loss: 0.7470\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6599 - val_loss: 0.7355\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6605 - val_loss: 0.7521\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6580 - val_loss: 0.7387\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6574 - val_loss: 0.7390\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6629 - val_loss: 0.7389\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6606 - val_loss: 0.7344\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6557 - val_loss: 0.7405\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6573 - val_loss: 0.7315\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6563 - val_loss: 0.7335\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6559 - val_loss: 0.7377\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6526 - val_loss: 0.7294\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6535 - val_loss: 0.7329\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6564 - val_loss: 0.7298\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6555 - val_loss: 0.7542\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6526 - val_loss: 0.7366\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6541 - val_loss: 0.7349\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6524 - val_loss: 0.7389\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6499 - val_loss: 0.7287\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6484 - val_loss: 0.7331\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6497 - val_loss: 0.7356\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6508 - val_loss: 0.7487\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6539 - val_loss: 0.7456\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6466 - val_loss: 0.7594\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6475 - val_loss: 0.7361\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6466 - val_loss: 0.7350\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6441 - val_loss: 0.7354\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6461 - val_loss: 0.7383\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6482 - val_loss: 0.7433\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6460 - val_loss: 0.7444\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6480 - val_loss: 0.7378\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6457 - val_loss: 0.7406\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6427 - val_loss: 0.7324\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6451 - val_loss: 0.7337\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6481 - val_loss: 0.7448\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6438 - val_loss: 0.7329\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6444 - val_loss: 0.7388\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6419 - val_loss: 0.7507\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6449 - val_loss: 0.7522\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6423 - val_loss: 0.7406\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6435 - val_loss: 0.7725\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6462 - val_loss: 0.7420\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6400 - val_loss: 0.7397\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6386 - val_loss: 0.7389\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6383 - val_loss: 0.7509\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6427 - val_loss: 0.7373\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6416 - val_loss: 0.7449\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6401 - val_loss: 0.7384\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6386 - val_loss: 0.7363\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 290us/step - loss: 20.2210 - val_loss: 9.1806\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 2.4785 - val_loss: 1.1528\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 1.0547 - val_loss: 0.9443\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.9072 - val_loss: 0.8665\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 180us/step - loss: 0.8487 - val_loss: 0.8495\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.8220 - val_loss: 0.8276\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.8148 - val_loss: 0.8285\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.8117 - val_loss: 0.8282\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.8065 - val_loss: 0.8261\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.8019 - val_loss: 0.8238\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7996 - val_loss: 0.8220\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7975 - val_loss: 0.8246\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7955 - val_loss: 0.8202\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7975 - val_loss: 0.8359\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7929 - val_loss: 0.8301\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7901 - val_loss: 0.8166\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7858 - val_loss: 0.8155\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7877 - val_loss: 0.8204\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7839 - val_loss: 0.8121\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7780 - val_loss: 0.8101\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7748 - val_loss: 0.8232\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7725 - val_loss: 0.8072\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7703 - val_loss: 0.8250\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7667 - val_loss: 0.8027\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7615 - val_loss: 0.8040\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7602 - val_loss: 0.7934\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7594 - val_loss: 0.8174\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7560 - val_loss: 0.7924\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7555 - val_loss: 0.7986\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7558 - val_loss: 0.7949\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7444 - val_loss: 0.8059\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7443 - val_loss: 0.7863\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7411 - val_loss: 0.7848\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7390 - val_loss: 0.7812\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7337 - val_loss: 0.7884\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7325 - val_loss: 0.7770\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7321 - val_loss: 0.7774\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7307 - val_loss: 0.7795\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7267 - val_loss: 0.7668\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7228 - val_loss: 0.7720\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7244 - val_loss: 0.7679\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7210 - val_loss: 0.7720\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7177 - val_loss: 0.7640\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7162 - val_loss: 0.7632\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7114 - val_loss: 0.7588\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7155 - val_loss: 0.7695\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7082 - val_loss: 0.7577\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7086 - val_loss: 0.7664\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7050 - val_loss: 0.7569\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7050 - val_loss: 0.7607\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7030 - val_loss: 0.7590\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6985 - val_loss: 0.7541\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6992 - val_loss: 0.7589\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6965 - val_loss: 0.7535\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6922 - val_loss: 0.7460\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6962 - val_loss: 0.7491\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6928 - val_loss: 0.7526\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6892 - val_loss: 0.7462\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6869 - val_loss: 0.7505\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6853 - val_loss: 0.7482\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6864 - val_loss: 0.7465\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6850 - val_loss: 0.7468\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6823 - val_loss: 0.7595\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6819 - val_loss: 0.7518\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6808 - val_loss: 0.7420\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6806 - val_loss: 0.7501\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6782 - val_loss: 0.7453\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6755 - val_loss: 0.7387\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6761 - val_loss: 0.7399\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6726 - val_loss: 0.7448\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6786 - val_loss: 0.7358\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6762 - val_loss: 0.7419\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6718 - val_loss: 0.7456\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6711 - val_loss: 0.7362\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6684 - val_loss: 0.7365\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6664 - val_loss: 0.7437\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6706 - val_loss: 0.7336\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6663 - val_loss: 0.7382\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6681 - val_loss: 0.7376\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6675 - val_loss: 0.7319\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6649 - val_loss: 0.7348\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6594 - val_loss: 0.7371\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6641 - val_loss: 0.7414\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6584 - val_loss: 0.7391\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 179us/step - loss: 0.6601 - val_loss: 0.7325\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6602 - val_loss: 0.7377\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6635 - val_loss: 0.7399\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6587 - val_loss: 0.7476\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6613 - val_loss: 0.7373\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6582 - val_loss: 0.7438\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6544 - val_loss: 0.7347\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6557 - val_loss: 0.7385\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6565 - val_loss: 0.7328\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6534 - val_loss: 0.7372\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6532 - val_loss: 0.7439\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6505 - val_loss: 0.7422\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6503 - val_loss: 0.7404\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6533 - val_loss: 0.7618\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6498 - val_loss: 0.7329\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6469 - val_loss: 0.7348\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6542 - val_loss: 0.7329\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6481 - val_loss: 0.7501\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6509 - val_loss: 0.7549\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6482 - val_loss: 0.7363\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6473 - val_loss: 0.7325\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6469 - val_loss: 0.7330\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6524 - val_loss: 0.7426\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6499 - val_loss: 0.7315\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6523 - val_loss: 0.7431\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6457 - val_loss: 0.7388\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6418 - val_loss: 0.7424\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6421 - val_loss: 0.7320\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6423 - val_loss: 0.7372\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6420 - val_loss: 0.7280\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6401 - val_loss: 0.7297\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6416 - val_loss: 0.7311\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6430 - val_loss: 0.7275\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6399 - val_loss: 0.7363\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6435 - val_loss: 0.7315\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6400 - val_loss: 0.7333\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6388 - val_loss: 0.7464\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6388 - val_loss: 0.7255\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6404 - val_loss: 0.7520\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6395 - val_loss: 0.7275\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6367 - val_loss: 0.7422\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6378 - val_loss: 0.7290\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6383 - val_loss: 0.7362\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6374 - val_loss: 0.7368\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6375 - val_loss: 0.7298\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6441 - val_loss: 0.7461\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6340 - val_loss: 0.7364\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6344 - val_loss: 0.7414\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6360 - val_loss: 0.7353\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6335 - val_loss: 0.7321\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6334 - val_loss: 0.7292\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6329 - val_loss: 0.7259\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6308 - val_loss: 0.7310\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6338 - val_loss: 0.7237\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6321 - val_loss: 0.7295\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6302 - val_loss: 0.7306\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6309 - val_loss: 0.7300\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6311 - val_loss: 0.7269\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6309 - val_loss: 0.7255\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6285 - val_loss: 0.7309\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6287 - val_loss: 0.7238\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6298 - val_loss: 0.7674\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6303 - val_loss: 0.7312\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6297 - val_loss: 0.7328\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6263 - val_loss: 0.7454\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6312 - val_loss: 0.7286\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 286us/step - loss: 20.3486 - val_loss: 9.8560\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 2.6836 - val_loss: 1.2067\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 1.0500 - val_loss: 0.9389\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.8858 - val_loss: 0.8425\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.8228 - val_loss: 0.8246\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.8013 - val_loss: 0.8010\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7957 - val_loss: 0.7925\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7903 - val_loss: 0.7902\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7872 - val_loss: 0.8064\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7872 - val_loss: 0.8018\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7901 - val_loss: 0.8072\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7833 - val_loss: 0.8179\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7826 - val_loss: 0.8005\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7881 - val_loss: 0.7929\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7847 - val_loss: 0.7950\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7842 - val_loss: 0.7971\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7792 - val_loss: 0.7948\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7788 - val_loss: 0.7907\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7754 - val_loss: 0.8025\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7749 - val_loss: 0.7893\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7737 - val_loss: 0.7919\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7750 - val_loss: 0.7897\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7699 - val_loss: 0.7884\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7720 - val_loss: 0.7876\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7681 - val_loss: 0.7883\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7652 - val_loss: 0.7858\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7613 - val_loss: 0.7867\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7619 - val_loss: 0.7864\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7637 - val_loss: 0.7848\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7577 - val_loss: 0.7934\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7607 - val_loss: 0.7855\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7543 - val_loss: 0.8215\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7529 - val_loss: 0.7806\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7493 - val_loss: 0.7800\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7483 - val_loss: 0.7807\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7463 - val_loss: 0.7878\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7424 - val_loss: 0.8172\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7467 - val_loss: 0.7976\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7428 - val_loss: 0.7845\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7374 - val_loss: 0.7758\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7341 - val_loss: 0.7862\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7345 - val_loss: 0.7741\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7316 - val_loss: 0.7804\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7301 - val_loss: 0.7744\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7292 - val_loss: 0.7702\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7261 - val_loss: 0.8116\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7232 - val_loss: 0.7782\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7247 - val_loss: 0.7784\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7188 - val_loss: 0.7748\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7166 - val_loss: 0.7764\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7190 - val_loss: 0.7740\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7137 - val_loss: 0.7763\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7103 - val_loss: 0.7693\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7115 - val_loss: 0.7645\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7099 - val_loss: 0.7782\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7098 - val_loss: 0.7661\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7062 - val_loss: 0.7619\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7011 - val_loss: 0.7675\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.7047 - val_loss: 0.7675\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6991 - val_loss: 0.7703\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7008 - val_loss: 0.7613\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6967 - val_loss: 0.7649\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6955 - val_loss: 0.7660\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6950 - val_loss: 0.7613\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6928 - val_loss: 0.8051\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6922 - val_loss: 0.7669\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6898 - val_loss: 0.7643\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6888 - val_loss: 0.7585\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6848 - val_loss: 0.7485\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6854 - val_loss: 0.7492\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6854 - val_loss: 0.7515\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6870 - val_loss: 0.7557\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6823 - val_loss: 0.7467\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6792 - val_loss: 0.7526\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6828 - val_loss: 0.7533\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6761 - val_loss: 0.7513\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6822 - val_loss: 0.7530\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6746 - val_loss: 0.7554\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6720 - val_loss: 0.7558\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6710 - val_loss: 0.7500\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6710 - val_loss: 0.7423\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6680 - val_loss: 0.7568\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6709 - val_loss: 0.7465\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6685 - val_loss: 0.7454\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6674 - val_loss: 0.7492\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6626 - val_loss: 0.7409\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6611 - val_loss: 0.7461\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6618 - val_loss: 0.7399\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6639 - val_loss: 0.7523\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6592 - val_loss: 0.7510\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6586 - val_loss: 0.7436\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6584 - val_loss: 0.7364\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6558 - val_loss: 0.7494\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6575 - val_loss: 0.7411\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6591 - val_loss: 0.7397\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6530 - val_loss: 0.7420\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6511 - val_loss: 0.7498\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6517 - val_loss: 0.7383\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6498 - val_loss: 0.7428\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6502 - val_loss: 0.7454\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6501 - val_loss: 0.7500\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6468 - val_loss: 0.7421\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6484 - val_loss: 0.7352\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6438 - val_loss: 0.7512\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6478 - val_loss: 0.7488\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6450 - val_loss: 0.7486\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6449 - val_loss: 0.7486\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6445 - val_loss: 0.7398\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6414 - val_loss: 0.7368\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6422 - val_loss: 0.7372\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6442 - val_loss: 0.7493\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6401 - val_loss: 0.7413\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6380 - val_loss: 0.7479\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6384 - val_loss: 0.7510\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6350 - val_loss: 0.7637\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6363 - val_loss: 0.7369\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6346 - val_loss: 0.7415\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6314 - val_loss: 0.7510\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6346 - val_loss: 0.7412\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6328 - val_loss: 0.7350\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6326 - val_loss: 0.7474\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6308 - val_loss: 0.7497\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6326 - val_loss: 0.7336\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6295 - val_loss: 0.7435\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6289 - val_loss: 0.7551\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6260 - val_loss: 0.7859\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6290 - val_loss: 0.7391\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6285 - val_loss: 0.7375\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6296 - val_loss: 0.7440\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6239 - val_loss: 0.7508\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6288 - val_loss: 0.7415\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6249 - val_loss: 0.7420\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6284 - val_loss: 0.7324\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6246 - val_loss: 0.7499\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6241 - val_loss: 0.7399\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6229 - val_loss: 0.7347\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6196 - val_loss: 0.7424\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6200 - val_loss: 0.7394\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6204 - val_loss: 0.7378\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6183 - val_loss: 0.7461\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6202 - val_loss: 0.7425\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6219 - val_loss: 0.7550\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6201 - val_loss: 0.7394\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6175 - val_loss: 0.7700\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6156 - val_loss: 0.7534\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6134 - val_loss: 0.7562\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6128 - val_loss: 0.7374\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6185 - val_loss: 0.7423\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6161 - val_loss: 0.7542\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6104 - val_loss: 0.7563\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 201us/step - loss: 5.9219 - val_loss: 0.8550\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.8129 - val_loss: 0.8282\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.8026 - val_loss: 0.7981\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7992 - val_loss: 0.7967\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7927 - val_loss: 0.7876\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7854 - val_loss: 0.7774\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7760 - val_loss: 0.7798\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7674 - val_loss: 0.7789\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7575 - val_loss: 0.7588\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7486 - val_loss: 0.7674\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7421 - val_loss: 0.7458\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7358 - val_loss: 0.7518\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.7298 - val_loss: 0.7325\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.7228 - val_loss: 0.7308\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.7169 - val_loss: 0.7304\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7118 - val_loss: 0.7207\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7065 - val_loss: 0.7333\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7017 - val_loss: 0.7359\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6972 - val_loss: 0.7206\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6934 - val_loss: 0.7126\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6881 - val_loss: 0.7153\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6867 - val_loss: 0.7130\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6817 - val_loss: 0.7070\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6823 - val_loss: 0.7016\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6785 - val_loss: 0.7086\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6770 - val_loss: 0.7058\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6740 - val_loss: 0.7004\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6700 - val_loss: 0.6975\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6673 - val_loss: 0.6950\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6642 - val_loss: 0.7061\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6618 - val_loss: 0.6936\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6608 - val_loss: 0.7000\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6569 - val_loss: 0.7067\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6560 - val_loss: 0.6917\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6522 - val_loss: 0.6954\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6514 - val_loss: 0.6960\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6511 - val_loss: 0.6919\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6483 - val_loss: 0.6892\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6477 - val_loss: 0.6884\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6455 - val_loss: 0.6922\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6426 - val_loss: 0.6910\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6430 - val_loss: 0.7009\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6409 - val_loss: 0.6996\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6390 - val_loss: 0.6855\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6390 - val_loss: 0.6840\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6375 - val_loss: 0.6814\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6352 - val_loss: 0.6813\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6349 - val_loss: 0.6855\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6328 - val_loss: 0.6831\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6351 - val_loss: 0.6832\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6332 - val_loss: 0.6805\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6308 - val_loss: 0.6986\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6302 - val_loss: 0.6823\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6274 - val_loss: 0.6984\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6282 - val_loss: 0.6782\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6272 - val_loss: 0.6863\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6251 - val_loss: 0.6778\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6235 - val_loss: 0.6794\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6235 - val_loss: 0.6762\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6229 - val_loss: 0.6780\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6209 - val_loss: 0.6829\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6221 - val_loss: 0.6735\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6221 - val_loss: 0.6817\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6209 - val_loss: 0.6815\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6189 - val_loss: 0.6737\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6184 - val_loss: 0.6737\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6179 - val_loss: 0.6826\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6201 - val_loss: 0.6791\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6168 - val_loss: 0.6738\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6163 - val_loss: 0.6751\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6144 - val_loss: 0.6948\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6159 - val_loss: 0.6793\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6134 - val_loss: 0.6763\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6139 - val_loss: 0.6733\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6130 - val_loss: 0.6746\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6131 - val_loss: 0.6836\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6113 - val_loss: 0.6766\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6113 - val_loss: 0.6779\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6109 - val_loss: 0.6835\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6090 - val_loss: 0.6691\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6074 - val_loss: 0.6748\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6086 - val_loss: 0.6842\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6075 - val_loss: 0.6709\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6070 - val_loss: 0.6726\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6066 - val_loss: 0.6747\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6066 - val_loss: 0.6785\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6059 - val_loss: 0.6914\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6072 - val_loss: 0.6765\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6053 - val_loss: 0.6703\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6028 - val_loss: 0.6706\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6038 - val_loss: 0.6800\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6035 - val_loss: 0.6777\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6033 - val_loss: 0.6682\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6027 - val_loss: 0.6698\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6019 - val_loss: 0.6694\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6014 - val_loss: 0.6809\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6012 - val_loss: 0.6769\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6006 - val_loss: 0.6767\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6026 - val_loss: 0.6760\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6027 - val_loss: 0.6756\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5989 - val_loss: 0.6703\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6001 - val_loss: 0.6776\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5974 - val_loss: 0.6771\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5986 - val_loss: 0.6859\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5999 - val_loss: 0.6891\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5986 - val_loss: 0.6687\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5979 - val_loss: 0.6771\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5976 - val_loss: 0.6760\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5972 - val_loss: 0.6743\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5974 - val_loss: 0.6737\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5949 - val_loss: 0.6686\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5967 - val_loss: 0.6871\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5957 - val_loss: 0.6811\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5950 - val_loss: 0.6798\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5954 - val_loss: 0.6676\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5946 - val_loss: 0.6742\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5960 - val_loss: 0.6711\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5956 - val_loss: 0.6772\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5937 - val_loss: 0.6741\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 178us/step - loss: 0.5947 - val_loss: 0.6707\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5935 - val_loss: 0.6787\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5939 - val_loss: 0.6877\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5948 - val_loss: 0.6701\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5920 - val_loss: 0.6708\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5940 - val_loss: 0.6755\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5902 - val_loss: 0.6735\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5952 - val_loss: 0.6681\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5926 - val_loss: 0.6762\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5917 - val_loss: 0.6748\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5927 - val_loss: 0.6728\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5899 - val_loss: 0.6697\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5908 - val_loss: 0.6752\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5909 - val_loss: 0.6834\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5906 - val_loss: 0.6915\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5894 - val_loss: 0.6723\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5892 - val_loss: 0.6655\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5909 - val_loss: 0.6737\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5880 - val_loss: 0.6708\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5932 - val_loss: 0.6770\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5902 - val_loss: 0.6756\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5898 - val_loss: 0.6863\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5914 - val_loss: 0.6713\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5887 - val_loss: 0.6848\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5896 - val_loss: 0.6685\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5915 - val_loss: 0.6701\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5893 - val_loss: 0.6741\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5886 - val_loss: 0.6769\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5881 - val_loss: 0.6663\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5882 - val_loss: 0.6678\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5860 - val_loss: 0.6743\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 202us/step - loss: 5.9861 - val_loss: 0.8867\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.8206 - val_loss: 0.8384\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.8068 - val_loss: 0.8295\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.8025 - val_loss: 0.8309\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7986 - val_loss: 0.8306\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7914 - val_loss: 0.8223\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7881 - val_loss: 0.8121\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7849 - val_loss: 0.8287\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7807 - val_loss: 0.8014\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7740 - val_loss: 0.7997\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7689 - val_loss: 0.7937\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7626 - val_loss: 0.7955\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7616 - val_loss: 0.7920\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7563 - val_loss: 0.7803\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7533 - val_loss: 0.8222\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7502 - val_loss: 0.7857\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7495 - val_loss: 0.7736\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7443 - val_loss: 0.7834\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7409 - val_loss: 0.7804\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7404 - val_loss: 0.7718\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7341 - val_loss: 0.7647\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7335 - val_loss: 0.7639\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7315 - val_loss: 0.7590\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7289 - val_loss: 0.7623\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7280 - val_loss: 0.7585\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7263 - val_loss: 0.7561\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7230 - val_loss: 0.7811\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7199 - val_loss: 0.7757\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7195 - val_loss: 0.7525\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7193 - val_loss: 0.7553\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7158 - val_loss: 0.7715\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7138 - val_loss: 0.7641\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7114 - val_loss: 0.7571\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7117 - val_loss: 0.7517\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7099 - val_loss: 0.7559\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7075 - val_loss: 0.7495\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7066 - val_loss: 0.7446\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7059 - val_loss: 0.7450\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7043 - val_loss: 0.7424\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7010 - val_loss: 0.7696\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7001 - val_loss: 0.7518\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6968 - val_loss: 0.7395\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6942 - val_loss: 0.7370\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6934 - val_loss: 0.7373\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6913 - val_loss: 0.7330\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6896 - val_loss: 0.7417\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6876 - val_loss: 0.7355\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6871 - val_loss: 0.7416\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6843 - val_loss: 0.7329\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6839 - val_loss: 0.7931\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6827 - val_loss: 0.7298\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6810 - val_loss: 0.7275\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6819 - val_loss: 0.7303\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6789 - val_loss: 0.7385\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6780 - val_loss: 0.7306\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6763 - val_loss: 0.7376\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6759 - val_loss: 0.7366\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6741 - val_loss: 0.7238\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6722 - val_loss: 0.7290\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6726 - val_loss: 0.7187\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6723 - val_loss: 0.7240\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6684 - val_loss: 0.7258\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6708 - val_loss: 0.7253\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6703 - val_loss: 0.7244\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6666 - val_loss: 0.7242\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6679 - val_loss: 0.7248\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6660 - val_loss: 0.7191\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6666 - val_loss: 0.7209\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6673 - val_loss: 0.7221\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6654 - val_loss: 0.7251\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6671 - val_loss: 0.7130\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6648 - val_loss: 0.7188\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6648 - val_loss: 0.7132\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6624 - val_loss: 0.7237\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6642 - val_loss: 0.7246\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6629 - val_loss: 0.7197\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6627 - val_loss: 0.7099\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6615 - val_loss: 0.7102\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6612 - val_loss: 0.7228\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6618 - val_loss: 0.7088\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6604 - val_loss: 0.7185\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6615 - val_loss: 0.7207\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6589 - val_loss: 0.7175\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6593 - val_loss: 0.7143\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6596 - val_loss: 0.7318\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6573 - val_loss: 0.7176\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6586 - val_loss: 0.7105\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6590 - val_loss: 0.7257\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6570 - val_loss: 0.7220\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6573 - val_loss: 0.7118\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6580 - val_loss: 0.7078\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6547 - val_loss: 0.7158\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6559 - val_loss: 0.7123\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6552 - val_loss: 0.7144\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6548 - val_loss: 0.7034\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6545 - val_loss: 0.7192\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6533 - val_loss: 0.7234\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6544 - val_loss: 0.7271\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6546 - val_loss: 0.7114\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6535 - val_loss: 0.7148\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6532 - val_loss: 0.7092\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6535 - val_loss: 0.7172\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6523 - val_loss: 0.7119\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6523 - val_loss: 0.7044\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6531 - val_loss: 0.7065\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6517 - val_loss: 0.7206\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6508 - val_loss: 0.7154\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6522 - val_loss: 0.7056\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6520 - val_loss: 0.7147\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6524 - val_loss: 0.7106\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6516 - val_loss: 0.7020\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6502 - val_loss: 0.7033\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6502 - val_loss: 0.7149\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6497 - val_loss: 0.7249\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6508 - val_loss: 0.7048\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6494 - val_loss: 0.7114\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6505 - val_loss: 0.7033\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 178us/step - loss: 0.6503 - val_loss: 0.7096\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6482 - val_loss: 0.7069\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 180us/step - loss: 0.6489 - val_loss: 0.7111\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6491 - val_loss: 0.7457\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6503 - val_loss: 0.7118\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6485 - val_loss: 0.7067\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6485 - val_loss: 0.7275\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6489 - val_loss: 0.7024\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6491 - val_loss: 0.7074\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6493 - val_loss: 0.7193\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6470 - val_loss: 0.7449\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6474 - val_loss: 0.7099\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6471 - val_loss: 0.7207\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6470 - val_loss: 0.7352\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6467 - val_loss: 0.7132\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6479 - val_loss: 0.7074\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6473 - val_loss: 0.7069\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6459 - val_loss: 0.7070\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6452 - val_loss: 0.7160\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6467 - val_loss: 0.7123\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6480 - val_loss: 0.7032\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6453 - val_loss: 0.7053\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6451 - val_loss: 0.7131\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6480 - val_loss: 0.7044\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6435 - val_loss: 0.7037\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6439 - val_loss: 0.7010\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6462 - val_loss: 0.7146\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6440 - val_loss: 0.7101\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6444 - val_loss: 0.7173\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6451 - val_loss: 0.7064\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6448 - val_loss: 0.7058\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6445 - val_loss: 0.7041\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6424 - val_loss: 0.7156\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 199us/step - loss: 6.3541 - val_loss: 0.8404\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.8040 - val_loss: 0.7933\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.7906 - val_loss: 0.7867\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7859 - val_loss: 0.7834\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7786 - val_loss: 0.7790\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7740 - val_loss: 0.7714\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7650 - val_loss: 0.8286\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7585 - val_loss: 0.7612\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7483 - val_loss: 0.7638\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.7421 - val_loss: 0.7481\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.7350 - val_loss: 0.7491\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7288 - val_loss: 0.7634\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7223 - val_loss: 0.7351\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.7157 - val_loss: 0.7311\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7105 - val_loss: 0.7238\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7058 - val_loss: 0.7404\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7041 - val_loss: 0.7261\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6972 - val_loss: 0.7165\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6975 - val_loss: 0.7098\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6894 - val_loss: 0.7301\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6878 - val_loss: 0.7088\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6862 - val_loss: 0.7044\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6843 - val_loss: 0.7020\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6795 - val_loss: 0.6981\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6794 - val_loss: 0.7387\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6767 - val_loss: 0.6993\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6741 - val_loss: 0.6992\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6725 - val_loss: 0.6966\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6698 - val_loss: 0.6986\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6683 - val_loss: 0.7105\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6664 - val_loss: 0.6982\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6649 - val_loss: 0.6933\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6626 - val_loss: 0.6952\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6611 - val_loss: 0.7010\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6629 - val_loss: 0.7004\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6597 - val_loss: 0.7101\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6591 - val_loss: 0.7004\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6575 - val_loss: 0.7078\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6557 - val_loss: 0.6912\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6548 - val_loss: 0.6912\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6532 - val_loss: 0.6853\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6521 - val_loss: 0.6892\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6503 - val_loss: 0.6904\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6518 - val_loss: 0.6850\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6491 - val_loss: 0.6879\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6474 - val_loss: 0.6870\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6457 - val_loss: 0.6818\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6438 - val_loss: 0.6795\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6456 - val_loss: 0.6838\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6443 - val_loss: 0.6831\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6416 - val_loss: 0.6939\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6426 - val_loss: 0.7040\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6385 - val_loss: 0.6886\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6387 - val_loss: 0.6894\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6353 - val_loss: 0.6831\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6359 - val_loss: 0.6829\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6347 - val_loss: 0.6831\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6355 - val_loss: 0.6809\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6335 - val_loss: 0.6815\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6323 - val_loss: 0.6967\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6323 - val_loss: 0.6876\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6320 - val_loss: 0.6805\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6316 - val_loss: 0.6794\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6307 - val_loss: 0.6860\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6278 - val_loss: 0.6747\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6287 - val_loss: 0.6774\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6265 - val_loss: 0.6772\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6265 - val_loss: 0.6821\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6248 - val_loss: 0.6785\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6245 - val_loss: 0.6786\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6247 - val_loss: 0.6857\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6223 - val_loss: 0.6840\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6222 - val_loss: 0.6862\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6234 - val_loss: 0.6834\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6218 - val_loss: 0.6752\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6192 - val_loss: 0.6799\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6196 - val_loss: 0.6897\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6164 - val_loss: 0.7007\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6193 - val_loss: 0.6780\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6167 - val_loss: 0.7053\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6175 - val_loss: 0.6789\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6150 - val_loss: 0.6731\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6152 - val_loss: 0.6845\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6154 - val_loss: 0.6942\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6134 - val_loss: 0.6813\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6148 - val_loss: 0.6828\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6162 - val_loss: 0.6752\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6149 - val_loss: 0.6759\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6124 - val_loss: 0.6797\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6121 - val_loss: 0.6801\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6111 - val_loss: 0.6753\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6112 - val_loss: 0.6765\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6114 - val_loss: 0.6766\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6110 - val_loss: 0.6811\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6108 - val_loss: 0.6710\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6091 - val_loss: 0.6706\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6118 - val_loss: 0.6720\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6099 - val_loss: 0.6765\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6089 - val_loss: 0.6840\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6087 - val_loss: 0.6781\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6074 - val_loss: 0.6734\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6080 - val_loss: 0.6704\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6069 - val_loss: 0.6747\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6059 - val_loss: 0.6721\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6059 - val_loss: 0.6713\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6051 - val_loss: 0.6721\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6069 - val_loss: 0.6831\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6067 - val_loss: 0.6810\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6055 - val_loss: 0.6849\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6055 - val_loss: 0.6727\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6034 - val_loss: 0.6822\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6041 - val_loss: 0.6762\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6051 - val_loss: 0.6746\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6025 - val_loss: 0.6739\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6049 - val_loss: 0.6715\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6052 - val_loss: 0.6705\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6011 - val_loss: 0.6716\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6019 - val_loss: 0.6957\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6027 - val_loss: 0.6699\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6038 - val_loss: 0.6765\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6004 - val_loss: 0.6687\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6000 - val_loss: 0.6877\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6002 - val_loss: 0.6821\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6016 - val_loss: 0.6658\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6005 - val_loss: 0.6933\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6003 - val_loss: 0.6667\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5968 - val_loss: 0.6764\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5990 - val_loss: 0.6704\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5982 - val_loss: 0.6705\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5990 - val_loss: 0.6749\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5986 - val_loss: 0.6886\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5996 - val_loss: 0.6778\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5970 - val_loss: 0.6768\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5985 - val_loss: 0.6714\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5957 - val_loss: 0.6729\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5972 - val_loss: 0.6619\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5973 - val_loss: 0.6890\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5983 - val_loss: 0.6680\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.5964 - val_loss: 0.6639\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.5959 - val_loss: 0.6797\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5950 - val_loss: 0.6742\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5961 - val_loss: 0.6708\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5954 - val_loss: 0.6900\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5944 - val_loss: 0.6617\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5946 - val_loss: 0.6726\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5951 - val_loss: 0.6653\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5959 - val_loss: 0.6623\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5941 - val_loss: 0.6747\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5948 - val_loss: 0.6622\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5962 - val_loss: 0.6677\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 206us/step - loss: 6.0403 - val_loss: 0.8998\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.8163 - val_loss: 0.8320\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.7985 - val_loss: 0.8351\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7920 - val_loss: 0.8169\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7887 - val_loss: 0.8147\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7829 - val_loss: 0.8041\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7744 - val_loss: 0.8122\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7697 - val_loss: 0.7940\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7618 - val_loss: 0.7860\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7542 - val_loss: 0.7849\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7493 - val_loss: 0.7812\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.7446 - val_loss: 0.7728\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7407 - val_loss: 0.7684\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7338 - val_loss: 0.7672\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7298 - val_loss: 0.7793\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7266 - val_loss: 0.7742\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7224 - val_loss: 0.7540\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7194 - val_loss: 0.7663\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7148 - val_loss: 0.7520\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7106 - val_loss: 0.7530\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.7077 - val_loss: 0.7556\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7061 - val_loss: 0.7625\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7030 - val_loss: 0.7528\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7009 - val_loss: 0.7441\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6984 - val_loss: 0.7430\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6967 - val_loss: 0.7490\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6929 - val_loss: 0.7582\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6910 - val_loss: 0.7440\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6912 - val_loss: 0.7439\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6864 - val_loss: 0.7595\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6878 - val_loss: 0.7417\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6861 - val_loss: 0.7428\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6839 - val_loss: 0.7613\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6807 - val_loss: 0.7522\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6822 - val_loss: 0.7363\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6783 - val_loss: 0.7352\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6804 - val_loss: 0.7616\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6788 - val_loss: 0.7339\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6778 - val_loss: 0.7422\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6752 - val_loss: 0.7346\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6755 - val_loss: 0.7360\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6724 - val_loss: 0.7343\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6738 - val_loss: 0.7337\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6713 - val_loss: 0.7316\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6711 - val_loss: 0.7454\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6693 - val_loss: 0.7498\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6685 - val_loss: 0.7321\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6677 - val_loss: 0.7314\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6676 - val_loss: 0.7395\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6691 - val_loss: 0.7310\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6667 - val_loss: 0.7284\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6669 - val_loss: 0.7466\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6651 - val_loss: 0.7385\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6645 - val_loss: 0.7397\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6632 - val_loss: 0.7327\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6638 - val_loss: 0.7346\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6620 - val_loss: 0.7328\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6626 - val_loss: 0.7272\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6624 - val_loss: 0.7296\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6614 - val_loss: 0.7281\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6624 - val_loss: 0.7436\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6612 - val_loss: 0.7944\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6604 - val_loss: 0.7292\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6582 - val_loss: 0.7261\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6603 - val_loss: 0.7291\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6598 - val_loss: 0.7259\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6564 - val_loss: 0.7273\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6580 - val_loss: 0.7377\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6588 - val_loss: 0.7267\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6590 - val_loss: 0.7288\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6574 - val_loss: 0.7272\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6569 - val_loss: 0.7528\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6553 - val_loss: 0.7216\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6550 - val_loss: 0.7260\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6564 - val_loss: 0.7263\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6550 - val_loss: 0.7219\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6556 - val_loss: 0.7277\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6539 - val_loss: 0.7389\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6529 - val_loss: 0.7500\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6524 - val_loss: 0.7182\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6524 - val_loss: 0.7312\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6550 - val_loss: 0.7203\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6529 - val_loss: 0.7227\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6523 - val_loss: 0.7309\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6545 - val_loss: 0.7187\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6516 - val_loss: 0.7330\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6519 - val_loss: 0.7281\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6514 - val_loss: 0.7262\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6506 - val_loss: 0.7243\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6523 - val_loss: 0.7311\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6515 - val_loss: 0.7252\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6526 - val_loss: 0.7205\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6511 - val_loss: 0.7228\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6498 - val_loss: 0.7341\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6529 - val_loss: 0.7554\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6505 - val_loss: 0.7191\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6490 - val_loss: 0.7165\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6486 - val_loss: 0.7211\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6510 - val_loss: 0.7639\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6486 - val_loss: 0.7227\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6512 - val_loss: 0.7189\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6512 - val_loss: 0.7228\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6480 - val_loss: 0.7265\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6486 - val_loss: 0.7231\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6487 - val_loss: 0.7253\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6489 - val_loss: 0.7162\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6473 - val_loss: 0.7344\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6454 - val_loss: 0.7214\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6480 - val_loss: 0.7241\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6465 - val_loss: 0.7270\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6449 - val_loss: 0.7206\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6468 - val_loss: 0.7163\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6459 - val_loss: 0.7172\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6457 - val_loss: 0.7203\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6459 - val_loss: 0.7405\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6461 - val_loss: 0.7177\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6475 - val_loss: 0.7233\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6455 - val_loss: 0.7240\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6467 - val_loss: 0.7212\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6462 - val_loss: 0.7311\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6431 - val_loss: 0.7241\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6454 - val_loss: 0.7230\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6450 - val_loss: 0.7221\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6447 - val_loss: 0.7178\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6445 - val_loss: 0.7188\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6450 - val_loss: 0.7181\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6414 - val_loss: 0.7200\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6458 - val_loss: 0.7163\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6433 - val_loss: 0.7186\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6426 - val_loss: 0.7187\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6433 - val_loss: 0.7627\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6422 - val_loss: 0.7183\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6442 - val_loss: 0.7142\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6422 - val_loss: 0.7179\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6449 - val_loss: 0.7242\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6419 - val_loss: 0.7166\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6429 - val_loss: 0.7169\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6430 - val_loss: 0.7145\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6420 - val_loss: 0.7323\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6421 - val_loss: 0.7249\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6439 - val_loss: 0.7184\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6411 - val_loss: 0.7194\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6446 - val_loss: 0.7154\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6422 - val_loss: 0.7214\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6425 - val_loss: 0.7227\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6436 - val_loss: 0.7203\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6414 - val_loss: 0.7255\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6426 - val_loss: 0.7337\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6410 - val_loss: 0.7257\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6408 - val_loss: 0.7174\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 284us/step - loss: 20.1978 - val_loss: 7.6535\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 1.9330 - val_loss: 1.0316\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.9175 - val_loss: 0.8456\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.8101 - val_loss: 0.7994\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7757 - val_loss: 0.7804\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7606 - val_loss: 0.7688\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7583 - val_loss: 0.7695\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7543 - val_loss: 0.7700\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7549 - val_loss: 0.7682\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7477 - val_loss: 0.7662\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7479 - val_loss: 0.7628\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7486 - val_loss: 0.7630\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7483 - val_loss: 0.7843\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7496 - val_loss: 0.7879\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7460 - val_loss: 0.7664\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7443 - val_loss: 0.7656\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7414 - val_loss: 0.7602\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7429 - val_loss: 0.7669\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7420 - val_loss: 0.7761\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7350 - val_loss: 0.7704\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7360 - val_loss: 0.7776\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7384 - val_loss: 0.7580\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7328 - val_loss: 0.7688\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7313 - val_loss: 0.7604\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7308 - val_loss: 0.7720\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7286 - val_loss: 0.7564\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7277 - val_loss: 0.7557\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7248 - val_loss: 0.7889\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7295 - val_loss: 0.7533\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7257 - val_loss: 0.7588\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7228 - val_loss: 0.7500\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7198 - val_loss: 0.7595\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7262 - val_loss: 0.7666\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7215 - val_loss: 0.7573\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7129 - val_loss: 0.7498\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7163 - val_loss: 0.7630\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7155 - val_loss: 0.7479\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7099 - val_loss: 0.7518\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7081 - val_loss: 0.7442\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7093 - val_loss: 0.7621\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7064 - val_loss: 0.7510\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7051 - val_loss: 0.7414\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7074 - val_loss: 0.7471\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7043 - val_loss: 0.7611\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7013 - val_loss: 0.7413\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7010 - val_loss: 0.7511\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6988 - val_loss: 0.7409\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6994 - val_loss: 0.7385\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6979 - val_loss: 0.7470\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6972 - val_loss: 0.7383\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6944 - val_loss: 0.7422\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6926 - val_loss: 0.7415\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6932 - val_loss: 0.7417\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6879 - val_loss: 0.7521\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6915 - val_loss: 0.7560\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6871 - val_loss: 0.7361\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6875 - val_loss: 0.7575\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6853 - val_loss: 0.7618\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6837 - val_loss: 0.7367\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6852 - val_loss: 0.7383\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6870 - val_loss: 0.7390\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6798 - val_loss: 0.7354\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6825 - val_loss: 0.7390\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6797 - val_loss: 0.7358\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6816 - val_loss: 0.7442\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6762 - val_loss: 0.7473\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6784 - val_loss: 0.7470\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6776 - val_loss: 0.7386\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6762 - val_loss: 0.7354\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6749 - val_loss: 0.7311\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6736 - val_loss: 0.7365\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6769 - val_loss: 0.7370\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6715 - val_loss: 0.7330\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6724 - val_loss: 0.7334\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6696 - val_loss: 0.7441\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6714 - val_loss: 0.7325\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6670 - val_loss: 0.7370\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6669 - val_loss: 0.7315\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6711 - val_loss: 0.7544\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6685 - val_loss: 0.7520\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6725 - val_loss: 0.7314\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6685 - val_loss: 0.7406\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6661 - val_loss: 0.7467\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6669 - val_loss: 0.7338\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6641 - val_loss: 0.7316\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6617 - val_loss: 0.7322\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6644 - val_loss: 0.7303\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6633 - val_loss: 0.7275\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6636 - val_loss: 0.7463\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6639 - val_loss: 0.7340\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6602 - val_loss: 0.7338\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6570 - val_loss: 0.7399\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6587 - val_loss: 0.7267\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6602 - val_loss: 0.7390\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6584 - val_loss: 0.7528\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6543 - val_loss: 0.7265\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6602 - val_loss: 0.7376\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6582 - val_loss: 0.7345\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6565 - val_loss: 0.7256\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6555 - val_loss: 0.7224\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6521 - val_loss: 0.7281\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6556 - val_loss: 0.7399\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6553 - val_loss: 0.7186\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6569 - val_loss: 0.7302\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6563 - val_loss: 0.7303\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6528 - val_loss: 0.7282\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6528 - val_loss: 0.7248\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6522 - val_loss: 0.7329\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6512 - val_loss: 0.7318\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6557 - val_loss: 0.7255\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6497 - val_loss: 0.7231\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6500 - val_loss: 0.7244\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6509 - val_loss: 0.7334\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6483 - val_loss: 0.7275\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6476 - val_loss: 0.7408\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6513 - val_loss: 0.7380\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6497 - val_loss: 0.7232\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6450 - val_loss: 0.7208\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6488 - val_loss: 0.7283\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6463 - val_loss: 0.7249\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6443 - val_loss: 0.7182\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6485 - val_loss: 0.7325\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6463 - val_loss: 0.7222\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6475 - val_loss: 0.7211\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6437 - val_loss: 0.7202\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6402 - val_loss: 0.7280\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6434 - val_loss: 0.7440\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6448 - val_loss: 0.7301\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6413 - val_loss: 0.7165\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6425 - val_loss: 0.7253\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6412 - val_loss: 0.7200\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6432 - val_loss: 0.7200\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6426 - val_loss: 0.7203\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6407 - val_loss: 0.7167\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6390 - val_loss: 0.7170\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6408 - val_loss: 0.7212\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6443 - val_loss: 0.7225\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6417 - val_loss: 0.7173\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6403 - val_loss: 0.7132\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6385 - val_loss: 0.7189\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6399 - val_loss: 0.7356\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6419 - val_loss: 0.7173\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6384 - val_loss: 0.7218\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6376 - val_loss: 0.7248\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6390 - val_loss: 0.7145\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6382 - val_loss: 0.7256\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6409 - val_loss: 0.7195\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6391 - val_loss: 0.7153\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6369 - val_loss: 0.7140\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6359 - val_loss: 0.7163\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 288us/step - loss: 20.8391 - val_loss: 10.2801\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 2.6515 - val_loss: 1.0911\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.9787 - val_loss: 0.8604\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.8418 - val_loss: 0.8170\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7892 - val_loss: 0.7620\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7678 - val_loss: 0.7471\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7578 - val_loss: 0.7408\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7552 - val_loss: 0.7383\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7506 - val_loss: 0.7450\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7512 - val_loss: 0.7402\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7526 - val_loss: 0.7516\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7497 - val_loss: 0.7420\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7485 - val_loss: 0.7419\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7518 - val_loss: 0.7415\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7444 - val_loss: 0.7430\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7457 - val_loss: 0.7537\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7447 - val_loss: 0.7434\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7446 - val_loss: 0.7522\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7460 - val_loss: 0.7689\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7475 - val_loss: 0.7356\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7448 - val_loss: 0.7667\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7465 - val_loss: 0.7736\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7427 - val_loss: 0.7354\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7404 - val_loss: 0.7329\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7423 - val_loss: 0.7403\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7424 - val_loss: 0.7341\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7391 - val_loss: 0.7366\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7402 - val_loss: 0.7340\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7381 - val_loss: 0.7392\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7371 - val_loss: 0.7284\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7385 - val_loss: 0.7314\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7352 - val_loss: 0.7324\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7359 - val_loss: 0.7347\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7353 - val_loss: 0.7269\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7319 - val_loss: 0.7335\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7345 - val_loss: 0.7286\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7341 - val_loss: 0.7470\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7323 - val_loss: 0.7243\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7303 - val_loss: 0.7268\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7299 - val_loss: 0.7666\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7303 - val_loss: 0.7295\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7298 - val_loss: 0.7325\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7315 - val_loss: 0.7211\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7324 - val_loss: 0.7180\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7259 - val_loss: 0.7182\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7264 - val_loss: 0.7166\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7253 - val_loss: 0.7163\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7240 - val_loss: 0.7405\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7235 - val_loss: 0.7146\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7270 - val_loss: 0.7161\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7212 - val_loss: 0.7363\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7251 - val_loss: 0.7185\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7214 - val_loss: 0.7181\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7178 - val_loss: 0.7161\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7198 - val_loss: 0.7181\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7186 - val_loss: 0.7073\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7199 - val_loss: 0.7181\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7182 - val_loss: 0.7071\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7188 - val_loss: 0.7387\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7152 - val_loss: 0.7148\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7201 - val_loss: 0.7134\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7188 - val_loss: 0.7153\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7133 - val_loss: 0.7118\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7164 - val_loss: 0.7068\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7160 - val_loss: 0.7175\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7134 - val_loss: 0.7065\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7092 - val_loss: 0.7089\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7103 - val_loss: 0.7108\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7102 - val_loss: 0.7271\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7086 - val_loss: 0.7054\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7079 - val_loss: 0.7060\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7078 - val_loss: 0.6974\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7072 - val_loss: 0.7003\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7058 - val_loss: 0.7282\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7053 - val_loss: 0.7070\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7014 - val_loss: 0.7167\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7049 - val_loss: 0.6984\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7004 - val_loss: 0.7017\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6979 - val_loss: 0.7078\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6999 - val_loss: 0.6988\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6974 - val_loss: 0.7022\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6989 - val_loss: 0.6902\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6997 - val_loss: 0.6978\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6960 - val_loss: 0.7014\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6948 - val_loss: 0.6996\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6935 - val_loss: 0.6950\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6933 - val_loss: 0.6895\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6945 - val_loss: 0.6974\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6886 - val_loss: 0.6926\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6885 - val_loss: 0.6929\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6907 - val_loss: 0.6989\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6896 - val_loss: 0.6868\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6877 - val_loss: 0.7061\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6903 - val_loss: 0.7015\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6876 - val_loss: 0.6929\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6877 - val_loss: 0.6886\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6843 - val_loss: 0.7086\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6821 - val_loss: 0.6908\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6862 - val_loss: 0.6903\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6824 - val_loss: 0.6893\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6813 - val_loss: 0.6885\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6813 - val_loss: 0.6915\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6810 - val_loss: 0.7046\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6790 - val_loss: 0.6850\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6820 - val_loss: 0.7101\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6769 - val_loss: 0.6907\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6791 - val_loss: 0.6809\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6765 - val_loss: 0.6804\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6782 - val_loss: 0.6878\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6733 - val_loss: 0.6899\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6761 - val_loss: 0.6983\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6732 - val_loss: 0.7035\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6756 - val_loss: 0.6908\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6763 - val_loss: 0.6822\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6714 - val_loss: 0.6931\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6744 - val_loss: 0.6999\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6729 - val_loss: 0.7108\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6782 - val_loss: 0.6892\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6695 - val_loss: 0.6889\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6719 - val_loss: 0.7074\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6693 - val_loss: 0.6889\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6709 - val_loss: 0.7016\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6725 - val_loss: 0.6852\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6639 - val_loss: 0.6896\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6674 - val_loss: 0.6931\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6660 - val_loss: 0.6968\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6723 - val_loss: 0.6897\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6651 - val_loss: 0.6834\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6677 - val_loss: 0.6890\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6686 - val_loss: 0.6920\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6627 - val_loss: 0.6853\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6643 - val_loss: 0.6879\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6639 - val_loss: 0.6779\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6642 - val_loss: 0.6891\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6624 - val_loss: 0.6849\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6622 - val_loss: 0.6852\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6620 - val_loss: 0.6810\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6628 - val_loss: 0.6930\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.6645 - val_loss: 0.6811\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6598 - val_loss: 0.7125\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6660 - val_loss: 0.6817\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6630 - val_loss: 0.6832\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6620 - val_loss: 0.6752\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6599 - val_loss: 0.6869\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6588 - val_loss: 0.6914\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6587 - val_loss: 0.6857\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6615 - val_loss: 0.6828\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6555 - val_loss: 0.6857\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6579 - val_loss: 0.6845\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6571 - val_loss: 0.6832\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 287us/step - loss: 20.3716 - val_loss: 8.4471\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 2.1711 - val_loss: 0.9850\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.9229 - val_loss: 0.8421\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.8185 - val_loss: 0.7982\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7787 - val_loss: 0.7718\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7654 - val_loss: 0.7540\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 180us/step - loss: 0.7542 - val_loss: 0.7724\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.7548 - val_loss: 0.7583\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.7503 - val_loss: 0.7621\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7507 - val_loss: 0.7529\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7489 - val_loss: 0.7577\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7462 - val_loss: 0.7540\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7450 - val_loss: 0.7859\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7444 - val_loss: 0.7695\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7464 - val_loss: 0.7593\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 182us/step - loss: 0.7427 - val_loss: 0.7531\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7409 - val_loss: 0.7748\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.7400 - val_loss: 0.7786\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7346 - val_loss: 0.7758\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7369 - val_loss: 0.7552\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7335 - val_loss: 0.7505\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7325 - val_loss: 0.7607\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7287 - val_loss: 0.7679\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7285 - val_loss: 0.7489\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7248 - val_loss: 0.7462\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7234 - val_loss: 0.7397\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7290 - val_loss: 0.7436\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7211 - val_loss: 0.7444\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7178 - val_loss: 0.7478\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7179 - val_loss: 0.7421\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7165 - val_loss: 0.7435\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7162 - val_loss: 0.7338\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7091 - val_loss: 0.7515\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 180us/step - loss: 0.7097 - val_loss: 0.7356\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.7074 - val_loss: 0.7310\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7044 - val_loss: 0.7434\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7105 - val_loss: 0.7279\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.7002 - val_loss: 0.7420\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6979 - val_loss: 0.7326\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6965 - val_loss: 0.7302\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6954 - val_loss: 0.7349\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6933 - val_loss: 0.7315\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6912 - val_loss: 0.7349\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6890 - val_loss: 0.7266\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6895 - val_loss: 0.7420\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6862 - val_loss: 0.7219\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6836 - val_loss: 0.7243\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6870 - val_loss: 0.7449\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6832 - val_loss: 0.7255\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6822 - val_loss: 0.7331\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6773 - val_loss: 0.7206\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6801 - val_loss: 0.7170\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6734 - val_loss: 0.7169\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6734 - val_loss: 0.7198\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6740 - val_loss: 0.7220\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6706 - val_loss: 0.7305\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6723 - val_loss: 0.7128\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.6689 - val_loss: 0.7181\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6684 - val_loss: 0.7190\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6654 - val_loss: 0.7248\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6656 - val_loss: 0.7168\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6673 - val_loss: 0.7154\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6614 - val_loss: 0.7160\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 179us/step - loss: 0.6613 - val_loss: 0.7250\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6604 - val_loss: 0.7230\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6609 - val_loss: 0.7206\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6587 - val_loss: 0.7145\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6537 - val_loss: 0.7346\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6593 - val_loss: 0.7172\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6574 - val_loss: 0.7121\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6580 - val_loss: 0.7161\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6542 - val_loss: 0.7212\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6556 - val_loss: 0.7153\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6548 - val_loss: 0.7128\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6507 - val_loss: 0.7123\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6530 - val_loss: 0.7164\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6542 - val_loss: 0.7235\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6488 - val_loss: 0.7190\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6558 - val_loss: 0.7284\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6507 - val_loss: 0.7113\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6455 - val_loss: 0.7141\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6453 - val_loss: 0.7310\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6473 - val_loss: 0.7283\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6427 - val_loss: 0.7164\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6414 - val_loss: 0.7194\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6441 - val_loss: 0.7108\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 180us/step - loss: 0.6443 - val_loss: 0.7172\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6426 - val_loss: 0.7163\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6446 - val_loss: 0.7121\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6384 - val_loss: 0.7381\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6408 - val_loss: 0.7158\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6401 - val_loss: 0.7091\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6382 - val_loss: 0.7131\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6416 - val_loss: 0.7099\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6379 - val_loss: 0.7132\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6363 - val_loss: 0.7111\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6375 - val_loss: 0.7431\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.6357 - val_loss: 0.7413\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6373 - val_loss: 0.7155\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6348 - val_loss: 0.7145\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6361 - val_loss: 0.7121\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6322 - val_loss: 0.7107\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6335 - val_loss: 0.7088\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6360 - val_loss: 0.7136\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.6309 - val_loss: 0.7085\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6360 - val_loss: 0.7049\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6332 - val_loss: 0.7100\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6314 - val_loss: 0.7117\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6332 - val_loss: 0.7139\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6324 - val_loss: 0.7068\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6317 - val_loss: 0.7073\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6302 - val_loss: 0.7130\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6285 - val_loss: 0.7099\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6294 - val_loss: 0.7082\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6263 - val_loss: 0.7246\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6259 - val_loss: 0.7115\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6261 - val_loss: 0.7095\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6281 - val_loss: 0.7062\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6284 - val_loss: 0.7073\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6263 - val_loss: 0.7076\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6288 - val_loss: 0.7060\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6253 - val_loss: 0.7193\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6255 - val_loss: 0.7546\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6269 - val_loss: 0.7049\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6292 - val_loss: 0.7559\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6263 - val_loss: 0.7105\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6259 - val_loss: 0.7052\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6267 - val_loss: 0.7082\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6226 - val_loss: 0.7072\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6220 - val_loss: 0.7067\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6226 - val_loss: 0.7533\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6240 - val_loss: 0.6984\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6264 - val_loss: 0.7044\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6207 - val_loss: 0.7168\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6223 - val_loss: 0.7076\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6214 - val_loss: 0.7009\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6215 - val_loss: 0.7120\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6185 - val_loss: 0.7069\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.6215 - val_loss: 0.7058\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 180us/step - loss: 0.6192 - val_loss: 0.7024\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6200 - val_loss: 0.7302\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6186 - val_loss: 0.7236\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6164 - val_loss: 0.7080\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6128 - val_loss: 0.7002\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6179 - val_loss: 0.7068\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6192 - val_loss: 0.7283\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6155 - val_loss: 0.7054\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6149 - val_loss: 0.7521\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6183 - val_loss: 0.7074\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6162 - val_loss: 0.7101\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 286us/step - loss: 20.2972 - val_loss: 9.0442\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 2.2892 - val_loss: 1.0864\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.9449 - val_loss: 0.8621\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.8217 - val_loss: 0.7933\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7777 - val_loss: 0.7776\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7636 - val_loss: 0.7777\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7567 - val_loss: 0.7543\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7511 - val_loss: 0.7828\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7544 - val_loss: 0.7575\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7458 - val_loss: 0.7721\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7461 - val_loss: 0.7510\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7443 - val_loss: 0.7533\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7466 - val_loss: 0.7673\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7432 - val_loss: 0.7542\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7419 - val_loss: 0.7614\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7372 - val_loss: 0.7518\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7369 - val_loss: 0.7534\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7344 - val_loss: 0.7595\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7343 - val_loss: 0.7470\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7306 - val_loss: 0.7435\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7273 - val_loss: 0.7463\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7291 - val_loss: 0.7412\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7283 - val_loss: 0.7385\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7204 - val_loss: 0.7466\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7185 - val_loss: 0.7384\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7184 - val_loss: 0.7243\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7128 - val_loss: 0.7318\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7145 - val_loss: 0.7329\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7101 - val_loss: 0.7239\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7077 - val_loss: 0.7324\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7021 - val_loss: 0.7215\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7031 - val_loss: 0.7348\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7001 - val_loss: 0.7358\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6986 - val_loss: 0.7213\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6926 - val_loss: 0.7323\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6929 - val_loss: 0.7253\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6879 - val_loss: 0.7243\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6913 - val_loss: 0.7198\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6854 - val_loss: 0.7075\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6886 - val_loss: 0.7153\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6852 - val_loss: 0.7203\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6821 - val_loss: 0.7265\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6784 - val_loss: 0.7071\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6769 - val_loss: 0.7183\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6739 - val_loss: 0.7116\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6766 - val_loss: 0.7136\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6739 - val_loss: 0.7097\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6705 - val_loss: 0.7071\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6733 - val_loss: 0.7021\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6707 - val_loss: 0.7132\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6690 - val_loss: 0.7019\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6671 - val_loss: 0.7070\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6643 - val_loss: 0.7027\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6621 - val_loss: 0.7019\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6618 - val_loss: 0.6894\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6592 - val_loss: 0.6968\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6541 - val_loss: 0.7083\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6592 - val_loss: 0.6966\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6577 - val_loss: 0.6980\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6560 - val_loss: 0.6969\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6533 - val_loss: 0.6917\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6528 - val_loss: 0.6882\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6555 - val_loss: 0.6873\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6522 - val_loss: 0.6920\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6522 - val_loss: 0.6844\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6501 - val_loss: 0.6902\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6469 - val_loss: 0.7034\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6468 - val_loss: 0.6887\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6499 - val_loss: 0.6886\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6435 - val_loss: 0.6887\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6423 - val_loss: 0.6882\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6425 - val_loss: 0.6900\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6442 - val_loss: 0.6767\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6401 - val_loss: 0.6894\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6425 - val_loss: 0.6829\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6425 - val_loss: 0.6821\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6437 - val_loss: 0.6846\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6379 - val_loss: 0.6888\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6396 - val_loss: 0.6814\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6360 - val_loss: 0.6765\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6393 - val_loss: 0.6827\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6342 - val_loss: 0.6857\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6308 - val_loss: 0.6807\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6342 - val_loss: 0.6827\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6317 - val_loss: 0.6992\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6383 - val_loss: 0.6794\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6302 - val_loss: 0.7101\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6323 - val_loss: 0.6834\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6302 - val_loss: 0.6825\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6250 - val_loss: 0.6827\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6269 - val_loss: 0.6985\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6270 - val_loss: 0.6847\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6286 - val_loss: 0.6756\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6237 - val_loss: 0.6934\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6256 - val_loss: 0.6788\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6234 - val_loss: 0.6793\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6207 - val_loss: 0.6812\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6231 - val_loss: 0.6937\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6203 - val_loss: 0.6807\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6213 - val_loss: 0.6783\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6214 - val_loss: 0.6737\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6219 - val_loss: 0.6979\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6212 - val_loss: 0.6718\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6184 - val_loss: 0.6801\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6200 - val_loss: 0.6831\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6192 - val_loss: 0.6795\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6152 - val_loss: 0.6793\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6164 - val_loss: 0.6814\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6160 - val_loss: 0.6735\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6125 - val_loss: 0.6780\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6146 - val_loss: 0.6769\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6126 - val_loss: 0.6717\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6104 - val_loss: 0.6944\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6143 - val_loss: 0.6698\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6136 - val_loss: 0.6717\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6101 - val_loss: 0.6691\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6088 - val_loss: 0.6743\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6133 - val_loss: 0.6667\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6108 - val_loss: 0.6754\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6087 - val_loss: 0.6683\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6110 - val_loss: 0.6779\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6092 - val_loss: 0.6784\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6103 - val_loss: 0.6797\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6052 - val_loss: 0.6719\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6082 - val_loss: 0.6715\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6076 - val_loss: 0.6702\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6067 - val_loss: 0.6679\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6057 - val_loss: 0.6683\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6022 - val_loss: 0.6654\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6017 - val_loss: 0.6697\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6017 - val_loss: 0.6780\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.5997 - val_loss: 0.6750\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6001 - val_loss: 0.6668\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6007 - val_loss: 0.6867\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6007 - val_loss: 0.6761\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6008 - val_loss: 0.6752\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.5981 - val_loss: 0.6752\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6022 - val_loss: 0.6798\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.5973 - val_loss: 0.6668\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6005 - val_loss: 0.6682\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.5989 - val_loss: 0.6810\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.5949 - val_loss: 0.6735\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.5957 - val_loss: 0.6687\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.5967 - val_loss: 0.6813\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.5936 - val_loss: 0.6999\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.5948 - val_loss: 0.6744\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.5934 - val_loss: 0.6837\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.5923 - val_loss: 0.6657\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.5941 - val_loss: 0.6727\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.5947 - val_loss: 0.6739\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 200us/step - loss: 6.2146 - val_loss: 0.8090\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7684 - val_loss: 0.7678\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7534 - val_loss: 0.7567\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7487 - val_loss: 0.7516\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7441 - val_loss: 0.7474\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7358 - val_loss: 0.7370\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7290 - val_loss: 0.7320\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7198 - val_loss: 0.7295\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7124 - val_loss: 0.7217\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7052 - val_loss: 0.7257\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7003 - val_loss: 0.7207\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6921 - val_loss: 0.7156\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6902 - val_loss: 0.7010\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6848 - val_loss: 0.6982\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6779 - val_loss: 0.6907\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6753 - val_loss: 0.6904\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6708 - val_loss: 0.7085\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6654 - val_loss: 0.7163\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 179us/step - loss: 0.6649 - val_loss: 0.6824\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6608 - val_loss: 0.6825\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6582 - val_loss: 0.6797\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6574 - val_loss: 0.6780\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6536 - val_loss: 0.6773\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6525 - val_loss: 0.6678\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6485 - val_loss: 0.6738\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 3s 203us/step - loss: 0.6476 - val_loss: 0.6720\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6436 - val_loss: 0.6672\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6424 - val_loss: 0.6968\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6430 - val_loss: 0.6709\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6416 - val_loss: 0.6701\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6393 - val_loss: 0.6598\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6376 - val_loss: 0.6628\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6350 - val_loss: 0.6639\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6361 - val_loss: 0.6676\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6347 - val_loss: 0.6618\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6346 - val_loss: 0.7077\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6338 - val_loss: 0.6663\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6311 - val_loss: 0.6573\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6301 - val_loss: 0.6639\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6278 - val_loss: 0.6720\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6282 - val_loss: 0.6621\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6282 - val_loss: 0.6608\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6267 - val_loss: 0.6553\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6274 - val_loss: 0.6893\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6245 - val_loss: 0.6717\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6256 - val_loss: 0.6690\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6246 - val_loss: 0.6602\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6230 - val_loss: 0.6559\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6212 - val_loss: 0.6608\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6218 - val_loss: 0.6556\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6208 - val_loss: 0.6556\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6206 - val_loss: 0.6522\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6210 - val_loss: 0.6623\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6203 - val_loss: 0.6655\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6189 - val_loss: 0.6568\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6202 - val_loss: 0.6554\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6175 - val_loss: 0.6542\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6196 - val_loss: 0.6703\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6169 - val_loss: 0.6599\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6149 - val_loss: 0.6589\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6162 - val_loss: 0.6693\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6139 - val_loss: 0.6692\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6143 - val_loss: 0.6534\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6159 - val_loss: 0.6765\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6148 - val_loss: 0.6653\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6155 - val_loss: 0.6531\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6141 - val_loss: 0.6568\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6136 - val_loss: 0.6490\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6131 - val_loss: 0.6485\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6147 - val_loss: 0.6567\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6119 - val_loss: 0.6573\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6107 - val_loss: 0.6572\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6128 - val_loss: 0.6509\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6129 - val_loss: 0.6541\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6092 - val_loss: 0.6503\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6104 - val_loss: 0.6528\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6106 - val_loss: 0.6493\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6118 - val_loss: 0.6581\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6106 - val_loss: 0.6472\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6101 - val_loss: 0.6470\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6076 - val_loss: 0.6743\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6103 - val_loss: 0.6687\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6102 - val_loss: 0.6750\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6086 - val_loss: 0.6500\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6083 - val_loss: 0.6517\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6068 - val_loss: 0.6574\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6070 - val_loss: 0.6609\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6089 - val_loss: 0.6623\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6079 - val_loss: 0.6536\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6082 - val_loss: 0.6515\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6067 - val_loss: 0.6675\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6062 - val_loss: 0.6517\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6080 - val_loss: 0.6451\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6069 - val_loss: 0.6456\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6056 - val_loss: 0.6589\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6063 - val_loss: 0.6513\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6075 - val_loss: 0.6454\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6051 - val_loss: 0.6532\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6046 - val_loss: 0.6475\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6062 - val_loss: 0.6588\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6067 - val_loss: 0.6495\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6057 - val_loss: 0.6633\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6043 - val_loss: 0.6482\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6051 - val_loss: 0.6557\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6062 - val_loss: 0.6559\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6051 - val_loss: 0.6611\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6062 - val_loss: 0.6532\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6029 - val_loss: 0.6542\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6026 - val_loss: 0.6515\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6031 - val_loss: 0.6491\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6042 - val_loss: 0.6496\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6030 - val_loss: 0.6534\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6036 - val_loss: 0.6490\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6041 - val_loss: 0.6557\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6039 - val_loss: 0.6464\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6041 - val_loss: 0.6462\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6024 - val_loss: 0.6486\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6025 - val_loss: 0.6486\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6034 - val_loss: 0.6490\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6027 - val_loss: 0.6502\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6026 - val_loss: 0.6489\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6035 - val_loss: 0.6493\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6028 - val_loss: 0.6686\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6039 - val_loss: 0.6512\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6032 - val_loss: 0.6448\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6031 - val_loss: 0.6559\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6029 - val_loss: 0.6545\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6038 - val_loss: 0.6518\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6029 - val_loss: 0.6746\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6013 - val_loss: 0.6515\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6013 - val_loss: 0.6607\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6019 - val_loss: 0.6483\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6027 - val_loss: 0.6523\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6014 - val_loss: 0.6528\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6017 - val_loss: 0.6549\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6021 - val_loss: 0.6555\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6006 - val_loss: 0.6451\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5996 - val_loss: 0.6545\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6013 - val_loss: 0.6530\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6030 - val_loss: 0.6489\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.5992 - val_loss: 0.6510\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6014 - val_loss: 0.6727\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.5994 - val_loss: 0.6606\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6013 - val_loss: 0.6454\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6011 - val_loss: 0.6507\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.5992 - val_loss: 0.6440\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6010 - val_loss: 0.6566\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.5999 - val_loss: 0.6539\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.5988 - val_loss: 0.6675\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6002 - val_loss: 0.6531\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 195us/step - loss: 5.9945 - val_loss: 0.7992\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 164us/step - loss: 0.7744 - val_loss: 0.7685\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 163us/step - loss: 0.7645 - val_loss: 0.7532\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 163us/step - loss: 0.7587 - val_loss: 0.7472\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 164us/step - loss: 0.7542 - val_loss: 0.7460\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 165us/step - loss: 0.7469 - val_loss: 0.7368\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.7387 - val_loss: 0.7353\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.7351 - val_loss: 0.7229\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.7264 - val_loss: 0.7216\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.7204 - val_loss: 0.7054\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.7136 - val_loss: 0.7029\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.7102 - val_loss: 0.7017\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.7038 - val_loss: 0.6974\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6993 - val_loss: 0.6977\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6943 - val_loss: 0.6928\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6912 - val_loss: 0.6847\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6865 - val_loss: 0.6841\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6830 - val_loss: 0.6816\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6802 - val_loss: 0.6914\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6759 - val_loss: 0.6846\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6730 - val_loss: 0.6736\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6728 - val_loss: 0.6757\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6681 - val_loss: 0.6794\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6663 - val_loss: 0.6738\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6643 - val_loss: 0.6709\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6644 - val_loss: 0.6815\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6585 - val_loss: 0.6625\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6578 - val_loss: 0.6891\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6566 - val_loss: 0.6573\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6543 - val_loss: 0.6603\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6542 - val_loss: 0.6769\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6507 - val_loss: 0.6546\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6515 - val_loss: 0.6580\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6491 - val_loss: 0.6586\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6492 - val_loss: 0.6612\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6456 - val_loss: 0.6554\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6450 - val_loss: 0.6607\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6428 - val_loss: 0.6576\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6429 - val_loss: 0.6733\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6443 - val_loss: 0.7105\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6412 - val_loss: 0.6526\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6378 - val_loss: 0.6529\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6377 - val_loss: 0.6506\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6373 - val_loss: 0.6564\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6376 - val_loss: 0.6525\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6332 - val_loss: 0.6552\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6338 - val_loss: 0.6544\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6306 - val_loss: 0.6517\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6304 - val_loss: 0.6529\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6304 - val_loss: 0.6474\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6295 - val_loss: 0.6603\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6301 - val_loss: 0.6435\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6288 - val_loss: 0.6471\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6281 - val_loss: 0.6565\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6260 - val_loss: 0.6500\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6243 - val_loss: 0.6488\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6258 - val_loss: 0.6442\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6236 - val_loss: 0.6609\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6224 - val_loss: 0.6442\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6218 - val_loss: 0.6433\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6213 - val_loss: 0.6486\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6202 - val_loss: 0.6546\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6175 - val_loss: 0.6698\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6192 - val_loss: 0.6402\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6167 - val_loss: 0.6461\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6169 - val_loss: 0.6478\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6171 - val_loss: 0.6462\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6145 - val_loss: 0.6362\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6135 - val_loss: 0.6421\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6144 - val_loss: 0.6436\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6129 - val_loss: 0.6394\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6138 - val_loss: 0.6555\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6141 - val_loss: 0.6347\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6116 - val_loss: 0.6535\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6108 - val_loss: 0.6358\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6111 - val_loss: 0.6411\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6110 - val_loss: 0.6528\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6082 - val_loss: 0.6333\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6108 - val_loss: 0.6354\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6075 - val_loss: 0.6343\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6075 - val_loss: 0.6401\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6086 - val_loss: 0.6419\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6079 - val_loss: 0.6483\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6070 - val_loss: 0.6395\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6078 - val_loss: 0.6414\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6048 - val_loss: 0.6354\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6047 - val_loss: 0.6381\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6064 - val_loss: 0.6308\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6030 - val_loss: 0.6378\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6043 - val_loss: 0.6343\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6031 - val_loss: 0.6543\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6024 - val_loss: 0.6334\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6018 - val_loss: 0.6372\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6027 - val_loss: 0.6489\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6036 - val_loss: 0.6314\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6028 - val_loss: 0.6383\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6020 - val_loss: 0.6356\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6009 - val_loss: 0.6359\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6011 - val_loss: 0.6347\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6014 - val_loss: 0.6307\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6026 - val_loss: 0.6317\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6004 - val_loss: 0.6301\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6013 - val_loss: 0.6687\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5997 - val_loss: 0.6349\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6001 - val_loss: 0.6382\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5983 - val_loss: 0.6342\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5990 - val_loss: 0.6348\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5984 - val_loss: 0.6539\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.5999 - val_loss: 0.6288\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5999 - val_loss: 0.6391\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5960 - val_loss: 0.6376\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5964 - val_loss: 0.6322\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5969 - val_loss: 0.6362\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5973 - val_loss: 0.6330\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5989 - val_loss: 0.6357\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5958 - val_loss: 0.6270\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5978 - val_loss: 0.6315\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5969 - val_loss: 0.6293\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5957 - val_loss: 0.6412\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5946 - val_loss: 0.6503\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5945 - val_loss: 0.6346\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5959 - val_loss: 0.6293\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5930 - val_loss: 0.6259\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5950 - val_loss: 0.6321\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5960 - val_loss: 0.6312\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5918 - val_loss: 0.6251\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5938 - val_loss: 0.6616\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5924 - val_loss: 0.6328\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5925 - val_loss: 0.6338\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5921 - val_loss: 0.6500\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5932 - val_loss: 0.6440\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5899 - val_loss: 0.6323\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5918 - val_loss: 0.6274\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5903 - val_loss: 0.6421\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5901 - val_loss: 0.6274\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5912 - val_loss: 0.6335\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5893 - val_loss: 0.6257\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5920 - val_loss: 0.6395\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5920 - val_loss: 0.6241\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5900 - val_loss: 0.6296\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5907 - val_loss: 0.6282\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5885 - val_loss: 0.6323\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5887 - val_loss: 0.6276\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5890 - val_loss: 0.6430\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5885 - val_loss: 0.6396\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5893 - val_loss: 0.6286\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5897 - val_loss: 0.6284\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5871 - val_loss: 0.6418\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5891 - val_loss: 0.6219\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5874 - val_loss: 0.6289\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 197us/step - loss: 5.9139 - val_loss: 0.7773\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7770 - val_loss: 0.7453\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7653 - val_loss: 0.7350\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7577 - val_loss: 0.7355\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7518 - val_loss: 0.7354\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7448 - val_loss: 0.7286\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7400 - val_loss: 0.7246\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7330 - val_loss: 0.7240\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7275 - val_loss: 0.7257\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7206 - val_loss: 0.7116\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7155 - val_loss: 0.7206\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7129 - val_loss: 0.7031\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7058 - val_loss: 0.7202\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7040 - val_loss: 0.7169\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6992 - val_loss: 0.7004\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6960 - val_loss: 0.6989\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6936 - val_loss: 0.6990\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6882 - val_loss: 0.6897\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6856 - val_loss: 0.6844\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6842 - val_loss: 0.6819\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6814 - val_loss: 0.6815\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6776 - val_loss: 0.6829\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6770 - val_loss: 0.6856\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6736 - val_loss: 0.6863\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6714 - val_loss: 0.6753\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6661 - val_loss: 0.6801\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6664 - val_loss: 0.6760\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6631 - val_loss: 0.6724\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6595 - val_loss: 0.6677\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6584 - val_loss: 0.6831\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6563 - val_loss: 0.6723\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6552 - val_loss: 0.6821\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6546 - val_loss: 0.6766\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6524 - val_loss: 0.6649\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6494 - val_loss: 0.6721\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6509 - val_loss: 0.6906\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6502 - val_loss: 0.6654\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6487 - val_loss: 0.6590\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6442 - val_loss: 0.6611\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6443 - val_loss: 0.6618\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6439 - val_loss: 0.6625\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6429 - val_loss: 0.6564\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6432 - val_loss: 0.6524\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6407 - val_loss: 0.6623\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6394 - val_loss: 0.6535\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6372 - val_loss: 0.6669\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6404 - val_loss: 0.6674\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6368 - val_loss: 0.6950\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6361 - val_loss: 0.6542\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6374 - val_loss: 0.6503\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6347 - val_loss: 0.6576\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6318 - val_loss: 0.6542\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6323 - val_loss: 0.6786\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6321 - val_loss: 0.6565\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6311 - val_loss: 0.6585\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6308 - val_loss: 0.6592\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6316 - val_loss: 0.6713\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6298 - val_loss: 0.6554\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6315 - val_loss: 0.6672\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6278 - val_loss: 0.6513\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6322 - val_loss: 0.6601\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6284 - val_loss: 0.6520\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6276 - val_loss: 0.6570\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6280 - val_loss: 0.6531\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6246 - val_loss: 0.6478\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6236 - val_loss: 0.6556\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6261 - val_loss: 0.6495\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6233 - val_loss: 0.6497\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6241 - val_loss: 0.6485\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6243 - val_loss: 0.6572\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6231 - val_loss: 0.6519\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6234 - val_loss: 0.6490\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6240 - val_loss: 0.6497\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6225 - val_loss: 0.6529\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6227 - val_loss: 0.6466\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6241 - val_loss: 0.6592\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6204 - val_loss: 0.6519\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6225 - val_loss: 0.6492\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6219 - val_loss: 0.6529\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6196 - val_loss: 0.6533\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6195 - val_loss: 0.6465\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6197 - val_loss: 0.7047\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6184 - val_loss: 0.6616\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6185 - val_loss: 0.6470\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6188 - val_loss: 0.6676\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6191 - val_loss: 0.6655\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6183 - val_loss: 0.6516\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6161 - val_loss: 0.6609\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6179 - val_loss: 0.6615\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6180 - val_loss: 0.6478\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6165 - val_loss: 0.6491\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6185 - val_loss: 0.6494\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6165 - val_loss: 0.6555\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6150 - val_loss: 0.6484\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6162 - val_loss: 0.6604\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6165 - val_loss: 0.6533\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6168 - val_loss: 0.6676\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6167 - val_loss: 0.6529\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6157 - val_loss: 0.6555\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6131 - val_loss: 0.6435\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6143 - val_loss: 0.6472\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6159 - val_loss: 0.6464\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6129 - val_loss: 0.6502\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6134 - val_loss: 0.6508\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6163 - val_loss: 0.6451\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6145 - val_loss: 0.6782\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6113 - val_loss: 0.6607\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6149 - val_loss: 0.6463\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6133 - val_loss: 0.6465\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6118 - val_loss: 0.6800\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6117 - val_loss: 0.6513\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6110 - val_loss: 0.6572\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6121 - val_loss: 0.6794\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6111 - val_loss: 0.6481\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6122 - val_loss: 0.6565\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6111 - val_loss: 0.6630\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 178us/step - loss: 0.6110 - val_loss: 0.6487\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6111 - val_loss: 0.6566\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6110 - val_loss: 0.6451\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6131 - val_loss: 0.6605\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6101 - val_loss: 0.6491\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6100 - val_loss: 0.6452\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6093 - val_loss: 0.6433\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6106 - val_loss: 0.6425\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6080 - val_loss: 0.6529\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6101 - val_loss: 0.6687\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6093 - val_loss: 0.6452\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6101 - val_loss: 0.6502\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6084 - val_loss: 0.6586\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6096 - val_loss: 0.6656\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6106 - val_loss: 0.6475\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6086 - val_loss: 0.6465\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6074 - val_loss: 0.6431\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6104 - val_loss: 0.6537\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6088 - val_loss: 0.6449\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6068 - val_loss: 0.6459\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6094 - val_loss: 0.6533\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6081 - val_loss: 0.6497\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6084 - val_loss: 0.6466\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6074 - val_loss: 0.6507\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6083 - val_loss: 0.6515\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6073 - val_loss: 0.6605\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6079 - val_loss: 0.6455\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6087 - val_loss: 0.6472\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6074 - val_loss: 0.6438\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6077 - val_loss: 0.6577\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6062 - val_loss: 0.6474\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6071 - val_loss: 0.6482\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6061 - val_loss: 0.6633\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6054 - val_loss: 0.6473\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 201us/step - loss: 6.0223 - val_loss: 0.8004\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.7722 - val_loss: 0.7658\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.7585 - val_loss: 0.7533\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.7551 - val_loss: 0.7471\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.7499 - val_loss: 0.7413\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.7438 - val_loss: 0.7418\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7383 - val_loss: 0.7755\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7333 - val_loss: 0.7373\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7234 - val_loss: 0.7172\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7188 - val_loss: 0.7195\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7104 - val_loss: 0.7208\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.7062 - val_loss: 0.7015\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6992 - val_loss: 0.7199\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6932 - val_loss: 0.7104\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6868 - val_loss: 0.6911\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6839 - val_loss: 0.7256\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6798 - val_loss: 0.6802\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6771 - val_loss: 0.6926\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6705 - val_loss: 0.6764\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6666 - val_loss: 0.6758\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6649 - val_loss: 0.6785\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6582 - val_loss: 0.7072\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6579 - val_loss: 0.6632\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6548 - val_loss: 0.6674\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6527 - val_loss: 0.6661\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6502 - val_loss: 0.6645\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6487 - val_loss: 0.6956\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6456 - val_loss: 0.6649\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6446 - val_loss: 0.6749\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6439 - val_loss: 0.6631\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6422 - val_loss: 0.6617\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6392 - val_loss: 0.6634\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6398 - val_loss: 0.6568\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6400 - val_loss: 0.6690\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6369 - val_loss: 0.6571\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6347 - val_loss: 0.6566\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6329 - val_loss: 0.6570\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6341 - val_loss: 0.6747\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6340 - val_loss: 0.6675\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6319 - val_loss: 0.6620\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6312 - val_loss: 0.6558\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6282 - val_loss: 0.6652\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6248 - val_loss: 0.6556\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6285 - val_loss: 0.6534\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6289 - val_loss: 0.6565\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6261 - val_loss: 0.6660\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6263 - val_loss: 0.6491\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6244 - val_loss: 0.6512\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6235 - val_loss: 0.6537\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6233 - val_loss: 0.6531\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6231 - val_loss: 0.6506\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6244 - val_loss: 0.6504\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6210 - val_loss: 0.6597\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6219 - val_loss: 0.6581\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6207 - val_loss: 0.6537\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6201 - val_loss: 0.6553\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6189 - val_loss: 0.6492\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6184 - val_loss: 0.6497\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6171 - val_loss: 0.6600\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6186 - val_loss: 0.6666\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6163 - val_loss: 0.6466\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6177 - val_loss: 0.6508\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6179 - val_loss: 0.6512\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6172 - val_loss: 0.6493\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6169 - val_loss: 0.6739\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6156 - val_loss: 0.6602\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6167 - val_loss: 0.6510\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6157 - val_loss: 0.6474\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6152 - val_loss: 0.6509\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6159 - val_loss: 0.6487\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6145 - val_loss: 0.6583\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6129 - val_loss: 0.6723\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6142 - val_loss: 0.6675\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6116 - val_loss: 0.6502\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6123 - val_loss: 0.6471\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6107 - val_loss: 0.6442\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6128 - val_loss: 0.6440\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6137 - val_loss: 0.6451\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6095 - val_loss: 0.6526\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6109 - val_loss: 0.6474\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6106 - val_loss: 0.6453\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6117 - val_loss: 0.6494\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6097 - val_loss: 0.6495\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6105 - val_loss: 0.6502\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6108 - val_loss: 0.6501\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6107 - val_loss: 0.6466\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6085 - val_loss: 0.6507\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6083 - val_loss: 0.6543\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6109 - val_loss: 0.6470\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6098 - val_loss: 0.6429\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6085 - val_loss: 0.6430\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6102 - val_loss: 0.6436\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6087 - val_loss: 0.6530\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6099 - val_loss: 0.6678\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6101 - val_loss: 0.6450\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6058 - val_loss: 0.6428\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6085 - val_loss: 0.6509\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6079 - val_loss: 0.6545\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6070 - val_loss: 0.6559\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6081 - val_loss: 0.6834\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6062 - val_loss: 0.6541\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6063 - val_loss: 0.6519\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6056 - val_loss: 0.6472\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6068 - val_loss: 0.6451\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6055 - val_loss: 0.6434\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6038 - val_loss: 0.6561\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6038 - val_loss: 0.6617\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6060 - val_loss: 0.6520\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6052 - val_loss: 0.6423\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6040 - val_loss: 0.6490\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6046 - val_loss: 0.6421\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6077 - val_loss: 0.6498\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6058 - val_loss: 0.6659\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6059 - val_loss: 0.6440\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6035 - val_loss: 0.6580\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6050 - val_loss: 0.6502\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6043 - val_loss: 0.6467\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6037 - val_loss: 0.6470\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6036 - val_loss: 0.6476\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6033 - val_loss: 0.6509\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 178us/step - loss: 0.6054 - val_loss: 0.6455\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6029 - val_loss: 0.6509\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6020 - val_loss: 0.6467\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6029 - val_loss: 0.6647\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6023 - val_loss: 0.6416\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6027 - val_loss: 0.6545\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6025 - val_loss: 0.6393\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6022 - val_loss: 0.6487\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6022 - val_loss: 0.6489\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6023 - val_loss: 0.6414\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6014 - val_loss: 0.6448\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6018 - val_loss: 0.6836\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6020 - val_loss: 0.6595\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6025 - val_loss: 0.6543\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6020 - val_loss: 0.6600\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6024 - val_loss: 0.6496\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6000 - val_loss: 0.6419\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6014 - val_loss: 0.6423\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6014 - val_loss: 0.6468\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.5992 - val_loss: 0.6459\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.5999 - val_loss: 0.6416\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6006 - val_loss: 0.6475\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6024 - val_loss: 0.6401\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6031 - val_loss: 0.6438\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6017 - val_loss: 0.6467\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6006 - val_loss: 0.6395\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6011 - val_loss: 0.6540\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6003 - val_loss: 0.6469\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6013 - val_loss: 0.6449\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6017 - val_loss: 0.6392\n"
     ]
    }
   ],
   "source": [
    "# To save time: always apply PCA for now\n",
    "apply_pca = True\n",
    "apply_iqr = [False, True]\n",
    "n_samples = [5000, 20000] #[5000, 20000]\n",
    "n_iter = 4 #5\n",
    "\n",
    "noiqr_5000 = []\n",
    "noiqr_20000 = []\n",
    "iqr_5000 = []\n",
    "iqr_20000 = []\n",
    "\n",
    "for b in apply_iqr:\n",
    "    for n in n_samples:\n",
    "        for i in range(n_iter):\n",
    "            # load each time a different set (kind of cross-val)\n",
    "            X_train, X_test, y_train, y_test = load_data_train_test(n, X_tot, y_tot, iqr=b)\n",
    "            \n",
    "            # PCA\n",
    "            if apply_pca:\n",
    "                n_pcs = 80\n",
    "                X_train, X_test = do_PCA(X_train, X_test, n_pcs)\n",
    "                #nb_input_neurons = n\n",
    "            \n",
    "            # NN\n",
    "            k_ann = k_models.model_6(X_train, 'mean_absolute_error') \n",
    "            k_ann.fit(X_train, y_train, epochs=150, batch_size=10, validation_split=0.2)\n",
    "            \n",
    "            y_hat = k_ann.predict(X_test)\n",
    "            mse_nn, mae_nn = compute_score(y_test, y_hat)\n",
    "            \n",
    "            # Store result\n",
    "            # Case: iqr, 5000 samples\n",
    "            if (b and (n == n_samples[0])):\n",
    "                iqr_5000.append(mse_nn)\n",
    "            elif ((not b) and (n == n_samples[0])):\n",
    "                noiqr_5000.append(mse_nn)\n",
    "            elif (b and (n == n_samples[1])):\n",
    "                iqr_20000.append(mse_nn)\n",
    "            else:\n",
    "                noiqr_20000.append(mse_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-5, 65), Text(0.5, 0, 'Shielding')]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "sns.set(font_scale=1) \n",
    "plt.subplot(2,2,1)\n",
    "plt.title('Shielding distribution \\n (w/o outlier removal)', fontsize=15)\n",
    "sns.distplot(y_tot).set(xlim=(-5,65),ylim=(0,0.14))\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('Shielding distribution \\n (after applying IQR)', fontsize=15)\n",
    "sns.distplot(y_filtered).set(xlim=(-5,65),ylim=(0,0.14))\n",
    "plt.subplot(2,2,3)\n",
    "b1 = sns.boxplot(x=y_tot, linewidth=1.5).set(xlabel='Shielding', xlim=(-5,65))\n",
    "#b1.set_xlabel(\"Shielding\",fontsize=10)\n",
    "plt.subplot(2,2,4)\n",
    "sns.boxplot(x=y_filtered, linewidth=1.5).set(xlabel='Shielding', xlim=(-5,65))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"iqr.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noiqr_5000</th>\n",
       "      <th>noiqr_20000</th>\n",
       "      <th>iqr_5000</th>\n",
       "      <th>iqr_20000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.966410</td>\n",
       "      <td>1.164088</td>\n",
       "      <td>0.891626</td>\n",
       "      <td>0.832680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.286497</td>\n",
       "      <td>1.257255</td>\n",
       "      <td>0.887988</td>\n",
       "      <td>0.744232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.757331</td>\n",
       "      <td>0.852787</td>\n",
       "      <td>1.002124</td>\n",
       "      <td>0.833200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.238798</td>\n",
       "      <td>1.663109</td>\n",
       "      <td>1.009964</td>\n",
       "      <td>0.722377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   noiqr_5000  noiqr_20000  iqr_5000  iqr_20000\n",
       "0    1.966410     1.164088  0.891626   0.832680\n",
       "1    1.286497     1.257255  0.887988   0.744232\n",
       "2    1.757331     0.852787  1.002124   0.833200\n",
       "3    1.238798     1.663109  1.009964   0.722377"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iqr = pd.DataFrame({\n",
    "    'noiqr_5000': noiqr_5000,\n",
    "    'noiqr_20000': noiqr_20000,\n",
    "    'iqr_5000': iqr_5000,\n",
    "    'iqr_20000': iqr_20000\n",
    "})\n",
    "df_iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>iqr</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.966410</td>\n",
       "      <td>no</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.286497</td>\n",
       "      <td>no</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.757331</td>\n",
       "      <td>no</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.238798</td>\n",
       "      <td>no</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.164088</td>\n",
       "      <td>no</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.257255</td>\n",
       "      <td>no</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.852787</td>\n",
       "      <td>no</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.663109</td>\n",
       "      <td>no</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.891626</td>\n",
       "      <td>yes</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.887988</td>\n",
       "      <td>yes</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.002124</td>\n",
       "      <td>yes</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.009964</td>\n",
       "      <td>yes</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.832680</td>\n",
       "      <td>yes</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.744232</td>\n",
       "      <td>yes</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.833200</td>\n",
       "      <td>yes</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.722377</td>\n",
       "      <td>yes</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mse  iqr      n\n",
       "0   1.966410   no   5000\n",
       "1   1.286497   no   5000\n",
       "2   1.757331   no   5000\n",
       "3   1.238798   no   5000\n",
       "4   1.164088   no  20000\n",
       "5   1.257255   no  20000\n",
       "6   0.852787   no  20000\n",
       "7   1.663109   no  20000\n",
       "8   0.891626  yes   5000\n",
       "9   0.887988  yes   5000\n",
       "10  1.002124  yes   5000\n",
       "11  1.009964  yes   5000\n",
       "12  0.832680  yes  20000\n",
       "13  0.744232  yes  20000\n",
       "14  0.833200  yes  20000\n",
       "15  0.722377  yes  20000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_iqr_n = noiqr_5000 + noiqr_20000 + iqr_5000 + iqr_20000\n",
    "\n",
    "iqrs_labels = ['no','no','no','no','no','no','no','no','yes','yes','yes','yes','yes','yes','yes','yes']\n",
    "n_labels = [5000,5000,5000,5000,20000,20000,20000,20000,5000,5000,5000,5000,20000,20000,20000,20000]\n",
    "\n",
    "my_df = pd.DataFrame({\n",
    "    'mse': tot_iqr_n,\n",
    "    'iqr': iqrs_labels,\n",
    "    'n': n_labels\n",
    "})\n",
    "my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iqr_vs_n, _ = plt.subplots(figsize=(10,8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "ax = sns.boxplot(data=my_df, x='iqr', y='mse', hue='n', linewidth=1.)\n",
    "ax.set_xticklabels(['No IQR', 'IQR'], fontsize=20)\n",
    "ax.set_xlabel('', fontsize=20)\n",
    "ax.set_ylabel('MSE', fontsize=20)\n",
    "\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='20')\n",
    "plt.setp(ax.get_legend().get_title(), fontsize='28')\n",
    "\n",
    "for patch in ax.artists:\n",
    "    r, g, b, a = patch.get_facecolor()\n",
    "    patch.set_facecolor((r, g, b, .75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_vs_n.savefig(\"iqr_vs_n_2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Min/max Scaling\n",
    "<a id='minmax'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minmax scaling seems as an appropriate way to scale our data only if an outlier removing method is applied beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot of min/max scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Data Normalization\n",
    "<a id='minmax'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot of normalized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Standardization\n",
    "<a id='std'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our data definitely does not follow a normal distribution, standardization has not been taken into account for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 PCA\n",
    "<a id='pca'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_X,data_y = hl.load_data(100,tot_data_X,tot_data_Y)\n",
    "hl.plot_PCA(100,data_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 2.4 Feature Selection\n",
    "<a id='feat_sel'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.3 Univariate feature selection\n",
    "<a id='un_feat_sel'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running univariate feature selection on the laptops turned out to be extremely costly: this is why we performed it after running a PCA on the trained subset of data. This was before having access to more computational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,axes = plt.subplots(2,2,figsize = (20,10))\n",
    "for i in range(4):\n",
    "    data_X,data_y = hl.load_data(100,tot_data_X,tot_data_Y)\n",
    "    pca = PCA(n_components = 100)\n",
    "    pca.fit(data_X)\n",
    "    data_X_pca = pca.transform(data_X)\n",
    "    hl.univ_feat_sel(data_X_pca,data_y,ax = axes[np.unravel_index(i,(2,2))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the importance of the various PCs is not regular across various random samples for data_X: this is probably due to the fact that the PCs are not the same for a different subset of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.4 Recursive feature elimination\n",
    "<a id='rec_feat_sel'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS turns out to be extensively demanding in computational power: it crashed many times on our laptops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 3\n",
      "Selected Features: [False False False ... False False False]\n",
      "Feature Ranking: [   94 13683 13650 ...  7192  6586  5788]\n"
     ]
    }
   ],
   "source": [
    "#almost impossible without a pca ==> pca first\n",
    "data_X,data_y = hl.load_data(60,tot_data_X,tot_data_Y)\n",
    "pca = PCA(n_components = 3)\n",
    "pca.fit(data_X)\n",
    "data_X_pca = pca.transform(data_X)\n",
    "ridge = Ridge(alpha = 0.1)\n",
    "rfe = RFE(ridge,0.1)\n",
    "fit = rfe.fit(data_X,data_y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 2.5 Linear Models Optimization\n",
    "<a id='models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X,data_y = hl.load_data(100,tot_data_X,tot_data_Y)\n",
    "interv = np.logspace(np.log10(0.00001),np.log10(10000),10)\n",
    "#parameter for the kfold\n",
    "K = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alph_opt = lm.test_alphas_meth(Ridge,interv,data_X,data_y,k = K)\n",
    "print(alph_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.638375670845235, tolerance: 0.066077814875\n",
      "  positive)\n",
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4224873284017687, tolerance: 0.0647251395\n",
      "  positive)\n",
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.356063655054948, tolerance: 0.059179989875000005\n",
      "  positive)\n",
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.3393515366652227, tolerance: 0.048420355500000005\n",
      "  positive)\n",
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.3947809913391342, tolerance: 0.06614936800000001\n",
      "  positive)\n",
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.3702519757049725, tolerance: 0.066077814875\n",
      "  positive)\n",
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.6652205163165394, tolerance: 0.0647251395\n",
      "  positive)\n",
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.375761571872129, tolerance: 0.059179989875000005\n",
      "  positive)\n",
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0260479643455316, tolerance: 0.048420355500000005\n",
      "  positive)\n",
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.265945357864682, tolerance: 0.06614936800000001\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alph_opt = lm.test_alphas_meth(Lasso,interv,data_X,data_y,k = K,)\n",
    "print(alph_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3 Elastic Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.378006706015026, tolerance: 0.066077814875\n",
      "  positive)\n",
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.792103649274598, tolerance: 0.0647251395\n",
      "  positive)\n",
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.015482076080817, tolerance: 0.059179989875000005\n",
      "  positive)\n",
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.832322242872916, tolerance: 0.048420355500000005\n",
      "  positive)\n",
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.107859131025182, tolerance: 0.06614936800000001\n",
      "  positive)\n",
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.329627203169949, tolerance: 0.066077814875\n",
      "  positive)\n",
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.827704057634037, tolerance: 0.0647251395\n",
      "  positive)\n",
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.6690601668650835, tolerance: 0.059179989875000005\n",
      "  positive)\n",
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.714399585331222, tolerance: 0.048420355500000005\n",
      "  positive)\n",
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.335524171932892, tolerance: 0.06614936800000001\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alph_opt = lm.test_alphas_meth(ElasticNet,interv,data_X,data_y,k = K)\n",
    "print(alph_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out = IQR_outlier()\n",
    "min_max = MinMaxScaler\n",
    "pca = PCA(n_components=80)\n",
    "data_X,data_y = out.fit_transform(data_X,data_y)\n",
    "pipeline = Pipeline([('min_max', min_max), ('pca', pca)])\n",
    "data_X = pipeline.fit_transform(data_X)\n",
    "\n",
    "interv = np.logspace(np.log10(0.001),np.log10(10000),25)\n",
    "alph_opt = test_alphas_meth(ElasticNet,interv,data_X,data_y)\n",
    "print(alph_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Neural Networks\n",
    "<a id='pytorch'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Pytorch NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best architecture was reached by GIANNI COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_c.f._ `neural_nets.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Keras NN\n",
    "<a id='keras'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_c.f._ `k_models.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 NN3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terefore we developped a ensemble method taking an object oriented form as a class who could take advantage of the non redondant information of the 3 different datasets. Since the best architecture was the same for the 3 datasets, the class sequentially trains on times the same architecture on each dataset, before \"averaging\" their respecive inputs thanks to the following formula (which is robust to outliers, and assumes that as 2 predictions are close, it is not by chance and that we should not take the third one into account. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: FORMULA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_c.f._ `NN3.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Main\n",
    "<a id='main'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main consists of the different grid search for linear models and trainings for NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Linear Models Pipelines\n",
    "<a id='main_pytorch'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each cell here is meant to do a whole pipeline, from loading a certain number of samples, preprocessing etc. We keep using the R2 score, the MSE and the MAE as our metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORING = ['neg_mean_squared_error', 'neg_mean_absolute_error','r2']\n",
    "path_dic = {'CSD-10k_H_fps_1k_MD_n_12_l_9_rc_3.0_gw_0.3_rsr_1.0_rss_2.5_rse_5':'3angst',\n",
    "           'CSD-10k_H_fps_1k_MD_n_12_l_9_rc_5.0_gw_0.3_rsr_1.0_rss_2.5_rse_5':'5angst',\n",
    "           'CSD-10k_H_fps_1k_MD_n_12_l_9_rc_7.0_gw_0.3_rsr_1.0_rss_2.5_rse_5':'7angst'}\n",
    "\n",
    "data_path = 'CSD-10k_H_fps_1k_MD_n_12_l_9_rc_3.0_gw_0.3_rsr_1.0_rss_2.5_rse_5'\n",
    "n_samples = 100\n",
    "K = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 0: Pure Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log saved\n",
      "On 4 folds\n",
      "Obtained MSE on test set 13.64 \n",
      "Obtained MAE on test set 2.54 \n",
      "Obtained r2 on test set 0.18 \n",
      "{'rid': Ridge(alpha=10.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "      normalize=False, random_state=None, solver='auto', tol=0.001)}\n"
     ]
    }
   ],
   "source": [
    "data_X,data_y = hl.load_data(n_samples,tot_data_X,tot_data_Y)\n",
    "\n",
    "rid = Ridge()\n",
    "pipeline = Pipeline([\n",
    "  ('rid', rid)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'rid__alpha': np.logspace(np.log10(0.00001),np.log10(10),2)\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits = K)\n",
    "search = GridSearchCV(pipeline, param_grid, scoring = 'neg_mean_absolute_error',iid=False, cv=cv)\n",
    "search.fit(data_X,data_y)\n",
    "\n",
    "best_score = search.best_score_\n",
    "bp_ = search.best_params_\n",
    "be_ = search.best_estimator_\n",
    "\n",
    "#refit with the best parameters\n",
    "pipeline.set_params(rid__alpha = bp_['rid__alpha'])\n",
    "\n",
    "results = cross_validate(pipeline,data_X, data_y, cv = cv,scoring=SCORING)\n",
    "lm.log_model(results,pipeline,param_grid,path_dic[data_path])\n",
    "\n",
    "lm.display_score(results)\n",
    "\n",
    "print(search.best_estimator_.named_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log saved\n",
      "On 4 folds\n",
      "Obtained MSE on test set 3.15 \n",
      "Obtained MAE on test set 1.32 \n",
      "Obtained r2 on test set 0.54 \n",
      "{'min_max': MinMaxScaler(copy=True, feature_range=(0, 1)), 'feat_sel': SelectKBest(k=5000, score_func=<function f_regression at 0x7fed035e0560>), 'pca': PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
      "    svd_solver='auto', tol=0.0, whiten=False), 'rid': Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "      normalize=False, random_state=None, solver='auto', tol=0.001)}\n"
     ]
    }
   ],
   "source": [
    "data_X,data_y = hl.load_data(n_samples,tot_data_X,tot_data_Y)\n",
    "\n",
    "data_X,data_y = out.IQR_y_outliers(data_X,data_y)\n",
    "\n",
    "pca = PCA()\n",
    "min_max = MinMaxScaler()\n",
    "rid = Ridge()\n",
    "sel = SelectKBest(score_func=f_regression)\n",
    "pipeline = Pipeline([\n",
    "  ('min_max',min_max),\n",
    "  ('feat_sel', sel),\n",
    "  ('pca',pca),\n",
    "  ('rid', rid)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': [3],\n",
    "    'rid__alpha': np.logspace(np.log10(0.1),np.log10(100),2),\n",
    "    'feat_sel__k': [5000],\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits = K)\n",
    "search = GridSearchCV(pipeline, param_grid, scoring = 'neg_mean_absolute_error',iid=False, cv=cv)\n",
    "search.fit(data_X,data_y)\n",
    "\n",
    "best_score = search.best_score_\n",
    "bp_ = search.best_params_\n",
    "be_ = search.best_estimator_\n",
    "\n",
    "#refit with the best parameters\n",
    "pipeline.set_params(pca__n_components= bp_['pca__n_components'],feat_sel__k = bp_['feat_sel__k'],rid__alpha = bp_['rid__alpha'])\n",
    "\n",
    "results = cross_validate(pipeline,data_X, data_y, cv = cv,scoring=SCORING)\n",
    "lm.log_model(results,pipeline,param_grid,path_dic[data_path])\n",
    "\n",
    "lm.display_score(results)\n",
    "\n",
    "print(search.best_estimator_.named_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log saved\n",
      "On 4 folds\n",
      "Obtained MSE on test set 14.48 \n",
      "Obtained MAE on test set 2.91 \n",
      "Obtained r2 on test set 0.03 \n",
      "{'rob': RobustScaler(copy=True, quantile_range=(15.0, 85.0), with_centering=True,\n",
      "             with_scaling=True), 'pca': PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
      "    svd_solver='auto', tol=0.0, whiten=False), 'rid': Ridge(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "      normalize=False, random_state=None, solver='auto', tol=0.001)}\n"
     ]
    }
   ],
   "source": [
    "#Pipeline 2\n",
    "\n",
    "data_X,data_y = hl.load_data(n_samples,tot_data_X,tot_data_Y)\n",
    "\n",
    "#data_X,data_y = out.IQR_y_outliers(data_X,data_y)\n",
    "\n",
    "pca = PCA()\n",
    "rob = RobustScaler()\n",
    "rid = Ridge()\n",
    "pipeline = Pipeline([\n",
    "  ('rob',rob),\n",
    "  ('pca',pca),\n",
    "  ('rid', rid)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': [3],\n",
    "    'rid__alpha': np.logspace(np.log10(0.001),np.log10(1000),2),\n",
    "    'rob__quantile_range': [(25.0,75.0),(15.0,85.0),(5.0,95.0)]\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits = K)\n",
    "search = GridSearchCV(pipeline, param_grid, scoring = 'neg_mean_absolute_error',iid=False, cv=cv)\n",
    "search.fit(data_X,data_y)\n",
    "\n",
    "best_score = search.best_score_\n",
    "bp_ = search.best_params_\n",
    "be_ = search.best_estimator_\n",
    "\n",
    "pipeline.set_params(pca__n_components= bp_['pca__n_components'],rid__alpha = bp_['rid__alpha'])\n",
    "\n",
    "\n",
    "results = cross_validate(pipeline,data_X, data_y, cv = cv,scoring=SCORING)\n",
    "lm.log_model(results,pipeline,param_grid,path_dic[data_path])\n",
    "\n",
    "lm.display_score(results)\n",
    "\n",
    "print(search.best_estimator_.named_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline 3\n",
    "\n",
    "data_X,data_y = hl.load_data(n_samples,tot_data_X,tot_data_Y)\n",
    "\n",
    "data_X,data_y = out.IQR_y_outliers(data_X,data_y)\n",
    "\n",
    "pca = KernelPCA()\n",
    "min_max = MinMaxScaler()\n",
    "rid = Ridge()\n",
    "pipeline = Pipeline([\n",
    "  ('min_max',min_max),\n",
    "  ('pca',pca),\n",
    "  ('ridge', rid)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': [80, 100],\n",
    "    'ridge__alpha': np.logspace(np.log10(0.001),np.log10(1000),5),\n",
    "    'pca__kernel': ['linear','poly','rbf'],\n",
    "    'pca__degree': [2,4,6],\n",
    "    'pca__gamma': [0.2]\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits = K)\n",
    "search = GridSearchCV(pipeline, param_grid, scoring = 'neg_mean_absolute_error',iid=False, cv=cv)\n",
    "search.fit(data_X,data_y)\n",
    "\n",
    "best_score = search.best_score_\n",
    "bp_ = search.best_params_\n",
    "be_ = search.best_estimator_\n",
    "\n",
    "pipeline.set_params(pca__n_components= bp_['pca__n_components'],ridge__alpha = bp_['ridge__alpha'])\n",
    "\n",
    "results = cross_validate(pipeline,data_X, data_y, cv = cv,scoring=SCORING)\n",
    "lm.log_model(results,pipeline,param_grid,path_dic[data_path])\n",
    "\n",
    "lm.display_score(results)\n",
    "\n",
    "print(search.best_estimator_.named_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline 4\n",
    "\"\"\"\n",
    "# TODO parameters grid still to define clearly for this pipeline\n",
    "data_X,data_y = load_data(n_samples)\n",
    "\n",
    "data_X,data_y = IQR_y_outliers(data_X,data_y)\n",
    "min_max = MinMaxScaler()\n",
    "forest = RandomForestRegressor()\n",
    "boruta = BorutaPy(forest)\n",
    "rid = Ridge()\n",
    "pca = PCA()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "  ('pca',pca),\n",
    "  ('min_max',min_max),\n",
    "  ('boruta',boruta),\n",
    "  ('ridge', rid)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': [80, 100],\n",
    "    'ridge__alpha': np.logspace(np.log10(0.001),np.log10(1000),5),\n",
    "    'boruta__estimator__max_depth': [7],\n",
    "    'boruta__estimator__n_estimators': [10,100]\n",
    "\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits= K)\n",
    "search = GridSearchCV(pipeline, param_grid, scoring = 'neg_mean_absolute_error',iid=False, cv=cv)\n",
    "search.fit(data_X,data_y)\n",
    "\n",
    "best_score = search.best_score_\n",
    "bp_ = search.best_params_\n",
    "be_ = search.best_estimator_\n",
    "\n",
    "pipeline.set_params(pca__n_components = bp_['pca__n_components'],\n",
    "                    ridge__alpha = bp_['ridge__alpha'],\n",
    "                    boruta__estimator__max_depth = bp_['boruta__estimator__max_depth'],\n",
    "                    boruta__estimator__n_estimators = bp_['boruta__estimator__n_estimators'])\n",
    "results = cross_validate(pipeline,data_X, data_y, cv = cv,scoring=SCORING)\n",
    "lm.log_model(results,pipeline,param_grid,path_dic[data_path])\n",
    "\n",
    "lm.display_score(results)\n",
    "\n",
    "print(search.best_estimator_.named_steps)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log saved\n",
      "On 4 folds\n",
      "Obtained MSE on test set 2.03 \n",
      "Obtained MAE on test set 1.00 \n",
      "Obtained r2 on test set 0.71 \n",
      "{'scaler': MinMaxScaler(copy=True, feature_range=(0, 1)), 'elastic_net': ElasticNet(alpha=0.1, copy_X=True, fit_intercept=True, l1_ratio=0.1,\n",
      "           max_iter=2000, normalize=False, positive=False, precompute=False,\n",
      "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)}\n"
     ]
    }
   ],
   "source": [
    "# Pipeline 5\n",
    "# Load data\n",
    "data_X,data_y = hl.load_data(n_samples,tot_data_X,tot_data_Y)\n",
    "data_X, data_y = out.IQR_y_outliers(data_X,data_y)\n",
    "# Define pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()), \n",
    "    ('elastic_net', ElasticNet( max_iter=2000))])\n",
    "\n",
    "# Define a parameter grid (use in gridsearch)\n",
    "param_grid = {\n",
    "    'elastic_net__alpha': [0.1], #don't use 0\n",
    "    'elastic_net__l1_ratio': [0.1] #don't use 0       \n",
    "}\n",
    "\n",
    "cv = KFold(n_splits= K)\n",
    "search = GridSearchCV(pipeline, param_grid, scoring = 'neg_mean_absolute_error',iid=False, cv=cv)\n",
    "\n",
    "search.fit(data_X,data_y)\n",
    "\n",
    "best_score = search.best_score_\n",
    "bp_ = search.best_params_\n",
    "be_ = search.best_estimator_\n",
    "\n",
    "pipeline.set_params(elastic_net__alpha = bp_['elastic_net__alpha'],\n",
    "                   elastic_net__l1_ratio = bp_['elastic_net__l1_ratio'])\n",
    "\n",
    "results = cross_validate(pipeline,data_X, data_y, cv = cv,scoring=SCORING)\n",
    "lm.log_model(results,pipeline,param_grid,path_dic[data_path])\n",
    "\n",
    "lm.display_score(results)\n",
    "\n",
    "print(search.best_estimator_.named_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 PyTorch pipeline\n",
    "<a id='main_pytorch'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_process_data(n, iqr=False, pca=False, scale=False):\n",
    "    if iqr:\n",
    "        X_train, X_test, y_train, y_test = load_data_train_test(n, X_tot, y_tot, iqr=apply_iqr)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = load_data_train_test(n, X_tot, y_tot)\n",
    "        \n",
    "    if scale:\n",
    "        min_max = MinMaxScaler()\n",
    "        X_train, X_test = apply_scaler(min_max, X_train, X_test)\n",
    "        \n",
    "    if pca:\n",
    "        n = 80\n",
    "        X_train, X_test = do_PCA(X_train, X_test, n)\n",
    "        nb_input_neurons = n\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37338, 14400)\n",
      "(37338,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "mse_storage = []\n",
    "mae_storage = []\n",
    "r2_storage = []\n",
    "\n",
    "mini_batch_size = 10\n",
    "nb_input_neurons = 14400\n",
    "\n",
    "# Load data, remove outliers but do not split yet into train and test\n",
    "X, y = load_data(38514, tot_data_x=X_tot, tot_data_y=y_tot, iqr=True)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "[epoch 1] loss: 195956.91\n",
      "[epoch 2] loss: 5717.32\n",
      "[epoch 3] loss: 3410.36\n",
      "[epoch 4] loss: 2653.15\n",
      "[epoch 5] loss: 2306.10\n",
      "[epoch 6] loss: 2093.14\n",
      "[epoch 7] loss: 1942.76\n",
      "[epoch 8] loss: 1829.11\n",
      "[epoch 9] loss: 1739.60\n",
      "[epoch 10] loss: 1666.88\n",
      "[epoch 11] loss: 1606.23\n",
      "[epoch 12] loss: 1554.56\n",
      "[epoch 13] loss: 1509.86\n",
      "[epoch 14] loss: 1470.77\n",
      "[epoch 15] loss: 1436.31\n",
      "[epoch 16] loss: 1405.71\n",
      "[epoch 17] loss: 1378.39\n",
      "[epoch 18] loss: 1353.86\n",
      "[epoch 19] loss: 1331.73\n",
      "[epoch 20] loss: 1311.67\n",
      "[epoch 21] loss: 1293.40\n",
      "[epoch 22] loss: 1276.70\n",
      "[epoch 23] loss: 1261.37\n",
      "[epoch 24] loss: 1247.26\n",
      "[epoch 25] loss: 1234.22\n",
      "[epoch 26] loss: 1222.14\n",
      "[epoch 27] loss: 1210.92\n",
      "[epoch 28] loss: 1200.46\n",
      "[epoch 29] loss: 1190.69\n",
      "[epoch 30] loss: 1181.55\n",
      "[epoch 31] loss: 1172.98\n",
      "[epoch 32] loss: 1164.92\n",
      "[epoch 33] loss: 1157.34\n",
      "[epoch 34] loss: 1150.19\n",
      "[epoch 35] loss: 1143.43\n",
      "[epoch 36] loss: 1137.03\n",
      "[epoch 37] loss: 1130.97\n",
      "[epoch 38] loss: 1125.22\n",
      "[epoch 39] loss: 1119.75\n",
      "[epoch 40] loss: 1114.55\n",
      "[epoch 41] loss: 1109.59\n",
      "[epoch 42] loss: 1104.86\n",
      "[epoch 43] loss: 1100.34\n",
      "[epoch 44] loss: 1096.02\n",
      "[epoch 45] loss: 1091.88\n",
      "[epoch 46] loss: 1087.91\n",
      "[epoch 47] loss: 1084.10\n",
      "[epoch 48] loss: 1080.44\n",
      "[epoch 49] loss: 1076.92\n",
      "[epoch 50] loss: 1073.53\n",
      "[epoch 51] loss: 1070.26\n",
      "[epoch 52] loss: 1067.11\n",
      "[epoch 53] loss: 1064.07\n",
      "[epoch 54] loss: 1061.13\n",
      "[epoch 55] loss: 1058.28\n",
      "[epoch 56] loss: 1055.53\n",
      "[epoch 57] loss: 1052.86\n",
      "[epoch 58] loss: 1050.28\n",
      "[epoch 59] loss: 1047.77\n",
      "[epoch 60] loss: 1045.33\n",
      "[epoch 61] loss: 1042.97\n",
      "[epoch 62] loss: 1040.67\n",
      "[epoch 63] loss: 1038.43\n",
      "[epoch 64] loss: 1036.25\n",
      "[epoch 65] loss: 1034.12\n",
      "[epoch 66] loss: 1032.06\n",
      "[epoch 67] loss: 1030.04\n",
      "[epoch 68] loss: 1028.07\n",
      "[epoch 69] loss: 1026.15\n",
      "[epoch 70] loss: 1024.27\n",
      "[epoch 71] loss: 1022.44\n",
      "[epoch 72] loss: 1020.64\n",
      "[epoch 73] loss: 1018.89\n",
      "[epoch 74] loss: 1017.17\n",
      "[epoch 75] loss: 1015.49\n",
      "[epoch 76] loss: 1013.85\n",
      "[epoch 77] loss: 1012.24\n",
      "[epoch 78] loss: 1010.66\n",
      "[epoch 79] loss: 1009.11\n",
      "[epoch 80] loss: 1007.59\n",
      "[epoch 81] loss: 1006.10\n",
      "[epoch 82] loss: 1004.64\n",
      "[epoch 83] loss: 1003.20\n",
      "[epoch 84] loss: 1001.79\n",
      "[epoch 85] loss: 1000.41\n",
      "[epoch 86] loss: 999.05\n",
      "[epoch 87] loss: 997.71\n",
      "[epoch 88] loss: 996.40\n",
      "[epoch 89] loss: 995.11\n",
      "[epoch 90] loss: 993.84\n",
      "[epoch 91] loss: 992.59\n",
      "[epoch 92] loss: 991.36\n",
      "[epoch 93] loss: 990.15\n",
      "[epoch 94] loss: 988.96\n",
      "[epoch 95] loss: 987.78\n",
      "[epoch 96] loss: 986.63\n",
      "[epoch 97] loss: 985.49\n",
      "[epoch 98] loss: 984.37\n",
      "[epoch 99] loss: 983.27\n",
      "[epoch 100] loss: 982.18\n",
      "[epoch 101] loss: 981.11\n",
      "[epoch 102] loss: 980.05\n",
      "[epoch 103] loss: 979.01\n",
      "[epoch 104] loss: 977.98\n",
      "[epoch 105] loss: 976.96\n",
      "[epoch 106] loss: 975.96\n",
      "[epoch 107] loss: 974.98\n",
      "[epoch 108] loss: 974.00\n",
      "[epoch 109] loss: 973.04\n",
      "[epoch 110] loss: 972.09\n",
      "[epoch 111] loss: 971.16\n",
      "[epoch 112] loss: 970.23\n",
      "[epoch 113] loss: 969.32\n",
      "[epoch 114] loss: 968.42\n",
      "[epoch 115] loss: 967.53\n",
      "[epoch 116] loss: 966.65\n",
      "[epoch 117] loss: 965.79\n",
      "[epoch 118] loss: 964.93\n",
      "[epoch 119] loss: 964.08\n",
      "[epoch 120] loss: 963.24\n",
      "[epoch 121] loss: 962.42\n",
      "[epoch 122] loss: 961.60\n",
      "[epoch 123] loss: 960.79\n",
      "[epoch 124] loss: 959.99\n",
      "[epoch 125] loss: 959.20\n",
      "[epoch 126] loss: 958.42\n",
      "[epoch 127] loss: 957.65\n",
      "[epoch 128] loss: 956.88\n",
      "[epoch 129] loss: 956.13\n",
      "[epoch 130] loss: 955.38\n",
      "[epoch 131] loss: 954.64\n",
      "[epoch 132] loss: 953.91\n",
      "[epoch 133] loss: 953.19\n",
      "[epoch 134] loss: 952.47\n",
      "[epoch 135] loss: 951.76\n",
      "[epoch 136] loss: 951.06\n",
      "[epoch 137] loss: 950.37\n",
      "[epoch 138] loss: 949.68\n",
      "[epoch 139] loss: 949.00\n",
      "[epoch 140] loss: 948.33\n",
      "[epoch 141] loss: 947.66\n",
      "[epoch 142] loss: 947.01\n",
      "[epoch 143] loss: 946.35\n",
      "[epoch 144] loss: 945.71\n",
      "[epoch 145] loss: 945.07\n",
      "[epoch 146] loss: 944.43\n",
      "[epoch 147] loss: 943.80\n",
      "[epoch 148] loss: 943.18\n",
      "[epoch 149] loss: 942.57\n",
      "[epoch 150] loss: 941.96\n",
      "FOLD 2\n",
      "[epoch 1] loss: 206755.53\n",
      "[epoch 2] loss: 5089.68\n",
      "[epoch 3] loss: 2948.42\n",
      "[epoch 4] loss: 2376.50\n",
      "[epoch 5] loss: 2092.57\n",
      "[epoch 6] loss: 1911.44\n",
      "[epoch 7] loss: 1783.12\n",
      "[epoch 8] loss: 1685.23\n",
      "[epoch 9] loss: 1606.86\n",
      "[epoch 10] loss: 1542.26\n",
      "[epoch 11] loss: 1487.61\n",
      "[epoch 12] loss: 1440.78\n",
      "[epoch 13] loss: 1400.17\n",
      "[epoch 14] loss: 1364.95\n",
      "[epoch 15] loss: 1333.94\n",
      "[epoch 16] loss: 1306.22\n",
      "[epoch 17] loss: 1281.45\n",
      "[epoch 18] loss: 1259.13\n",
      "[epoch 19] loss: 1238.88\n",
      "[epoch 20] loss: 1220.54\n",
      "[epoch 21] loss: 1202.21\n",
      "[epoch 22] loss: 1182.42\n",
      "[epoch 23] loss: 1165.37\n",
      "[epoch 24] loss: 1149.02\n",
      "[epoch 25] loss: 1133.33\n",
      "[epoch 26] loss: 1118.36\n",
      "[epoch 27] loss: 1103.90\n",
      "[epoch 28] loss: 1089.83\n",
      "[epoch 29] loss: 1076.33\n",
      "[epoch 30] loss: 1063.30\n",
      "[epoch 31] loss: 1050.63\n",
      "[epoch 32] loss: 1038.49\n",
      "[epoch 33] loss: 1026.78\n",
      "[epoch 34] loss: 1015.59\n",
      "[epoch 35] loss: 1004.75\n",
      "[epoch 36] loss: 994.35\n",
      "[epoch 37] loss: 984.43\n",
      "[epoch 38] loss: 974.80\n",
      "[epoch 39] loss: 965.45\n",
      "[epoch 40] loss: 956.44\n",
      "[epoch 41] loss: 947.95\n",
      "[epoch 42] loss: 939.68\n",
      "[epoch 43] loss: 931.79\n",
      "[epoch 44] loss: 924.18\n",
      "[epoch 45] loss: 916.78\n",
      "[epoch 46] loss: 909.48\n",
      "[epoch 47] loss: 902.35\n",
      "[epoch 48] loss: 895.72\n",
      "[epoch 49] loss: 889.18\n",
      "[epoch 50] loss: 882.67\n",
      "[epoch 51] loss: 876.28\n",
      "[epoch 52] loss: 870.33\n",
      "[epoch 53] loss: 864.39\n",
      "[epoch 54] loss: 858.72\n",
      "[epoch 55] loss: 853.40\n",
      "[epoch 56] loss: 847.89\n",
      "[epoch 57] loss: 842.66\n",
      "[epoch 58] loss: 837.56\n",
      "[epoch 59] loss: 832.67\n",
      "[epoch 60] loss: 827.67\n",
      "[epoch 61] loss: 822.92\n",
      "[epoch 62] loss: 818.12\n",
      "[epoch 63] loss: 813.55\n",
      "[epoch 64] loss: 809.23\n",
      "[epoch 65] loss: 804.82\n",
      "[epoch 66] loss: 800.54\n",
      "[epoch 67] loss: 796.17\n",
      "[epoch 68] loss: 792.10\n",
      "[epoch 69] loss: 788.10\n",
      "[epoch 70] loss: 784.03\n",
      "[epoch 71] loss: 779.94\n",
      "[epoch 72] loss: 776.40\n",
      "[epoch 73] loss: 772.46\n",
      "[epoch 74] loss: 768.67\n",
      "[epoch 75] loss: 765.00\n",
      "[epoch 76] loss: 761.10\n",
      "[epoch 77] loss: 757.13\n",
      "[epoch 78] loss: 753.31\n",
      "[epoch 79] loss: 748.91\n",
      "[epoch 80] loss: 745.04\n",
      "[epoch 81] loss: 740.90\n",
      "[epoch 82] loss: 737.17\n",
      "[epoch 83] loss: 733.24\n",
      "[epoch 84] loss: 729.25\n",
      "[epoch 85] loss: 725.55\n",
      "[epoch 86] loss: 722.09\n",
      "[epoch 87] loss: 718.50\n",
      "[epoch 88] loss: 714.74\n",
      "[epoch 89] loss: 711.17\n",
      "[epoch 90] loss: 707.72\n",
      "[epoch 91] loss: 704.12\n",
      "[epoch 92] loss: 700.78\n",
      "[epoch 93] loss: 697.49\n",
      "[epoch 94] loss: 694.08\n",
      "[epoch 95] loss: 690.78\n",
      "[epoch 96] loss: 687.65\n",
      "[epoch 97] loss: 684.76\n",
      "[epoch 98] loss: 681.39\n",
      "[epoch 99] loss: 678.35\n",
      "[epoch 100] loss: 675.28\n",
      "[epoch 101] loss: 672.62\n",
      "[epoch 102] loss: 669.50\n",
      "[epoch 103] loss: 666.75\n",
      "[epoch 104] loss: 663.73\n",
      "[epoch 105] loss: 661.04\n",
      "[epoch 106] loss: 658.24\n",
      "[epoch 107] loss: 655.83\n",
      "[epoch 108] loss: 653.00\n",
      "[epoch 109] loss: 650.60\n",
      "[epoch 110] loss: 648.32\n",
      "[epoch 111] loss: 645.68\n",
      "[epoch 112] loss: 643.23\n",
      "[epoch 113] loss: 640.86\n",
      "[epoch 114] loss: 638.46\n",
      "[epoch 115] loss: 636.25\n",
      "[epoch 116] loss: 634.05\n",
      "[epoch 117] loss: 631.80\n",
      "[epoch 118] loss: 629.59\n",
      "[epoch 119] loss: 627.28\n",
      "[epoch 120] loss: 625.15\n",
      "[epoch 121] loss: 622.89\n",
      "[epoch 122] loss: 621.13\n",
      "[epoch 123] loss: 618.82\n",
      "[epoch 124] loss: 616.61\n",
      "[epoch 125] loss: 614.71\n",
      "[epoch 126] loss: 612.18\n",
      "[epoch 127] loss: 610.88\n",
      "[epoch 128] loss: 608.41\n",
      "[epoch 129] loss: 606.73\n",
      "[epoch 130] loss: 604.99\n",
      "[epoch 131] loss: 603.00\n",
      "[epoch 132] loss: 601.28\n",
      "[epoch 133] loss: 600.00\n",
      "[epoch 134] loss: 597.98\n",
      "[epoch 135] loss: 596.07\n",
      "[epoch 136] loss: 594.31\n",
      "[epoch 137] loss: 592.43\n",
      "[epoch 138] loss: 591.28\n",
      "[epoch 139] loss: 589.43\n",
      "[epoch 140] loss: 587.77\n",
      "[epoch 141] loss: 586.24\n",
      "[epoch 142] loss: 584.70\n",
      "[epoch 143] loss: 583.03\n",
      "[epoch 144] loss: 581.17\n",
      "[epoch 145] loss: 579.89\n",
      "[epoch 146] loss: 578.32\n",
      "[epoch 147] loss: 576.88\n",
      "[epoch 148] loss: 575.20\n",
      "[epoch 149] loss: 573.49\n",
      "[epoch 150] loss: 571.87\n",
      "FOLD 3\n",
      "[epoch 1] loss: 201723.32\n",
      "[epoch 2] loss: 5839.57\n",
      "[epoch 3] loss: 3380.44\n",
      "[epoch 4] loss: 2616.68\n",
      "[epoch 5] loss: 2272.29\n",
      "[epoch 6] loss: 2063.28\n",
      "[epoch 7] loss: 1918.59\n",
      "[epoch 8] loss: 1811.02\n",
      "[epoch 9] loss: 1726.68\n",
      "[epoch 10] loss: 1657.81\n",
      "[epoch 11] loss: 1600.05\n",
      "[epoch 12] loss: 1550.71\n",
      "[epoch 13] loss: 1508.01\n",
      "[epoch 14] loss: 1470.66\n",
      "[epoch 15] loss: 1437.71\n",
      "[epoch 16] loss: 1408.42\n",
      "[epoch 17] loss: 1382.20\n",
      "[epoch 18] loss: 1358.60\n",
      "[epoch 19] loss: 1337.23\n",
      "[epoch 20] loss: 1317.81\n",
      "[epoch 21] loss: 1300.06\n",
      "[epoch 22] loss: 1283.79\n",
      "[epoch 23] loss: 1268.82\n",
      "[epoch 24] loss: 1255.00\n",
      "[epoch 25] loss: 1242.20\n",
      "[epoch 26] loss: 1230.33\n",
      "[epoch 27] loss: 1219.28\n",
      "[epoch 28] loss: 1208.98\n",
      "[epoch 29] loss: 1199.35\n",
      "[epoch 30] loss: 1190.34\n",
      "[epoch 31] loss: 1181.88\n",
      "[epoch 32] loss: 1173.94\n",
      "[epoch 33] loss: 1166.46\n",
      "[epoch 34] loss: 1159.42\n",
      "[epoch 35] loss: 1152.76\n",
      "[epoch 36] loss: 1146.46\n",
      "[epoch 37] loss: 1140.50\n",
      "[epoch 38] loss: 1134.83\n",
      "[epoch 39] loss: 1129.46\n",
      "[epoch 40] loss: 1124.34\n",
      "[epoch 41] loss: 1119.47\n",
      "[epoch 42] loss: 1114.82\n",
      "[epoch 43] loss: 1110.38\n",
      "[epoch 44] loss: 1106.13\n",
      "[epoch 45] loss: 1102.06\n",
      "[epoch 46] loss: 1098.16\n",
      "[epoch 47] loss: 1094.41\n",
      "[epoch 48] loss: 1090.81\n",
      "[epoch 49] loss: 1087.35\n",
      "[epoch 50] loss: 1084.01\n",
      "[epoch 51] loss: 1080.80\n",
      "[epoch 52] loss: 1077.70\n",
      "[epoch 53] loss: 1074.70\n",
      "[epoch 54] loss: 1071.80\n",
      "[epoch 55] loss: 1069.00\n",
      "[epoch 56] loss: 1066.28\n",
      "[epoch 57] loss: 1063.65\n",
      "[epoch 58] loss: 1061.10\n",
      "[epoch 59] loss: 1058.63\n",
      "[epoch 60] loss: 1056.22\n",
      "[epoch 61] loss: 1053.88\n",
      "[epoch 62] loss: 1051.61\n",
      "[epoch 63] loss: 1049.39\n",
      "[epoch 64] loss: 1047.23\n",
      "[epoch 65] loss: 1045.13\n",
      "[epoch 66] loss: 1043.08\n",
      "[epoch 67] loss: 1041.08\n",
      "[epoch 68] loss: 1039.13\n",
      "[epoch 69] loss: 1037.22\n",
      "[epoch 70] loss: 1035.36\n",
      "[epoch 71] loss: 1033.54\n",
      "[epoch 72] loss: 1031.76\n",
      "[epoch 73] loss: 1030.01\n",
      "[epoch 74] loss: 1028.31\n",
      "[epoch 75] loss: 1026.64\n",
      "[epoch 76] loss: 1025.00\n",
      "[epoch 77] loss: 1023.39\n",
      "[epoch 78] loss: 1021.82\n",
      "[epoch 79] loss: 1020.28\n",
      "[epoch 80] loss: 1018.77\n",
      "[epoch 81] loss: 1017.28\n",
      "[epoch 82] loss: 1015.82\n",
      "[epoch 83] loss: 1014.39\n",
      "[epoch 84] loss: 1012.98\n",
      "[epoch 85] loss: 1011.60\n",
      "[epoch 86] loss: 1010.24\n",
      "[epoch 87] loss: 1008.91\n",
      "[epoch 88] loss: 1007.59\n",
      "[epoch 89] loss: 1006.30\n",
      "[epoch 90] loss: 1005.03\n",
      "[epoch 91] loss: 1003.78\n",
      "[epoch 92] loss: 1002.55\n",
      "[epoch 93] loss: 1001.34\n",
      "[epoch 94] loss: 1000.15\n",
      "[epoch 95] loss: 998.97\n",
      "[epoch 96] loss: 997.81\n",
      "[epoch 97] loss: 996.67\n",
      "[epoch 98] loss: 995.55\n",
      "[epoch 99] loss: 994.44\n",
      "[epoch 100] loss: 993.35\n",
      "[epoch 101] loss: 992.27\n",
      "[epoch 102] loss: 991.21\n",
      "[epoch 103] loss: 990.17\n",
      "[epoch 104] loss: 989.13\n",
      "[epoch 105] loss: 988.11\n",
      "[epoch 106] loss: 987.11\n",
      "[epoch 107] loss: 986.12\n",
      "[epoch 108] loss: 985.14\n",
      "[epoch 109] loss: 984.17\n",
      "[epoch 110] loss: 983.22\n",
      "[epoch 111] loss: 982.28\n",
      "[epoch 112] loss: 981.35\n",
      "[epoch 113] loss: 980.43\n",
      "[epoch 114] loss: 979.52\n",
      "[epoch 115] loss: 978.63\n",
      "[epoch 116] loss: 977.74\n",
      "[epoch 117] loss: 976.87\n",
      "[epoch 118] loss: 976.00\n",
      "[epoch 119] loss: 975.15\n",
      "[epoch 120] loss: 974.31\n",
      "[epoch 121] loss: 973.47\n",
      "[epoch 122] loss: 972.65\n",
      "[epoch 123] loss: 971.84\n",
      "[epoch 124] loss: 971.03\n",
      "[epoch 125] loss: 970.22\n",
      "[epoch 126] loss: 969.44\n",
      "[epoch 127] loss: 968.65\n",
      "[epoch 128] loss: 967.88\n",
      "[epoch 129] loss: 967.09\n",
      "[epoch 130] loss: 966.33\n",
      "[epoch 131] loss: 965.49\n",
      "[epoch 132] loss: 964.67\n",
      "[epoch 133] loss: 963.84\n",
      "[epoch 134] loss: 963.02\n",
      "[epoch 135] loss: 962.19\n",
      "[epoch 136] loss: 961.36\n",
      "[epoch 137] loss: 960.50\n",
      "[epoch 138] loss: 959.63\n",
      "[epoch 139] loss: 958.83\n",
      "[epoch 140] loss: 957.88\n",
      "[epoch 141] loss: 956.91\n",
      "[epoch 142] loss: 956.06\n",
      "[epoch 143] loss: 955.08\n",
      "[epoch 144] loss: 954.18\n",
      "[epoch 145] loss: 953.02\n",
      "[epoch 146] loss: 951.93\n",
      "[epoch 147] loss: 950.80\n",
      "[epoch 148] loss: 949.59\n",
      "[epoch 149] loss: 948.40\n",
      "[epoch 150] loss: 947.25\n",
      "FOLD 4\n",
      "[epoch 1] loss: 209691.42\n",
      "[epoch 2] loss: 6241.92\n",
      "[epoch 3] loss: 3626.96\n",
      "[epoch 4] loss: 2755.96\n",
      "[epoch 5] loss: 2368.62\n",
      "[epoch 6] loss: 2137.14\n",
      "[epoch 7] loss: 1977.63\n",
      "[epoch 8] loss: 1859.38\n",
      "[epoch 9] loss: 1767.21\n",
      "[epoch 10] loss: 1692.37\n",
      "[epoch 11] loss: 1629.77\n",
      "[epoch 12] loss: 1576.32\n",
      "[epoch 13] loss: 1530.05\n",
      "[epoch 14] loss: 1489.56\n",
      "[epoch 15] loss: 1453.84\n",
      "[epoch 16] loss: 1422.11\n",
      "[epoch 17] loss: 1393.74\n",
      "[epoch 18] loss: 1368.26\n",
      "[epoch 19] loss: 1345.24\n",
      "[epoch 20] loss: 1324.36\n",
      "[epoch 21] loss: 1305.34\n",
      "[epoch 22] loss: 1287.94\n",
      "[epoch 23] loss: 1271.97\n",
      "[epoch 24] loss: 1257.26\n",
      "[epoch 25] loss: 1243.67\n",
      "[epoch 26] loss: 1231.07\n",
      "[epoch 27] loss: 1219.37\n",
      "[epoch 28] loss: 1208.48\n",
      "[epoch 29] loss: 1198.31\n",
      "[epoch 30] loss: 1188.79\n",
      "[epoch 31] loss: 1179.88\n",
      "[epoch 32] loss: 1171.50\n",
      "[epoch 33] loss: 1163.62\n",
      "[epoch 34] loss: 1156.20\n",
      "[epoch 35] loss: 1149.19\n",
      "[epoch 36] loss: 1142.56\n",
      "[epoch 37] loss: 1136.28\n",
      "[epoch 38] loss: 1130.33\n",
      "[epoch 39] loss: 1124.68\n",
      "[epoch 40] loss: 1119.30\n",
      "[epoch 41] loss: 1114.18\n",
      "[epoch 42] loss: 1109.30\n",
      "[epoch 43] loss: 1104.64\n",
      "[epoch 44] loss: 1100.19\n",
      "[epoch 45] loss: 1095.93\n",
      "[epoch 46] loss: 1091.84\n",
      "[epoch 47] loss: 1087.92\n",
      "[epoch 48] loss: 1084.16\n",
      "[epoch 49] loss: 1080.54\n",
      "[epoch 50] loss: 1077.06\n",
      "[epoch 51] loss: 1073.71\n",
      "[epoch 52] loss: 1070.48\n",
      "[epoch 53] loss: 1067.36\n",
      "[epoch 54] loss: 1064.35\n",
      "[epoch 55] loss: 1061.43\n",
      "[epoch 56] loss: 1058.62\n",
      "[epoch 57] loss: 1055.89\n",
      "[epoch 58] loss: 1053.24\n",
      "[epoch 59] loss: 1050.68\n",
      "[epoch 60] loss: 1048.19\n",
      "[epoch 61] loss: 1045.77\n",
      "[epoch 62] loss: 1043.42\n",
      "[epoch 63] loss: 1041.13\n",
      "[epoch 64] loss: 1038.91\n",
      "[epoch 65] loss: 1036.75\n",
      "[epoch 66] loss: 1034.64\n",
      "[epoch 67] loss: 1032.58\n",
      "[epoch 68] loss: 1030.57\n",
      "[epoch 69] loss: 1028.61\n",
      "[epoch 70] loss: 1026.70\n",
      "[epoch 71] loss: 1024.84\n",
      "[epoch 72] loss: 1023.01\n",
      "[epoch 73] loss: 1021.23\n",
      "[epoch 74] loss: 1019.48\n",
      "[epoch 75] loss: 1017.77\n",
      "[epoch 76] loss: 1016.10\n",
      "[epoch 77] loss: 1014.46\n",
      "[epoch 78] loss: 1012.86\n",
      "[epoch 79] loss: 1011.29\n",
      "[epoch 80] loss: 1009.75\n",
      "[epoch 81] loss: 1008.24\n",
      "[epoch 82] loss: 1006.75\n",
      "[epoch 83] loss: 1005.30\n",
      "[epoch 84] loss: 1003.87\n",
      "[epoch 85] loss: 1002.47\n",
      "[epoch 86] loss: 1001.09\n",
      "[epoch 87] loss: 999.74\n",
      "[epoch 88] loss: 998.40\n",
      "[epoch 89] loss: 997.10\n",
      "[epoch 90] loss: 995.81\n",
      "[epoch 91] loss: 994.55\n",
      "[epoch 92] loss: 993.30\n",
      "[epoch 93] loss: 992.08\n",
      "[epoch 94] loss: 990.88\n",
      "[epoch 95] loss: 989.69\n",
      "[epoch 96] loss: 988.53\n",
      "[epoch 97] loss: 987.38\n",
      "[epoch 98] loss: 986.25\n",
      "[epoch 99] loss: 985.13\n",
      "[epoch 100] loss: 984.03\n",
      "[epoch 101] loss: 982.95\n",
      "[epoch 102] loss: 981.89\n",
      "[epoch 103] loss: 980.84\n",
      "[epoch 104] loss: 979.80\n",
      "[epoch 105] loss: 978.78\n",
      "[epoch 106] loss: 977.77\n",
      "[epoch 107] loss: 976.78\n",
      "[epoch 108] loss: 975.80\n",
      "[epoch 109] loss: 974.83\n",
      "[epoch 110] loss: 973.87\n",
      "[epoch 111] loss: 972.93\n",
      "[epoch 112] loss: 972.00\n",
      "[epoch 113] loss: 971.09\n",
      "[epoch 114] loss: 970.18\n",
      "[epoch 115] loss: 969.29\n",
      "[epoch 116] loss: 968.40\n",
      "[epoch 117] loss: 967.53\n",
      "[epoch 118] loss: 966.67\n",
      "[epoch 119] loss: 965.82\n",
      "[epoch 120] loss: 964.98\n",
      "[epoch 121] loss: 964.15\n",
      "[epoch 122] loss: 963.32\n",
      "[epoch 123] loss: 962.51\n",
      "[epoch 124] loss: 961.71\n",
      "[epoch 125] loss: 960.92\n",
      "[epoch 126] loss: 960.14\n",
      "[epoch 127] loss: 959.36\n",
      "[epoch 128] loss: 958.60\n",
      "[epoch 129] loss: 957.84\n",
      "[epoch 130] loss: 957.09\n",
      "[epoch 131] loss: 956.35\n",
      "[epoch 132] loss: 955.62\n",
      "[epoch 133] loss: 954.89\n",
      "[epoch 134] loss: 954.18\n",
      "[epoch 135] loss: 953.47\n",
      "[epoch 136] loss: 952.77\n",
      "[epoch 137] loss: 952.07\n",
      "[epoch 138] loss: 951.39\n",
      "[epoch 139] loss: 950.71\n",
      "[epoch 140] loss: 950.03\n",
      "[epoch 141] loss: 949.37\n",
      "[epoch 142] loss: 948.71\n",
      "[epoch 143] loss: 948.06\n",
      "[epoch 144] loss: 947.41\n",
      "[epoch 145] loss: 946.77\n",
      "[epoch 146] loss: 946.14\n",
      "[epoch 147] loss: 945.51\n",
      "[epoch 148] loss: 944.89\n",
      "[epoch 149] loss: 944.28\n",
      "[epoch 150] loss: 943.67\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "\n",
    "for idx, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(\"FOLD {}\".format(idx+1))\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    train_input = torch.Tensor(X_train)\n",
    "    test_input = torch.Tensor(X_test)\n",
    "    train_target = torch.Tensor(y_train.reshape(len(y_train), 1))\n",
    "    test_target = torch.Tensor(y_test.reshape(len(y_test), 1))\n",
    "    \n",
    "    model = Net_4(nb_input_neurons) \n",
    "    losses = train_model(model, train_input, train_target, mini_batch_size, monitor_loss=True)\n",
    "\n",
    "    #Make predictions\n",
    "    y_hat = compute_pred(model, test_input)\n",
    "    #Compute score\n",
    "    mse_nn, mae_nn, r2_nn = compute_score(y_test, y_hat.detach().numpy())\n",
    "    \n",
    "    mse_storage.append(mse_nn)\n",
    "    mae_storage.append(mae_nn)\n",
    "    r2_storage.append(r2_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('metrics_all_dataset2_net4_4fold.pkl', 'wb') as f:\n",
    "    pickle.dump([mse_storage, mae_storage, r2_storage], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.42 +/- 0.0220 \n",
      "MAE: 0.48 +/- 0.0126 \n",
      "R2: 0.95 +/- 0.0019\n"
     ]
    }
   ],
   "source": [
    "print('MSE: {:0.2f} +/- {:0.4f} \\nMAE: {:0.2f} +/- {:0.4f} \\nR2: {:0.2f} +/- {:0.4f}'.format(np.mean(mse_storage), np.std(mse_storage), np.mean(mae_storage), np.std(mae_storage), np.mean(r2_storage), np.std(r2_storage)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Keras pipeline\n",
    "<a id='main_keras'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1280 samples, validate on 320 samples\n",
      "Epoch 1/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 20.5533 - mean_absolute_error: 20.5534 - val_loss: 10.6779 - val_mean_absolute_error: 10.6779\n",
      "Epoch 2/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 3.4412 - mean_absolute_error: 3.4412 - val_loss: 2.1116 - val_mean_absolute_error: 2.1116\n",
      "Epoch 3/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.8856 - mean_absolute_error: 1.8856 - val_loss: 1.7832 - val_mean_absolute_error: 1.7832\n",
      "Epoch 4/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.6338 - mean_absolute_error: 1.6338 - val_loss: 1.6364 - val_mean_absolute_error: 1.6364\n",
      "Epoch 5/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.4684 - mean_absolute_error: 1.4684 - val_loss: 1.4507 - val_mean_absolute_error: 1.4507\n",
      "Epoch 6/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.3487 - mean_absolute_error: 1.3487 - val_loss: 1.3620 - val_mean_absolute_error: 1.3620\n",
      "Epoch 7/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.2593 - mean_absolute_error: 1.2593 - val_loss: 1.3047 - val_mean_absolute_error: 1.3047\n",
      "Epoch 8/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.1838 - mean_absolute_error: 1.1838 - val_loss: 1.2502 - val_mean_absolute_error: 1.2502\n",
      "Epoch 9/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.1258 - mean_absolute_error: 1.1258 - val_loss: 1.2066 - val_mean_absolute_error: 1.2066\n",
      "Epoch 10/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.0773 - mean_absolute_error: 1.0773 - val_loss: 1.1537 - val_mean_absolute_error: 1.1537\n",
      "Epoch 11/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.0469 - mean_absolute_error: 1.0469 - val_loss: 1.1248 - val_mean_absolute_error: 1.1248\n",
      "Epoch 12/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.0024 - mean_absolute_error: 1.0024 - val_loss: 1.0928 - val_mean_absolute_error: 1.0928\n",
      "Epoch 13/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.9663 - mean_absolute_error: 0.9663 - val_loss: 1.0636 - val_mean_absolute_error: 1.0636\n",
      "Epoch 14/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.9486 - mean_absolute_error: 0.9486 - val_loss: 1.0322 - val_mean_absolute_error: 1.0322\n",
      "Epoch 15/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.9272 - mean_absolute_error: 0.9272 - val_loss: 1.0828 - val_mean_absolute_error: 1.0828\n",
      "Epoch 16/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.9107 - mean_absolute_error: 0.9107 - val_loss: 1.0040 - val_mean_absolute_error: 1.0040\n",
      "Epoch 17/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8827 - mean_absolute_error: 0.8827 - val_loss: 0.9901 - val_mean_absolute_error: 0.9901\n",
      "Epoch 18/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8781 - mean_absolute_error: 0.8781 - val_loss: 0.9749 - val_mean_absolute_error: 0.9749\n",
      "Epoch 19/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8446 - mean_absolute_error: 0.8446 - val_loss: 0.9781 - val_mean_absolute_error: 0.9781\n",
      "Epoch 20/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8415 - mean_absolute_error: 0.8415 - val_loss: 0.9896 - val_mean_absolute_error: 0.9896\n",
      "Epoch 21/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8250 - mean_absolute_error: 0.8250 - val_loss: 0.9458 - val_mean_absolute_error: 0.9458\n",
      "Epoch 22/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8086 - mean_absolute_error: 0.8086 - val_loss: 0.9559 - val_mean_absolute_error: 0.9559\n",
      "Epoch 23/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8053 - mean_absolute_error: 0.8053 - val_loss: 0.9622 - val_mean_absolute_error: 0.9622\n",
      "Epoch 24/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7821 - mean_absolute_error: 0.7821 - val_loss: 0.9340 - val_mean_absolute_error: 0.9340\n",
      "Epoch 25/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7826 - mean_absolute_error: 0.7826 - val_loss: 0.9283 - val_mean_absolute_error: 0.9283\n",
      "Epoch 26/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7803 - mean_absolute_error: 0.7803 - val_loss: 0.9226 - val_mean_absolute_error: 0.9226\n",
      "Epoch 27/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7768 - mean_absolute_error: 0.7768 - val_loss: 0.9319 - val_mean_absolute_error: 0.9319\n",
      "Epoch 28/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7615 - mean_absolute_error: 0.7615 - val_loss: 0.9995 - val_mean_absolute_error: 0.9995\n",
      "Epoch 29/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7684 - mean_absolute_error: 0.7684 - val_loss: 0.9637 - val_mean_absolute_error: 0.9637\n",
      "Epoch 30/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7622 - mean_absolute_error: 0.7622 - val_loss: 0.9131 - val_mean_absolute_error: 0.9131\n",
      "Epoch 31/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7485 - mean_absolute_error: 0.7485 - val_loss: 0.9188 - val_mean_absolute_error: 0.9188\n",
      "Epoch 32/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7424 - mean_absolute_error: 0.7424 - val_loss: 0.9017 - val_mean_absolute_error: 0.9017\n",
      "Epoch 33/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7434 - mean_absolute_error: 0.7434 - val_loss: 0.9331 - val_mean_absolute_error: 0.9331\n",
      "Epoch 34/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7264 - mean_absolute_error: 0.7264 - val_loss: 0.9186 - val_mean_absolute_error: 0.9186\n",
      "Epoch 35/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7198 - mean_absolute_error: 0.7198 - val_loss: 0.9513 - val_mean_absolute_error: 0.9513\n",
      "Epoch 36/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7331 - mean_absolute_error: 0.7331 - val_loss: 0.9123 - val_mean_absolute_error: 0.9123\n",
      "Epoch 37/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7221 - mean_absolute_error: 0.7221 - val_loss: 0.9004 - val_mean_absolute_error: 0.9004\n",
      "Epoch 38/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7271 - mean_absolute_error: 0.7271 - val_loss: 0.8990 - val_mean_absolute_error: 0.8990\n",
      "Epoch 39/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7113 - mean_absolute_error: 0.7113 - val_loss: 0.9280 - val_mean_absolute_error: 0.9280\n",
      "Epoch 40/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7045 - mean_absolute_error: 0.7045 - val_loss: 0.9209 - val_mean_absolute_error: 0.9209\n",
      "Epoch 41/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6925 - mean_absolute_error: 0.6925 - val_loss: 0.8963 - val_mean_absolute_error: 0.8963\n",
      "Epoch 42/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6879 - mean_absolute_error: 0.6879 - val_loss: 0.9023 - val_mean_absolute_error: 0.9023\n",
      "Epoch 43/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6798 - mean_absolute_error: 0.6798 - val_loss: 0.9094 - val_mean_absolute_error: 0.9094\n",
      "Epoch 44/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6825 - mean_absolute_error: 0.6825 - val_loss: 0.9322 - val_mean_absolute_error: 0.9322 - loss: 0.6855 - mean_absolut\n",
      "Epoch 45/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6736 - mean_absolute_error: 0.6736 - val_loss: 0.9072 - val_mean_absolute_error: 0.9072\n",
      "Epoch 46/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6767 - mean_absolute_error: 0.6767 - val_loss: 0.9403 - val_mean_absolute_error: 0.9403\n",
      "Epoch 47/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6700 - mean_absolute_error: 0.6700 - val_loss: 0.9227 - val_mean_absolute_error: 0.9227\n",
      "Epoch 48/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6590 - mean_absolute_error: 0.6590 - val_loss: 0.9006 - val_mean_absolute_error: 0.9006\n",
      "Epoch 49/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6623 - mean_absolute_error: 0.6623 - val_loss: 0.9027 - val_mean_absolute_error: 0.9027\n",
      "Epoch 50/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6640 - mean_absolute_error: 0.6640 - val_loss: 0.8982 - val_mean_absolute_error: 0.8982\n",
      "Epoch 51/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6573 - mean_absolute_error: 0.6573 - val_loss: 0.8995 - val_mean_absolute_error: 0.8995\n",
      "Epoch 52/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6508 - mean_absolute_error: 0.6508 - val_loss: 0.8898 - val_mean_absolute_error: 0.8898\n",
      "Epoch 53/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6592 - mean_absolute_error: 0.6592 - val_loss: 0.9011 - val_mean_absolute_error: 0.9011\n",
      "Epoch 54/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6454 - mean_absolute_error: 0.6454 - val_loss: 0.9154 - val_mean_absolute_error: 0.9154\n",
      "Epoch 55/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6478 - mean_absolute_error: 0.6478 - val_loss: 0.8987 - val_mean_absolute_error: 0.8987\n",
      "Epoch 56/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6349 - mean_absolute_error: 0.6349 - val_loss: 0.8987 - val_mean_absolute_error: 0.8987\n",
      "Epoch 57/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6553 - mean_absolute_error: 0.6553 - val_loss: 0.9073 - val_mean_absolute_error: 0.9073\n",
      "Epoch 58/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6306 - mean_absolute_error: 0.6306 - val_loss: 0.9502 - val_mean_absolute_error: 0.9502\n",
      "Epoch 59/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6256 - mean_absolute_error: 0.6256 - val_loss: 0.8995 - val_mean_absolute_error: 0.8995\n",
      "Epoch 60/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6298 - mean_absolute_error: 0.6298 - val_loss: 0.8885 - val_mean_absolute_error: 0.8885\n",
      "Epoch 61/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6302 - mean_absolute_error: 0.6302 - val_loss: 0.8880 - val_mean_absolute_error: 0.8880\n",
      "Epoch 62/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6201 - mean_absolute_error: 0.6201 - val_loss: 0.8977 - val_mean_absolute_error: 0.8977\n",
      "Epoch 63/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6212 - mean_absolute_error: 0.6212 - val_loss: 0.9090 - val_mean_absolute_error: 0.9090\n",
      "Epoch 64/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6221 - mean_absolute_error: 0.6221 - val_loss: 0.9033 - val_mean_absolute_error: 0.9033\n",
      "Epoch 65/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6105 - mean_absolute_error: 0.6105 - val_loss: 0.9020 - val_mean_absolute_error: 0.9020\n",
      "Epoch 66/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6172 - mean_absolute_error: 0.6172 - val_loss: 0.8895 - val_mean_absolute_error: 0.8895\n",
      "Epoch 67/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6110 - mean_absolute_error: 0.6110 - val_loss: 0.8952 - val_mean_absolute_error: 0.8952\n",
      "Epoch 68/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6133 - mean_absolute_error: 0.6133 - val_loss: 0.8986 - val_mean_absolute_error: 0.8986\n",
      "Epoch 69/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6068 - mean_absolute_error: 0.6068 - val_loss: 0.8976 - val_mean_absolute_error: 0.8976\n",
      "Epoch 70/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6164 - mean_absolute_error: 0.6164 - val_loss: 0.9015 - val_mean_absolute_error: 0.9015\n",
      "Epoch 71/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6087 - mean_absolute_error: 0.6087 - val_loss: 0.9093 - val_mean_absolute_error: 0.9093\n",
      "Epoch 72/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6060 - mean_absolute_error: 0.6060 - val_loss: 0.9320 - val_mean_absolute_error: 0.9320\n",
      "Epoch 73/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6069 - mean_absolute_error: 0.6069 - val_loss: 0.8818 - val_mean_absolute_error: 0.8818\n",
      "Epoch 74/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6012 - mean_absolute_error: 0.6012 - val_loss: 0.8895 - val_mean_absolute_error: 0.8895\n",
      "Epoch 75/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5948 - mean_absolute_error: 0.5948 - val_loss: 0.9510 - val_mean_absolute_error: 0.9510\n",
      "Epoch 76/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6030 - mean_absolute_error: 0.6030 - val_loss: 0.8884 - val_mean_absolute_error: 0.8884\n",
      "Epoch 77/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5905 - mean_absolute_error: 0.5905 - val_loss: 0.9292 - val_mean_absolute_error: 0.9292\n",
      "Epoch 78/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5869 - mean_absolute_error: 0.5869 - val_loss: 0.8999 - val_mean_absolute_error: 0.8999\n",
      "Epoch 79/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5858 - mean_absolute_error: 0.5858 - val_loss: 0.9045 - val_mean_absolute_error: 0.9045\n",
      "Epoch 80/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5877 - mean_absolute_error: 0.5877 - val_loss: 0.9128 - val_mean_absolute_error: 0.9128\n",
      "Epoch 81/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5888 - mean_absolute_error: 0.5888 - val_loss: 0.9007 - val_mean_absolute_error: 0.9007\n",
      "Epoch 82/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5773 - mean_absolute_error: 0.5773 - val_loss: 0.8913 - val_mean_absolute_error: 0.8913\n",
      "Epoch 83/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5893 - mean_absolute_error: 0.5893 - val_loss: 0.9104 - val_mean_absolute_error: 0.9104\n",
      "Epoch 84/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5784 - mean_absolute_error: 0.5784 - val_loss: 0.9112 - val_mean_absolute_error: 0.9112\n",
      "Epoch 85/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5794 - mean_absolute_error: 0.5794 - val_loss: 0.9158 - val_mean_absolute_error: 0.9158\n",
      "Epoch 86/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5762 - mean_absolute_error: 0.5762 - val_loss: 0.9056 - val_mean_absolute_error: 0.9056\n",
      "Epoch 87/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5746 - mean_absolute_error: 0.5746 - val_loss: 0.9235 - val_mean_absolute_error: 0.9235\n",
      "Epoch 88/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5826 - mean_absolute_error: 0.5826 - val_loss: 0.9050 - val_mean_absolute_error: 0.9050\n",
      "Epoch 89/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5721 - mean_absolute_error: 0.5721 - val_loss: 0.9014 - val_mean_absolute_error: 0.9014\n",
      "Epoch 90/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5667 - mean_absolute_error: 0.5667 - val_loss: 0.9011 - val_mean_absolute_error: 0.9011\n",
      "Epoch 91/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5798 - mean_absolute_error: 0.5798 - val_loss: 0.9099 - val_mean_absolute_error: 0.9099\n",
      "Epoch 92/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5689 - mean_absolute_error: 0.5689 - val_loss: 0.9198 - val_mean_absolute_error: 0.9198\n",
      "Epoch 93/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5734 - mean_absolute_error: 0.5734 - val_loss: 0.8973 - val_mean_absolute_error: 0.8973\n",
      "Epoch 94/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5832 - mean_absolute_error: 0.5832 - val_loss: 0.9035 - val_mean_absolute_error: 0.9035\n",
      "Epoch 95/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5598 - mean_absolute_error: 0.5598 - val_loss: 0.8960 - val_mean_absolute_error: 0.8960\n",
      "Epoch 96/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5584 - mean_absolute_error: 0.5584 - val_loss: 0.8992 - val_mean_absolute_error: 0.8992\n",
      "Epoch 97/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5649 - mean_absolute_error: 0.5649 - val_loss: 0.9141 - val_mean_absolute_error: 0.9141\n",
      "Epoch 98/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5523 - mean_absolute_error: 0.5523 - val_loss: 0.9010 - val_mean_absolute_error: 0.9010\n",
      "Epoch 99/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5598 - mean_absolute_error: 0.5598 - val_loss: 0.9065 - val_mean_absolute_error: 0.9065\n",
      "Epoch 100/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5583 - mean_absolute_error: 0.5583 - val_loss: 0.9037 - val_mean_absolute_error: 0.9037\n",
      "Train on 1280 samples, validate on 320 samples\n",
      "Epoch 1/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 18.8452 - val_loss: 4.8843\n",
      "Epoch 2/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.3295 - val_loss: 1.9703\n",
      "Epoch 3/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7548 - val_loss: 1.6349\n",
      "Epoch 4/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5050 - val_loss: 1.5179\n",
      "Epoch 5/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.3417 - val_loss: 1.3470\n",
      "Epoch 6/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.2435 - val_loss: 1.2814\n",
      "Epoch 7/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.1624 - val_loss: 1.2135\n",
      "Epoch 8/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.0929 - val_loss: 1.1724\n",
      "Epoch 9/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.0303 - val_loss: 1.1081\n",
      "Epoch 10/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.9942 - val_loss: 1.0730\n",
      "Epoch 11/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.9517 - val_loss: 1.0579\n",
      "Epoch 12/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.9239 - val_loss: 1.0167\n",
      "Epoch 13/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.9062 - val_loss: 0.9932\n",
      "Epoch 14/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.8659 - val_loss: 0.9700\n",
      "Epoch 15/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.8520 - val_loss: 0.9597\n",
      "Epoch 16/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.8371 - val_loss: 0.9447\n",
      "Epoch 17/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.8404 - val_loss: 0.9816\n",
      "Epoch 18/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.8127 - val_loss: 0.9249\n",
      "Epoch 19/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7918 - val_loss: 0.9186\n",
      "Epoch 20/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7891 - val_loss: 0.9090\n",
      "Epoch 21/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7738 - val_loss: 0.9247\n",
      "Epoch 22/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7743 - val_loss: 0.9096\n",
      "Epoch 23/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7558 - val_loss: 0.9220\n",
      "Epoch 24/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7526 - val_loss: 0.8966\n",
      "Epoch 25/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7420 - val_loss: 0.9184\n",
      "Epoch 26/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7467 - val_loss: 0.9166\n",
      "Epoch 27/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7442 - val_loss: 0.8978\n",
      "Epoch 28/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7223 - val_loss: 0.9736\n",
      "Epoch 29/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7249 - val_loss: 0.9099\n",
      "Epoch 30/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7342 - val_loss: 0.9018\n",
      "Epoch 31/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7155 - val_loss: 0.9038\n",
      "Epoch 32/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7000 - val_loss: 0.9975\n",
      "Epoch 33/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7012 - val_loss: 0.9412\n",
      "Epoch 34/100\n",
      "1280/1280 [==============================] - ETA: 0s - loss: 0.694 - 4s 3ms/step - loss: 0.6972 - val_loss: 0.9703\n",
      "Epoch 35/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6858 - val_loss: 0.8939\n",
      "Epoch 36/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6753 - val_loss: 0.9570\n",
      "Epoch 37/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6838 - val_loss: 0.8904\n",
      "Epoch 38/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6713 - val_loss: 0.8973\n",
      "Epoch 39/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6640 - val_loss: 0.9024\n",
      "Epoch 40/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6539 - val_loss: 0.8935\n",
      "Epoch 41/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6496 - val_loss: 0.9160\n",
      "Epoch 42/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6515 - val_loss: 0.8943\n",
      "Epoch 43/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6604 - val_loss: 0.8869\n",
      "Epoch 44/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6407 - val_loss: 0.8865\n",
      "Epoch 45/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6523 - val_loss: 0.9133\n",
      "Epoch 46/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6403 - val_loss: 0.9136\n",
      "Epoch 47/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6299 - val_loss: 0.8866\n",
      "Epoch 48/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6220 - val_loss: 0.9075\n",
      "Epoch 49/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6228 - val_loss: 0.8990\n",
      "Epoch 50/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6243 - val_loss: 0.8925\n",
      "Epoch 51/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6345 - val_loss: 0.8851\n",
      "Epoch 52/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6146 - val_loss: 0.8885\n",
      "Epoch 53/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6098 - val_loss: 0.9135\n",
      "Epoch 54/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6050 - val_loss: 0.9256\n",
      "Epoch 55/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6026 - val_loss: 0.8935\n",
      "Epoch 56/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6097 - val_loss: 0.8855\n",
      "Epoch 57/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6011 - val_loss: 0.9168\n",
      "Epoch 58/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5927 - val_loss: 0.8822\n",
      "Epoch 59/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5980 - val_loss: 0.8858\n",
      "Epoch 60/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5879 - val_loss: 0.8794\n",
      "Epoch 61/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6019 - val_loss: 0.8807\n",
      "Epoch 62/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5886 - val_loss: 0.8682\n",
      "Epoch 63/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5831 - val_loss: 0.8810\n",
      "Epoch 64/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5827 - val_loss: 0.9095\n",
      "Epoch 65/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5827 - val_loss: 0.8965\n",
      "Epoch 66/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5763 - val_loss: 0.8906\n",
      "Epoch 67/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5685 - val_loss: 0.9132\n",
      "Epoch 68/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5721 - val_loss: 0.8764\n",
      "Epoch 69/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5777 - val_loss: 0.8837\n",
      "Epoch 70/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5801 - val_loss: 0.9155\n",
      "Epoch 71/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5817 - val_loss: 0.8808\n",
      "Epoch 72/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5718 - val_loss: 0.9199\n",
      "Epoch 73/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5659 - val_loss: 0.9320\n",
      "Epoch 74/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5639 - val_loss: 0.8825\n",
      "Epoch 75/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5630 - val_loss: 0.8947\n",
      "Epoch 76/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5505 - val_loss: 0.8850\n",
      "Epoch 77/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5594 - val_loss: 0.9336\n",
      "Epoch 78/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5656 - val_loss: 0.9256\n",
      "Epoch 79/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5480 - val_loss: 0.8779\n",
      "Epoch 80/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5472 - val_loss: 0.8944\n",
      "Epoch 81/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5412 - val_loss: 0.8974\n",
      "Epoch 82/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5406 - val_loss: 0.9095\n",
      "Epoch 83/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5404 - val_loss: 0.9141\n",
      "Epoch 84/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5386 - val_loss: 0.8927\n",
      "Epoch 85/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5339 - val_loss: 0.8939\n",
      "Epoch 86/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5542 - val_loss: 0.8851\n",
      "Epoch 87/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5330 - val_loss: 0.9109\n",
      "Epoch 88/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5290 - val_loss: 0.8923\n",
      "Epoch 89/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5273 - val_loss: 0.9155\n",
      "Epoch 90/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5279 - val_loss: 0.9020\n",
      "Epoch 91/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5239 - val_loss: 0.8970\n",
      "Epoch 92/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5209 - val_loss: 0.8979\n",
      "Epoch 93/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5192 - val_loss: 0.9030\n",
      "Epoch 94/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5181 - val_loss: 0.9405\n",
      "Epoch 95/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5284 - val_loss: 0.9256\n",
      "Epoch 96/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5191 - val_loss: 0.9009\n",
      "Epoch 97/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5153 - val_loss: 0.9028\n",
      "Epoch 98/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5029 - val_loss: 0.8929\n",
      "Epoch 99/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5145 - val_loss: 0.9048\n",
      "Epoch 100/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5070 - val_loss: 0.9202\n",
      "Train on 1280 samples, validate on 320 samples\n",
      "Epoch 1/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 21.9362 - val_loss: 21.0831\n",
      "Epoch 2/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 10.6694 - val_loss: 4.9718\n",
      "Epoch 3/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 4.1598 - val_loss: 3.9340\n",
      "Epoch 4/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 3.2858 - val_loss: 3.5508\n",
      "Epoch 5/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.9754 - val_loss: 2.8293\n",
      "Epoch 6/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.9384 - val_loss: 2.2671\n",
      "Epoch 7/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.6124 - val_loss: 2.2294\n",
      "Epoch 8/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.5956 - val_loss: 1.9236\n",
      "Epoch 9/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.6133 - val_loss: 1.2556\n",
      "Epoch 10/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.4611 - val_loss: 1.4869\n",
      "Epoch 11/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.5565 - val_loss: 1.2181\n",
      "Epoch 12/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.3193 - val_loss: 2.0742\n",
      "Epoch 13/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.4152 - val_loss: 1.5683\n",
      "Epoch 14/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.3199 - val_loss: 1.9536\n",
      "Epoch 15/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.2937 - val_loss: 1.5653\n",
      "Epoch 16/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.3405 - val_loss: 1.2393\n",
      "Epoch 17/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.2545 - val_loss: 1.1195\n",
      "Epoch 18/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.1344 - val_loss: 1.1949\n",
      "Epoch 19/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.2316 - val_loss: 1.1902\n",
      "Epoch 20/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.0960 - val_loss: 1.5547\n",
      "Epoch 21/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.1313 - val_loss: 1.4119\n",
      "Epoch 22/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.0559 - val_loss: 1.0102\n",
      "Epoch 23/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.1752 - val_loss: 1.2884\n",
      "Epoch 24/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.1634 - val_loss: 1.2553\n",
      "Epoch 25/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9911 - val_loss: 1.1318\n",
      "Epoch 26/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.0460 - val_loss: 1.2042\n",
      "Epoch 27/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.0200 - val_loss: 1.0775\n",
      "Epoch 28/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.0328 - val_loss: 1.0665\n",
      "Epoch 29/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9516 - val_loss: 1.0490\n",
      "Epoch 30/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.0006 - val_loss: 0.9345\n",
      "Epoch 31/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9600 - val_loss: 1.1783\n",
      "Epoch 32/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9171 - val_loss: 1.1449\n",
      "Epoch 33/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9090 - val_loss: 0.9420\n",
      "Epoch 34/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9830 - val_loss: 0.9518\n",
      "Epoch 35/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8611 - val_loss: 0.9754\n",
      "Epoch 36/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9569 - val_loss: 1.0691\n",
      "Epoch 37/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8941 - val_loss: 0.9009\n",
      "Epoch 38/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9785 - val_loss: 1.0169\n",
      "Epoch 39/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8517 - val_loss: 1.3498\n",
      "Epoch 40/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8961 - val_loss: 1.1343\n",
      "Epoch 41/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8304 - val_loss: 0.9606\n",
      "Epoch 42/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8578 - val_loss: 1.0208\n",
      "Epoch 43/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8504 - val_loss: 0.9355\n",
      "Epoch 44/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8637 - val_loss: 1.4083\n",
      "Epoch 45/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8517 - val_loss: 1.3353\n",
      "Epoch 46/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8388 - val_loss: 0.9508\n",
      "Epoch 47/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8552 - val_loss: 1.1281\n",
      "Epoch 48/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8456 - val_loss: 0.9294\n",
      "Epoch 49/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8169 - val_loss: 1.0934\n",
      "Epoch 50/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8308 - val_loss: 1.1383\n",
      "Epoch 51/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7614 - val_loss: 0.9391\n",
      "Epoch 52/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7698 - val_loss: 0.8672\n",
      "Epoch 53/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7587 - val_loss: 1.6818\n",
      "Epoch 54/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7704 - val_loss: 1.4286\n",
      "Epoch 55/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7162 - val_loss: 0.9472\n",
      "Epoch 56/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7757 - val_loss: 0.9156\n",
      "Epoch 57/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7820 - val_loss: 1.0793\n",
      "Epoch 58/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7642 - val_loss: 0.9951\n",
      "Epoch 59/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8157 - val_loss: 0.9489\n",
      "Epoch 60/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7290 - val_loss: 1.1018\n",
      "Epoch 61/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6372 - val_loss: 1.0664\n",
      "Epoch 62/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7434 - val_loss: 0.9636\n",
      "Epoch 63/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6715 - val_loss: 0.9984\n",
      "Epoch 64/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7047 - val_loss: 1.1442\n",
      "Epoch 65/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7028 - val_loss: 0.9360\n",
      "Epoch 66/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6986 - val_loss: 0.8796\n",
      "Epoch 67/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7504 - val_loss: 0.9445\n",
      "Epoch 68/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6265 - val_loss: 1.0664\n",
      "Epoch 69/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6449 - val_loss: 0.8823\n",
      "Epoch 70/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7221 - val_loss: 1.0630\n",
      "Epoch 71/100\n",
      "1280/1280 [==============================] - 3s 3ms/step - loss: 1.7181 - val_loss: 0.9589\n",
      "Epoch 72/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6745 - val_loss: 1.2331\n",
      "Epoch 73/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6898 - val_loss: 1.0802\n",
      "Epoch 74/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5732 - val_loss: 0.9860\n",
      "Epoch 75/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5998 - val_loss: 0.9259\n",
      "Epoch 76/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7093 - val_loss: 0.9657\n",
      "Epoch 77/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5907 - val_loss: 1.0717\n",
      "Epoch 78/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5309 - val_loss: 0.9039\n",
      "Epoch 79/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6774 - val_loss: 0.9090\n",
      "Epoch 80/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6279 - val_loss: 0.9330\n",
      "Epoch 81/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6408 - val_loss: 0.8968\n",
      "Epoch 82/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5419 - val_loss: 1.0997\n",
      "Epoch 83/100\n",
      "1280/1280 [==============================] - 3s 3ms/step - loss: 1.6346 - val_loss: 0.8592\n",
      "Epoch 84/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5298 - val_loss: 1.0378\n",
      "Epoch 85/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5996 - val_loss: 0.8719\n",
      "Epoch 86/100\n",
      "1280/1280 [==============================] - 3s 3ms/step - loss: 1.5407 - val_loss: 0.9061\n",
      "Epoch 87/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6536 - val_loss: 0.9594\n",
      "Epoch 88/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5870 - val_loss: 0.9579\n",
      "Epoch 89/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6459 - val_loss: 0.8652\n",
      "Epoch 90/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5481 - val_loss: 0.9130\n",
      "Epoch 91/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5439 - val_loss: 0.9201\n",
      "Epoch 92/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6152 - val_loss: 0.9055\n",
      "Epoch 93/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5744 - val_loss: 0.8613\n",
      "Epoch 94/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5480 - val_loss: 0.9054\n",
      "Epoch 95/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5386 - val_loss: 0.8726\n",
      "Epoch 96/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5859 - val_loss: 1.0245\n",
      "Epoch 97/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6000 - val_loss: 1.2286\n",
      "Epoch 98/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6297 - val_loss: 0.9892\n",
      "Epoch 99/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5165 - val_loss: 0.9072\n",
      "Epoch 100/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5887 - val_loss: 0.9081\n"
     ]
    }
   ],
   "source": [
    "models = [k_models.model_5, k_models.model_6, k_models.model_7]\n",
    "\n",
    "model_names = [] #[m.__name__ for m in models]\n",
    "epochs_list = []\n",
    "mse_list = []\n",
    "mae_list = []\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_data_train_test(2000, X_tot, y_tot)\n",
    "\n",
    "nb_epochs = [100]\n",
    "for m in models:\n",
    "    for e in nb_epochs:\n",
    "        network = m(X_train, 'mean_absolute_error')\n",
    "        network.fit(X_train, y_train, epochs=e, batch_size=10, validation_split=0.2)\n",
    "\n",
    "        y_hat = network.predict(X_test)\n",
    "        mse_nn, mae_nn = compute_score(y_test, y_hat)\n",
    "\n",
    "        model_names.append(m.__name__)\n",
    "        epochs_list.append(e)\n",
    "        mse_list.append(mse_nn)\n",
    "        mae_list.append(mae_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒════════╤════════════╤════════════╤════════════╕\n",
      "│        │    model_5 │    model_6 │    model_7 │\n",
      "╞════════╪════════════╪════════════╪════════════╡\n",
      "│ Epochs │ 100        │ 100        │ 100        │\n",
      "├────────┼────────────┼────────────┼────────────┤\n",
      "│ MSE    │   1.01402  │   0.944624 │   1.2443   │\n",
      "├────────┼────────────┼────────────┼────────────┤\n",
      "│ MAE    │   0.768377 │   0.754768 │   0.866663 │\n",
      "╘════════╧════════════╧════════════╧════════════╛\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([model_names, ['Epochs']+epochs_list, ['MSE']+mse_list, ['MAE']+mae_list], headers=\"firstrow\", tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick tests\n",
    "apply_pca = False\n",
    "apply_iqr = True\n",
    "\n",
    "if apply_iqr:\n",
    "    X_train, X_test, y_train, y_test = load_data_train_test(12000, X_tot, y_tot, iqr=apply_iqr)\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = load_data_train_test(12000, X_tot, y_tot)\n",
    "\n",
    "if apply_pca:\n",
    "    n = 100\n",
    "    X_train, X_test = do_PCA(X_train, X_test, n)\n",
    "    nb_input_neurons = n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7680 samples, validate on 1920 samples\n",
      "Epoch 1/150\n",
      "7680/7680 [==============================] - 27s 3ms/step - loss: 4.3517 - val_loss: 1.1326\n",
      "Epoch 2/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 1.0146 - val_loss: 0.8708\n",
      "Epoch 3/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 0.8197 - val_loss: 0.7872\n",
      "Epoch 4/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.7380 - val_loss: 0.7370\n",
      "Epoch 5/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.6953 - val_loss: 0.6850\n",
      "Epoch 6/150\n",
      "7680/7680 [==============================] - 28s 4ms/step - loss: 0.6662 - val_loss: 0.6687\n",
      "Epoch 7/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 0.6464 - val_loss: 0.6513\n",
      "Epoch 8/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.6306 - val_loss: 0.6370\n",
      "Epoch 9/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.6140 - val_loss: 0.6369\n",
      "Epoch 10/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 0.6005 - val_loss: 0.6448\n",
      "Epoch 11/150\n",
      "7680/7680 [==============================] - 24s 3ms/step - loss: 0.5937 - val_loss: 0.6185\n",
      "Epoch 12/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.5808 - val_loss: 0.6438\n",
      "Epoch 13/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.5702 - val_loss: 0.6016\n",
      "Epoch 14/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.5661 - val_loss: 0.6178\n",
      "Epoch 15/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.5595 - val_loss: 0.5951\n",
      "Epoch 16/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 0.5458 - val_loss: 0.5860\n",
      "Epoch 17/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.5400 - val_loss: 0.6213\n",
      "Epoch 18/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.5352 - val_loss: 0.5938\n",
      "Epoch 19/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.5322 - val_loss: 0.5697\n",
      "Epoch 20/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.5291 - val_loss: 0.5731\n",
      "Epoch 21/150\n",
      "7680/7680 [==============================] - 25s 3ms/step - loss: 0.5247 - val_loss: 0.5779\n",
      "Epoch 22/150\n",
      "7680/7680 [==============================] - 26s 3ms/step - loss: 0.5207 - val_loss: 0.5709\n",
      "Epoch 23/150\n",
      "7680/7680 [==============================] - 26s 3ms/step - loss: 0.5111 - val_loss: 0.5703\n",
      "Epoch 24/150\n",
      "7680/7680 [==============================] - 26s 3ms/step - loss: 0.5122 - val_loss: 0.5708\n",
      "Epoch 25/150\n",
      "7680/7680 [==============================] - 24s 3ms/step - loss: 0.5086 - val_loss: 0.5652\n",
      "Epoch 26/150\n",
      "7680/7680 [==============================] - 25s 3ms/step - loss: 0.5010 - val_loss: 0.5613\n",
      "Epoch 27/150\n",
      "7680/7680 [==============================] - 27s 3ms/step - loss: 0.4959 - val_loss: 0.5688\n",
      "Epoch 28/150\n",
      "7680/7680 [==============================] - 25s 3ms/step - loss: 0.5056 - val_loss: 0.5712\n",
      "Epoch 29/150\n",
      "7680/7680 [==============================] - 32s 4ms/step - loss: 0.4928 - val_loss: 0.5826\n",
      "Epoch 30/150\n",
      "7680/7680 [==============================] - 27s 4ms/step - loss: 0.4942 - val_loss: 0.5526\n",
      "Epoch 31/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4893 - val_loss: 0.5643\n",
      "Epoch 32/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4875 - val_loss: 0.5527\n",
      "Epoch 33/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4889 - val_loss: 0.5541\n",
      "Epoch 34/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4843 - val_loss: 0.5538\n",
      "Epoch 35/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4794 - val_loss: 0.5501\n",
      "Epoch 36/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4793 - val_loss: 0.5533\n",
      "Epoch 37/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4794 - val_loss: 0.5496\n",
      "Epoch 38/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4790 - val_loss: 0.5770\n",
      "Epoch 39/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4691 - val_loss: 0.5657\n",
      "Epoch 40/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4670 - val_loss: 0.5523\n",
      "Epoch 41/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4724 - val_loss: 0.5714\n",
      "Epoch 42/150\n",
      "7680/7680 [==============================] - 24s 3ms/step - loss: 0.4634 - val_loss: 0.5437\n",
      "Epoch 43/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4632 - val_loss: 0.5801\n",
      "Epoch 44/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4629 - val_loss: 0.5658\n",
      "Epoch 45/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4655 - val_loss: 0.5407\n",
      "Epoch 46/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4596 - val_loss: 0.5432\n",
      "Epoch 47/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4570 - val_loss: 0.5400\n",
      "Epoch 48/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4590 - val_loss: 0.5716\n",
      "Epoch 49/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4620 - val_loss: 0.6188\n",
      "Epoch 50/150\n",
      "7680/7680 [==============================] - 25s 3ms/step - loss: 0.4519 - val_loss: 0.5896\n",
      "Epoch 51/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4595 - val_loss: 0.5511\n",
      "Epoch 52/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4500 - val_loss: 0.5539\n",
      "Epoch 53/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4443 - val_loss: 0.5684\n",
      "Epoch 54/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4489 - val_loss: 0.5926\n",
      "Epoch 55/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4452 - val_loss: 0.5668\n",
      "Epoch 56/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4410 - val_loss: 0.5497\n",
      "Epoch 57/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4469 - val_loss: 0.5646\n",
      "Epoch 58/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4499 - val_loss: 0.5402\n",
      "Epoch 59/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4437 - val_loss: 0.5362\n",
      "Epoch 60/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4432 - val_loss: 0.5575\n",
      "Epoch 61/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4388 - val_loss: 0.5347\n",
      "Epoch 62/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4412 - val_loss: 0.5401\n",
      "Epoch 63/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4405 - val_loss: 0.5477\n",
      "Epoch 64/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4344 - val_loss: 0.5390\n",
      "Epoch 65/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4376 - val_loss: 0.5747\n",
      "Epoch 66/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4343 - val_loss: 0.5600\n",
      "Epoch 67/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4296 - val_loss: 0.5437\n",
      "Epoch 68/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4306 - val_loss: 0.5831\n",
      "Epoch 69/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4345 - val_loss: 0.6191\n",
      "Epoch 70/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4385 - val_loss: 0.5409\n",
      "Epoch 71/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4301 - val_loss: 0.5396\n",
      "Epoch 72/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4304 - val_loss: 0.5517\n",
      "Epoch 73/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4284 - val_loss: 0.5441\n",
      "Epoch 74/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4322 - val_loss: 0.5373\n",
      "Epoch 75/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4269 - val_loss: 0.5354\n",
      "Epoch 76/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4273 - val_loss: 0.5365\n",
      "Epoch 77/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4269 - val_loss: 0.5409\n",
      "Epoch 78/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4241 - val_loss: 0.5347\n",
      "Epoch 79/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4297 - val_loss: 0.5357\n",
      "Epoch 80/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4233 - val_loss: 0.5425\n",
      "Epoch 81/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4249 - val_loss: 0.5616\n",
      "Epoch 82/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4247 - val_loss: 0.5373\n",
      "Epoch 83/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4194 - val_loss: 0.5848\n",
      "Epoch 84/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4280 - val_loss: 0.5360\n",
      "Epoch 85/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4175 - val_loss: 0.5304\n",
      "Epoch 86/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4266 - val_loss: 0.5533\n",
      "Epoch 87/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4179 - val_loss: 0.5327\n",
      "Epoch 88/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4223 - val_loss: 0.5325\n",
      "Epoch 89/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4133 - val_loss: 0.5910\n",
      "Epoch 90/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4191 - val_loss: 0.5628\n",
      "Epoch 91/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4160 - val_loss: 0.5758\n",
      "Epoch 92/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4141 - val_loss: 0.5253\n",
      "Epoch 93/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4142 - val_loss: 0.5455\n",
      "Epoch 94/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 0.4177 - val_loss: 0.5700\n",
      "Epoch 95/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4138 - val_loss: 0.5318\n",
      "Epoch 96/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4152 - val_loss: 0.5523\n",
      "Epoch 97/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4158 - val_loss: 0.5749\n",
      "Epoch 98/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4095 - val_loss: 0.5516\n",
      "Epoch 99/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4130 - val_loss: 0.5440\n",
      "Epoch 100/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4109 - val_loss: 0.5351\n",
      "Epoch 101/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4167 - val_loss: 0.5329\n",
      "Epoch 102/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4066 - val_loss: 0.5391\n",
      "Epoch 103/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4071 - val_loss: 0.5538\n",
      "Epoch 104/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4097 - val_loss: 0.5388\n",
      "Epoch 105/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4115 - val_loss: 0.5352\n",
      "Epoch 106/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4086 - val_loss: 0.5520\n",
      "Epoch 107/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4156 - val_loss: 0.5346\n",
      "Epoch 108/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4067 - val_loss: 0.5387\n",
      "Epoch 109/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4106 - val_loss: 0.5554\n",
      "Epoch 110/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4053 - val_loss: 0.5339\n",
      "Epoch 111/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4091 - val_loss: 0.5254\n",
      "Epoch 112/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4096 - val_loss: 0.5432\n",
      "Epoch 113/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4046 - val_loss: 0.5603\n",
      "Epoch 114/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4054 - val_loss: 0.5843\n",
      "Epoch 115/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4117 - val_loss: 0.5337\n",
      "Epoch 116/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4076 - val_loss: 0.5339\n",
      "Epoch 117/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4081 - val_loss: 0.5346\n",
      "Epoch 118/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4041 - val_loss: 0.5430\n",
      "Epoch 119/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4039 - val_loss: 0.5296\n",
      "Epoch 120/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4007 - val_loss: 0.5362\n",
      "Epoch 121/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4099 - val_loss: 0.5773\n",
      "Epoch 122/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4040 - val_loss: 0.6434\n",
      "Epoch 123/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4040 - val_loss: 0.5409\n",
      "Epoch 124/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4082 - val_loss: 0.5318\n",
      "Epoch 125/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4047 - val_loss: 0.5287\n",
      "Epoch 126/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4008 - val_loss: 0.5337\n",
      "Epoch 127/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4019 - val_loss: 0.5473\n",
      "Epoch 128/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.3997 - val_loss: 0.5272\n",
      "Epoch 129/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.3977 - val_loss: 0.5296\n",
      "Epoch 130/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4032 - val_loss: 0.5574\n",
      "Epoch 131/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.3975 - val_loss: 0.5309\n",
      "Epoch 132/150\n",
      "7680/7680 [==============================] - 25s 3ms/step - loss: 0.3988 - val_loss: 0.5904\n",
      "Epoch 133/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4003 - val_loss: 0.5401\n",
      "Epoch 134/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3983 - val_loss: 0.5326\n",
      "Epoch 135/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3963 - val_loss: 0.5345\n",
      "Epoch 136/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4005 - val_loss: 0.5383\n",
      "Epoch 137/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3957 - val_loss: 0.5797\n",
      "Epoch 138/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4038 - val_loss: 0.5557\n",
      "Epoch 139/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3976 - val_loss: 0.5414\n",
      "Epoch 140/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3942 - val_loss: 0.5449\n",
      "Epoch 141/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3957 - val_loss: 0.5513\n",
      "Epoch 142/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3944 - val_loss: 0.5498\n",
      "Epoch 143/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 0.3943 - val_loss: 0.5328\n",
      "Epoch 144/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3936 - val_loss: 0.5315\n",
      "Epoch 145/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3965 - val_loss: 0.5308\n",
      "Epoch 146/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3920 - val_loss: 0.5547\n",
      "Epoch 147/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3971 - val_loss: 0.5512\n",
      "Epoch 148/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3891 - val_loss: 0.5330\n",
      "Epoch 149/150\n",
      "7680/7680 [==============================] - 24s 3ms/step - loss: 0.4047 - val_loss: 0.5488\n",
      "Epoch 150/150\n",
      "7680/7680 [==============================] - 24s 3ms/step - loss: 0.3939 - val_loss: 0.5453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2bd418eae10>"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_ann = k_models.model_8(X_train, 'mean_absolute_error') \n",
    "k_ann.fit(X_train, y_train, epochs=150, batch_size=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.58 \n",
      "MAE: 0.57\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "y_hat = k_ann.predict(X_test)\n",
    "\n",
    "# compute score\n",
    "mse_nn, mae_nn, _ = compute_score(y_test, y_hat)\n",
    "print('MSE: {:.2f} \\nMAE: {:.2f}'.format(mse_nn, mae_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('y_hat_12000_dataset2_model8.pkl', 'wb') as f:\n",
    "    pickle.dump(y_hat, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results tracking**\n",
    "\n",
    "Model    | MSE  | MAE  | PCA | IQR |    n     | dataset | *n* PCs | Epochs | Scaling | \n",
    "---      | ---  | ---  | --- | --- |   ---    |   ---   |   ---   |   ---  |   ---   |\n",
    "model_6  | 0.93 | 0.70 | Yes | Yes |  20000   |   2     |   40    |   150  |   No    |\n",
    "model_6  | 0.85 | 0.67 | No  | Yes |   4000   |   2     |    -    |   150  |   No    |\n",
    "model_6  | 0.78 | 0.64 | Yes | Yes |  20000   |   2     |   60    |   150  |   No    | \n",
    "model_6  | 0.75 | 0.63 | Yes | Yes |  25000   |   2     |   70    |   200  |   No    |\n",
    "model_6  | 0.67 | 0.60 | No  | Yes |   8000   |   2     |    -    |   150  |   No    |\n",
    "model_6  | 0.75 | 0.62 | Yes | Yes |  25000   |   2     |  100    |   150  |   No    |\n",
    "model_6  | 0.60 | 0.55 | No  | Yes |  12000   |   2     |    -    |   150  |   No    |\n",
    "Net_3    | 1.01 | 0.74 | Yes | Yes |   5000   |   2     |   80    |   100  |   No    |\n",
    "Net_3    | 1.03 | 0.73 | Yes | Yes |   8000   |   2     |   80    |   150  |   No    |\n",
    "Net_3    | 0.65 | 0.60 | No  | Yes |   8000   |   2     |    -    |   150  |   No    |\n",
    "Net_3    | 0.52 | 0.54 | No  | Yes |  12000   |   2     |    -    |   150  |   No    |\n",
    "model_8  | 0.58 | 0.57 | No  | Yes |  12000   |   2     |    -    |   150  |   No    |\n",
    "Net_3    | 0.57 | 0.56 | No  | Yes |  12000   |   3     |    -    |   150  |   No    |\n",
    "Net_3    | 0.65 | 0.62 | No  | Yes |  12000   |   2     |    -    |   150  |   Yes   |\n",
    "Net_3    | 1.00 | 0.76 | Yes | Yes |  12000   |   2     |   80    |   150  |   Yes   |\n",
    "Net_3    | 0.86 | 0.69 | Yes | Yes |  25000   |   2     |   80    |   150  |   Yes   |\n",
    "Net_4    | 0.47 | 0.52 | No  | Yes |  12000   |   2     |   -     |   150  |   Yes   |\n",
    "Net_5    | 0.54 | 0.54 | No  | Yes |  12000   |   2     |   -     |   150  |   Yes   |\n",
    "\n",
    "Note: \n",
    "- Larger increase in performances when using more PCs (say 60).\n",
    "- Obviously computationally less demanding\n",
    "- Will the increase in number of samples without PCA perform better? YES it does.\n",
    "- Loss evolution indicates that 150 epochs are not necessary (try with 120)\n",
    "- Plateau reached using larger number of PCs\n",
    "- $r^2 = 0.95$ for Net_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 NN3 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "tot_data_y = np.load('../data/CSD-10k_H_chemical_shieldings.npy',mmap_mode='r')\n",
    "mask = np.random.permutation(tot_data_y.shape[0])[:n_samples]\n",
    "data_y = tot_data_y[mask]\n",
    "#tot_data_X = np.load('../data/CSD-10k_H_fps_1k_MD_n_12_l_9_rc_3.0_gw_0.3_rsr_1.0_rss_2.5_rse_5.npy',mmap_mode='r')\n",
    "data_X1 = tot_data_X[mask]\n",
    "#tot_data_X = np.load('../data/CSD-10k_H_fps_1k_MD_n_12_l_9_rc_5.0_gw_0.3_rsr_1.0_rss_2.5_rse_5.npy',mmap_mode='r')\n",
    "data_X2 = tot_data_X[mask]\n",
    "#tot_data_X = np.load('../data/CSD-10k_H_fps_1k_MD_n_12_l_9_rc_7.0_gw_0.3_rsr_1.0_rss_2.5_rse_5.npy',mmap_mode='r')\n",
    "data_X3 = tot_data_X[mask]\n",
    "\n",
    "#instanciation of the bigbibo\n",
    "bibo1 = nrn.Net_3(14400)\n",
    "bibo2 = nrn.Net_3(14400)\n",
    "bibo3 = nrn.Net_3(14400)\n",
    "bigbibo  = nn3.NN3(bibo1,bibo2,bibo3,nb_epochs=150,assemble_y='custom')\n",
    "\n",
    "#Watch out! dangerous for laptops\n",
    "#scores_mean,scores_custom = nn3.KFold3(bigbibo,data_X1,data_X2,data_X3,data_y,n_splits=5)\n",
    "\n",
    "scores = {'mean_method':scores_mean,'custom_method':scores_custom}\n",
    "\n",
    "with open('kfoldresults.pickle','wb') as f:\n",
    "    pickle.dump(scores,f)\n",
    "\n",
    "print(np.mean(scores_mean['mse']))\n",
    "print(np.mean(scores_custom['mse']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stats\n",
    "<a id='stats'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stats_iqr_vs_n_2.pkl', 'wb') as f:\n",
    "    pickle.dump([noiqr_5000, noiqr_20000, iqr_5000, iqr_20000], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Feature distributions\n",
    "<a id='fig_feature_dist'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note NN\n",
    "- Simple model is the best (1 hidden of 100 neurons)\n",
    "- For now dropout does not help\n",
    "- Increasing the number of hidden neurons to 200 does not help\n",
    "- Decreasing the number of hidden neurons to 50 shows problem of cenvergence\n",
    "- Increasing the number of samples show little or no improvements (filter with **IQR** and **minmaxscaler** before and/or **PCA**)\n",
    "- Test Dropout in PyTorch (specify when training and when NOT training)!\n",
    "- Again even with batch norm, the simplest model with one hidden seems the best\n",
    "- Keras: Adam better for model 3 with 100 neurons in hidden layer (sinon does not converge with SGD). Can also try with opt='adam' (...)\n",
    "- **OK definitely best with 100 hidden neurons and adam as optimizer**\n",
    "- Now check with PCA, min_max scaling\n",
    "- Increasing the number of epochs seems a good idea\n",
    "- IQR helps (ouf)!\n",
    "- Increasing the number of samples helps also (ouf)!\n",
    "- Not sure if PCA useful with Keras models\n",
    "- Model_6 still the best at this point. Try with DEEP net (lots of 'small' layers)?\n",
    "\n",
    "> About PCA\n",
    "- Shall we normalize before?\n",
    "- How does the PCA method from sklearn work? Normalize before?\n",
    "- When trying without PCA > apply a SCALER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
