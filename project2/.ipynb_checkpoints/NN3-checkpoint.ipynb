{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Machine Learning project CS-433: NMR spectroscopy supervised learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedules:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Week 10 (18-24 November): \n",
    " * Tests of various linear models/simple NN on a 10% subset of data\n",
    "* Week 11 (25-1 December):\n",
    " * Feature selection: being able to come with a good set of features\n",
    "* Week 12 (2-8 December):\n",
    " * Start of big scale analysis with Spark, implementation of the models which perform well at small scale\n",
    "* Week 13 (9-15 December):\n",
    " * Wrapping up\n",
    "* Week 14 (16-22 December): \n",
    " * 19th December: Deadline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Log Book](#log)\n",
    "2. [Pipeline](#pipeline)\n",
    "3. [Data Processing](#data_proc) <br>\n",
    "&emsp;3.1. [Data Vizualisation](#data_viz) <br>\n",
    "&emsp;3.2 [Outliers detection](#outliers) <br>\n",
    "  &emsp;&emsp;3.2.1 [DBSCAN](#dbscan) <br>\n",
    "  &emsp;&emsp;3.2.2 [Inter quantile range method](#iqr) <br>\n",
    "&emsp;3.3 [Scaling](#scaling) <br>\n",
    "&emsp;&emsp;3.3.1 [Min max scaling](#minmax) <br>\n",
    "&emsp;3.4 [Dimensionality reduction](#dim_red) <br>\n",
    "  &emsp;&emsp;3.4.1 [PCA](#pca) <br>\n",
    "&emsp;3.5 [Feature Selection](#feat_sel) <br>\n",
    "  &emsp;&emsp;3.5.1 [Relative importance from linear regression](#rel_imp_lin) <br>\n",
    "  &emsp;&emsp;3.5.2 [Random forest](#rand_for) <br>\n",
    "  &emsp;&emsp;3.5.3 [Univariate feature selection](#un_feat_sel) <br>\n",
    "  &emsp;&emsp;3.5.4 [Recursive feature selection](#rec_feat_sel) <br>\n",
    "  &emsp;&emsp;3.5.5 [Lasso Regression](#lasso) <br>\n",
    "  &emsp;&emsp;3.5.6 [Boruta](#boruta) <br>\n",
    "&emsp;3.6 [Models](#models) <br>\n",
    "  &emsp;&emsp;3.6.1 [Linear Models](#lin_mods) <br>\n",
    "  &emsp;&emsp;3.6.2 [Neural Networks](#NN) <br>\n",
    "4. [Main](#main) <br>\n",
    "   4.1 [ANN implementation](#ann_imp) <br>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import scipy.stats\n",
    "import sklearn.metrics\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm\n",
    "from itertools import combinations\n",
    "\n",
    "#from boruta import BorutaPy\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For neural net part\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "#from keras.callbacks import ModelCheckpoint\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Activation, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_3(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(Net_3, self).__init__()\n",
    "        self.fc1 = nn.Linear(n,100)\n",
    "        self.fc2 = nn.Linear(100,1)\n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN3():\n",
    "    def __init__(self,mod_1,mod_2,mod_3,mini_batch_size = 10 ,\n",
    "                 apply_iqr = True,apply_scaler = False,apply_pca = False,\n",
    "                 assemble_y = 'custom',nb_epochs = 150,normalize = False):\n",
    "        self.mod1 = mod_1\n",
    "        self.mod2 = mod_2\n",
    "        self.mod3 = mod_3\n",
    "        self.minbatchsize = mini_batch_size\n",
    "        self.assemble_y = assemble_y\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.apply_iqr = apply_iqr\n",
    "        self.apply_pca = apply_pca\n",
    "        self.apply_scaler = apply_scaler\n",
    "        \n",
    "    def train_model(self,model, train_input, train_target, monitor_loss=False):\n",
    "        criterion = nn.MSELoss() #regression task\n",
    "        optimizer = optim.Adam(model.parameters(), lr = 1e-4) #1e-4 normalement\n",
    "\n",
    "        # Monitor loss\n",
    "        losses = []\n",
    "\n",
    "        for e in range(self.nb_epochs):\n",
    "            sum_loss = 0\n",
    "            N = train_input.size(0)\n",
    "            for b in range(0, N, self.minbatchsize):\n",
    "                output = model(train_input.narrow(0, b, min(self.minbatchsize,N - b)))\n",
    "                loss = criterion(output, train_target.narrow(0, b, min(self.minbatchsize,N - b)))\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                sum_loss += loss.item() #compute loss for each mini batch for 1 epoch\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "            # Monitor loss\n",
    "            losses.append(sum_loss)\n",
    "\n",
    "            print('[epoch {:d}] loss: {:0.2f}'.format(e+1, sum_loss))\n",
    "\n",
    "        if monitor_loss:\n",
    "            return losses\n",
    "        \n",
    "    def IQR_y_outliers(self,X1,X2,X3,y_data):\n",
    "        ''' aims at removing all rows whose label (i.e. shielding) is considered as outlier.\n",
    "        output:\n",
    "         - X_filtered\n",
    "         - y_filtered\n",
    "        '''\n",
    "        q1, q3 = np.percentile(y_data, [25, 75])\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - (iqr * 1.5)\n",
    "        upper_bound = q3 + (iqr * 1.5)\n",
    "\n",
    "        assert(q1 != q3)\n",
    "\n",
    "        idx = np.where((y_data > lower_bound) & (y_data < upper_bound))\n",
    "        X1, X2, X3 = X1[idx], X2[idx], X3[idx]\n",
    "        y_data = y_data[idx]\n",
    "\n",
    "        assert(X1.shape[0] == y_data.shape[0] and X2.shape[0] == y_data.shape[0] and X3.shape[0] == y_data.shape[0])\n",
    "        return X1, X2, X3, y_data\n",
    "    \n",
    "    def fit(self, X1,X2,X3,y):\n",
    "        if self.apply_iqr:\n",
    "            X1,X2,X3,y = self.IQR_y_outliers(X1,X2,X3,y)\n",
    "        if self.normalize:\n",
    "            self.trans1 = Normalizer().fit(X1)\n",
    "            self.trans2 = Normalizer().fit(X2)\n",
    "            self.tran3 = Normalizer.fit(X3)\n",
    "        X1 = torch.Tensor(X1)\n",
    "        X2 = torch.Tensor(X2)\n",
    "        X3 = torch.Tensor(X3)\n",
    "        y = torch.Tensor(y.reshape(len(y), 1))\n",
    "        print('#' * 30 + 'Training model 1'+ '#' * 30)\n",
    "        loss1 = self.train_model(self.mod1, X1, y, monitor_loss=True)\n",
    "        print('#' * 30 + 'Training model 2'+ '#' * 30)\n",
    "        loss2 = self.train_model(self.mod2, X2, y, monitor_loss=True)\n",
    "        print('#' * 30 + 'Training model 3'+ '#' * 30)\n",
    "        loss3 = self.train_model(self.mod3, X3, y, monitor_loss=True)\n",
    "        print('#' * 30 + 'TRAINING TERMINATED'+ '#' * 30)\n",
    "        \n",
    "    \n",
    "    def droledemean(self,xs):\n",
    "        xs = list(xs)\n",
    "        invs = np.array([[1/np.abs(x -y) for y in xs if y is not x] for x in xs])\n",
    "        tot = np.sum(invs)\n",
    "        weights = np.sum(invs,axis = 1)\n",
    "        return np.sum(weights * xs)/tot\n",
    "        \n",
    "    def assemble_ys(self,y1,y2,y3):\n",
    "        if self.assemble_y == 'mean':\n",
    "            return np.mean([y1,y2,y3],axis = 0)\n",
    "        k = np.array([self.droledemean(i) for i in np.array([y1.reshape(y1.shape[0]),\n",
    "                                                             y2.reshape(y2.shape[0]),\n",
    "                                                             y3.reshape(y3.shape[0])]).T])\n",
    "        return k\n",
    "    \n",
    "    def predict_indep(self,X1,X2,X3):\n",
    "        if self.normalize:\n",
    "            X1 = self.trans1.transform(X1)\n",
    "            X2 = self.trans2.transform(X2)\n",
    "            X3 = self.trans3.transform(X3)\n",
    "        X1 = torch.Tensor(X1)\n",
    "        X2 = torch.Tensor(X2)\n",
    "        X3 = torch.Tensor(X3)\n",
    "        y1_hat = self.mod1(X1).detach().numpy()\n",
    "        y2_hat = self.mod2(X2).detach().numpy()\n",
    "        y3_hat = self.mod3(X3).detach().numpy()\n",
    "        return y1_hat,y2_hat,y3_hat\n",
    "    \n",
    "    def set_mean(self,meth):\n",
    "        assert(meth == 'mean' or meth == 'custom')\n",
    "        self.assemble_y = meth\n",
    "        \n",
    "    def predict(self,X1,X2,X3):\n",
    "        y1_hat,y2_hat,y3_hat = self.predict_indep(X1,X2,X3)\n",
    "        return self.assemble_ys(y1_hat,y2_hat,y3_hat)      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each cell here is meant to do a whole pipeline, from loading a certain number of samples, preprocessing etc. We keep using the R2 score, the MSE and the MAE as our metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(y_actual, y_pred,verbose = False):\n",
    "    mse = mean_squared_error(y_actual, y_pred)\n",
    "    mae = mean_absolute_error(y_actual, y_pred)\n",
    "    if verbose:\n",
    "        print(\"Obtained MSE on test set %2.2f \" % mse)\n",
    "        print(\"Obtained MAE on test set %2.2f \" % mae)\n",
    "    return {'mse':mse,'mae':mae}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFold3(model3,data_X1,data_X2,data_X3,data_y,n_splits = 4,verbose = True):\n",
    "    \"\"\"\n",
    "    perform Kfold cross validation on an NN3 object\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, random_state=14, shuffle=True)\n",
    "    scores_mean = {'mse':[],'mae':[]}\n",
    "    scores_custom = {'mse':[],'mae':[]}\n",
    "    for kindx,(train_index, test_index) in enumerate(kf.split(data_y)):\n",
    "        \n",
    "        print('%i / %i fold' % (kindx+1,n_splits))\n",
    "        X1_train, X1_test = data_X1[train_index],data_X1[test_index]\n",
    "        X2_train, X2_test = data_X2[train_index],data_X2[test_index]\n",
    "        X3_train, X3_test = data_X3[train_index],data_X3[test_index]\n",
    "        y_train, y_test = data_y[train_index], data_y[test_index]\n",
    "        model3.fit(X1_train,X2_train,X3_train,y_train)\n",
    "        \n",
    "        model3.set_mean('mean')\n",
    "        y_hat = model3.predict(X1_test,X2_test,X3_test)\n",
    "        score_mean = compute_score(y_test,y_hat)\n",
    "        scores_mean['mse'].append(score_mean['mse'])\n",
    "        scores_mean['mae'].append(score_mean['mae'])\n",
    "        \n",
    "        model3.set_mean('custom')\n",
    "        y_hat = model3.predict(X1_test,X2_test,X3_test)\n",
    "        score_custom = compute_score(y_test,y_hat)\n",
    "        scores_custom['mse'].append(score_custom['mse'])\n",
    "        scores_custom['mae'].append(score_custom['mae'])\n",
    "        \n",
    "        if verbose:\n",
    "            print('Mean method:{}'.format(score_mean))\n",
    "            print('Custom method:{}'.format(score_custom))\n",
    "    return scores_mean,scores_custom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of samples we take from the datasets\n",
    "n_samples = 15000\n",
    "tot_data_y = np.load('data/CSD-10k_H_chemical_shieldings.npy',mmap_mode='r')\n",
    "mask = np.random.permutation(tot_data_y.shape[0])[:n_samples]\n",
    "data_y = tot_data_y[mask]\n",
    "tot_data_X = np.load('data/CSD-10k_H_fps_1k_MD_n_12_l_9_rc_3.0_gw_0.3_rsr_1.0_rss_2.5_rse_5.npy',mmap_mode='r')\n",
    "data_X1 = tot_data_X[mask]\n",
    "tot_data_X = np.load('data/CSD-10k_H_fps_1k_MD_n_12_l_9_rc_5.0_gw_0.3_rsr_1.0_rss_2.5_rse_5.npy',mmap_mode='r')\n",
    "data_X2 = tot_data_X[mask]\n",
    "tot_data_X = np.load('data/CSD-10k_H_fps_1k_MD_n_12_l_9_rc_7.0_gw_0.3_rsr_1.0_rss_2.5_rse_5.npy',mmap_mode='r')\n",
    "data_X3 = tot_data_X[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instanciation of the bigbibo\n",
    "bibo1 = Net_3(14400)\n",
    "bibo2 = Net_3(14400)\n",
    "bibo3 = Net_3(14400)\n",
    "bigbibo  = NN3(bibo1,bibo2,bibo3,nb_epochs=150,assemble_y='custom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 4 fold\n",
      "##############################Training model 1##############################\n",
      "[epoch 1] loss: 412502.44\n",
      "[epoch 2] loss: 20114.33\n",
      "[epoch 3] loss: 6874.56\n",
      "[epoch 4] loss: 5572.46\n",
      "[epoch 5] loss: 4463.00\n",
      "[epoch 6] loss: 3584.23\n",
      "[epoch 7] loss: 2919.80\n",
      "[epoch 8] loss: 2333.25\n",
      "[epoch 9] loss: 1988.05\n",
      "[epoch 10] loss: 1769.02\n",
      "[epoch 11] loss: 1612.25\n",
      "[epoch 12] loss: 1498.91\n",
      "[epoch 13] loss: 1411.02\n",
      "[epoch 14] loss: 1340.83\n",
      "[epoch 15] loss: 1283.62\n",
      "[epoch 16] loss: 1236.21\n",
      "[epoch 17] loss: 1196.33\n",
      "[epoch 18] loss: 1162.33\n",
      "[epoch 19] loss: 1132.99\n",
      "[epoch 20] loss: 1107.37\n",
      "[epoch 21] loss: 1084.78\n",
      "[epoch 22] loss: 1064.67\n",
      "[epoch 23] loss: 1046.63\n",
      "[epoch 24] loss: 1030.33\n",
      "[epoch 25] loss: 1015.51\n",
      "[epoch 26] loss: 1001.95\n",
      "[epoch 27] loss: 989.49\n",
      "[epoch 28] loss: 977.98\n",
      "[epoch 29] loss: 967.30\n",
      "[epoch 30] loss: 957.35\n",
      "[epoch 31] loss: 948.05\n",
      "[epoch 32] loss: 939.31\n",
      "[epoch 33] loss: 931.08\n",
      "[epoch 34] loss: 923.30\n",
      "[epoch 35] loss: 915.93\n",
      "[epoch 36] loss: 908.93\n",
      "[epoch 37] loss: 902.26\n",
      "[epoch 38] loss: 895.89\n",
      "[epoch 39] loss: 889.81\n",
      "[epoch 40] loss: 883.99\n",
      "[epoch 41] loss: 878.40\n",
      "[epoch 42] loss: 873.04\n",
      "[epoch 43] loss: 867.89\n",
      "[epoch 44] loss: 862.93\n",
      "[epoch 45] loss: 858.14\n",
      "[epoch 46] loss: 853.53\n",
      "[epoch 47] loss: 849.09\n",
      "[epoch 48] loss: 844.79\n",
      "[epoch 49] loss: 840.64\n",
      "[epoch 50] loss: 836.62\n",
      "[epoch 51] loss: 832.73\n",
      "[epoch 52] loss: 828.97\n",
      "[epoch 53] loss: 825.32\n",
      "[epoch 54] loss: 821.77\n",
      "[epoch 55] loss: 818.33\n",
      "[epoch 56] loss: 815.00\n",
      "[epoch 57] loss: 811.75\n",
      "[epoch 58] loss: 808.60\n",
      "[epoch 59] loss: 805.54\n",
      "[epoch 60] loss: 802.56\n",
      "[epoch 61] loss: 799.66\n",
      "[epoch 62] loss: 796.84\n",
      "[epoch 63] loss: 794.09\n",
      "[epoch 64] loss: 791.41\n",
      "[epoch 65] loss: 788.79\n",
      "[epoch 66] loss: 786.25\n",
      "[epoch 67] loss: 783.77\n",
      "[epoch 68] loss: 781.33\n",
      "[epoch 69] loss: 778.93\n",
      "[epoch 70] loss: 776.59\n",
      "[epoch 71] loss: 774.30\n",
      "[epoch 72] loss: 772.04\n",
      "[epoch 73] loss: 769.82\n",
      "[epoch 74] loss: 767.66\n",
      "[epoch 75] loss: 765.55\n",
      "[epoch 76] loss: 763.47\n",
      "[epoch 77] loss: 761.39\n",
      "[epoch 78] loss: 759.31\n",
      "[epoch 79] loss: 757.28\n",
      "[epoch 80] loss: 755.32\n",
      "[epoch 81] loss: 753.40\n",
      "[epoch 82] loss: 751.57\n",
      "[epoch 83] loss: 749.76\n",
      "[epoch 84] loss: 747.96\n",
      "[epoch 85] loss: 746.19\n",
      "[epoch 86] loss: 744.47\n",
      "[epoch 87] loss: 742.73\n",
      "[epoch 88] loss: 741.06\n",
      "[epoch 89] loss: 739.42\n",
      "[epoch 90] loss: 737.82\n",
      "[epoch 91] loss: 736.25\n",
      "[epoch 92] loss: 734.74\n",
      "[epoch 93] loss: 733.22\n",
      "[epoch 94] loss: 731.75\n",
      "[epoch 95] loss: 730.29\n",
      "[epoch 96] loss: 728.85\n",
      "[epoch 97] loss: 727.44\n",
      "[epoch 98] loss: 726.07\n",
      "[epoch 99] loss: 724.68\n",
      "[epoch 100] loss: 723.35\n",
      "[epoch 101] loss: 722.01\n",
      "[epoch 102] loss: 720.66\n",
      "[epoch 103] loss: 719.33\n",
      "[epoch 104] loss: 717.99\n",
      "[epoch 105] loss: 716.69\n",
      "[epoch 106] loss: 715.38\n",
      "[epoch 107] loss: 714.08\n",
      "[epoch 108] loss: 712.80\n",
      "[epoch 109] loss: 711.51\n",
      "[epoch 110] loss: 710.23\n",
      "[epoch 111] loss: 708.99\n",
      "[epoch 112] loss: 707.73\n",
      "[epoch 113] loss: 706.45\n",
      "[epoch 114] loss: 705.25\n",
      "[epoch 115] loss: 704.06\n",
      "[epoch 116] loss: 702.88\n",
      "[epoch 117] loss: 701.73\n",
      "[epoch 118] loss: 700.57\n",
      "[epoch 119] loss: 699.41\n",
      "[epoch 120] loss: 698.31\n",
      "[epoch 121] loss: 697.22\n",
      "[epoch 122] loss: 696.15\n",
      "[epoch 123] loss: 695.07\n",
      "[epoch 124] loss: 694.03\n",
      "[epoch 125] loss: 692.98\n",
      "[epoch 126] loss: 691.96\n",
      "[epoch 127] loss: 690.95\n",
      "[epoch 128] loss: 689.95\n",
      "[epoch 129] loss: 688.95\n",
      "[epoch 130] loss: 687.98\n",
      "[epoch 131] loss: 687.01\n",
      "[epoch 132] loss: 686.06\n",
      "[epoch 133] loss: 685.12\n",
      "[epoch 134] loss: 684.19\n",
      "[epoch 135] loss: 683.27\n",
      "[epoch 136] loss: 682.36\n",
      "[epoch 137] loss: 681.45\n",
      "[epoch 138] loss: 680.55\n",
      "[epoch 139] loss: 679.66\n",
      "[epoch 140] loss: 678.77\n",
      "[epoch 141] loss: 677.90\n",
      "[epoch 142] loss: 677.05\n",
      "[epoch 143] loss: 676.18\n",
      "[epoch 144] loss: 675.34\n",
      "[epoch 145] loss: 674.51\n",
      "[epoch 146] loss: 673.67\n",
      "[epoch 147] loss: 672.84\n",
      "[epoch 148] loss: 672.04\n",
      "[epoch 149] loss: 671.23\n",
      "[epoch 150] loss: 670.43\n",
      "##############################Training model 2##############################\n",
      "[epoch 1] loss: 450660.81\n",
      "[epoch 2] loss: 31814.35\n",
      "[epoch 3] loss: 6719.94\n",
      "[epoch 4] loss: 5618.56\n",
      "[epoch 5] loss: 4707.04\n",
      "[epoch 6] loss: 3945.81\n",
      "[epoch 7] loss: 3351.74\n",
      "[epoch 8] loss: 2893.50\n",
      "[epoch 9] loss: 2491.36\n",
      "[epoch 10] loss: 2110.65\n",
      "[epoch 11] loss: 1835.77\n",
      "[epoch 12] loss: 1644.49\n",
      "[epoch 13] loss: 1500.29\n",
      "[epoch 14] loss: 1386.49\n",
      "[epoch 15] loss: 1295.05\n",
      "[epoch 16] loss: 1220.36\n",
      "[epoch 17] loss: 1158.34\n",
      "[epoch 18] loss: 1106.01\n",
      "[epoch 19] loss: 1061.19\n",
      "[epoch 20] loss: 1022.31\n",
      "[epoch 21] loss: 988.14\n",
      "[epoch 22] loss: 957.52\n",
      "[epoch 23] loss: 930.04\n",
      "[epoch 24] loss: 905.37\n",
      "[epoch 25] loss: 883.00\n",
      "[epoch 26] loss: 862.53\n",
      "[epoch 27] loss: 843.69\n",
      "[epoch 28] loss: 826.27\n",
      "[epoch 29] loss: 810.16\n",
      "[epoch 30] loss: 795.22\n",
      "[epoch 31] loss: 781.27\n",
      "[epoch 32] loss: 768.22\n",
      "[epoch 33] loss: 756.02\n",
      "[epoch 34] loss: 744.56\n",
      "[epoch 35] loss: 733.74\n",
      "[epoch 36] loss: 723.54\n",
      "[epoch 37] loss: 713.89\n",
      "[epoch 38] loss: 704.74\n",
      "[epoch 39] loss: 696.04\n",
      "[epoch 40] loss: 687.75\n",
      "[epoch 41] loss: 679.83\n",
      "[epoch 42] loss: 672.25\n",
      "[epoch 43] loss: 664.99\n",
      "[epoch 44] loss: 658.03\n",
      "[epoch 45] loss: 651.33\n",
      "[epoch 46] loss: 644.89\n",
      "[epoch 47] loss: 638.68\n",
      "[epoch 48] loss: 632.69\n",
      "[epoch 49] loss: 626.90\n",
      "[epoch 50] loss: 621.30\n",
      "[epoch 51] loss: 615.88\n",
      "[epoch 52] loss: 610.63\n",
      "[epoch 53] loss: 605.54\n",
      "[epoch 54] loss: 600.61\n",
      "[epoch 55] loss: 595.82\n",
      "[epoch 56] loss: 591.16\n",
      "[epoch 57] loss: 586.63\n",
      "[epoch 58] loss: 582.23\n",
      "[epoch 59] loss: 577.94\n",
      "[epoch 60] loss: 573.77\n",
      "[epoch 61] loss: 569.70\n",
      "[epoch 62] loss: 565.73\n",
      "[epoch 63] loss: 561.86\n",
      "[epoch 64] loss: 558.09\n",
      "[epoch 65] loss: 554.40\n",
      "[epoch 66] loss: 550.80\n",
      "[epoch 67] loss: 547.28\n",
      "[epoch 68] loss: 543.84\n",
      "[epoch 69] loss: 540.48\n",
      "[epoch 70] loss: 537.20\n",
      "[epoch 71] loss: 533.98\n",
      "[epoch 72] loss: 530.83\n",
      "[epoch 73] loss: 527.75\n",
      "[epoch 74] loss: 524.74\n",
      "[epoch 75] loss: 521.77\n",
      "[epoch 76] loss: 518.88\n",
      "[epoch 77] loss: 516.04\n",
      "[epoch 78] loss: 513.25\n",
      "[epoch 79] loss: 510.52\n",
      "[epoch 80] loss: 507.83\n",
      "[epoch 81] loss: 505.19\n",
      "[epoch 82] loss: 502.58\n",
      "[epoch 83] loss: 500.02\n",
      "[epoch 84] loss: 497.51\n",
      "[epoch 85] loss: 495.05\n",
      "[epoch 86] loss: 492.62\n",
      "[epoch 87] loss: 490.25\n",
      "[epoch 88] loss: 487.90\n",
      "[epoch 89] loss: 485.61\n",
      "[epoch 90] loss: 483.35\n",
      "[epoch 91] loss: 481.12\n",
      "[epoch 92] loss: 478.95\n",
      "[epoch 93] loss: 476.81\n",
      "[epoch 94] loss: 474.70\n",
      "[epoch 95] loss: 472.63\n",
      "[epoch 96] loss: 470.59\n",
      "[epoch 97] loss: 468.59\n",
      "[epoch 98] loss: 466.62\n",
      "[epoch 99] loss: 464.68\n",
      "[epoch 100] loss: 462.76\n",
      "[epoch 101] loss: 460.89\n",
      "[epoch 102] loss: 459.03\n",
      "[epoch 103] loss: 457.20\n",
      "[epoch 104] loss: 455.37\n",
      "[epoch 105] loss: 453.58\n",
      "[epoch 106] loss: 451.73\n",
      "[epoch 107] loss: 449.36\n",
      "[epoch 108] loss: 447.01\n",
      "[epoch 109] loss: 444.86\n",
      "[epoch 110] loss: 442.96\n",
      "[epoch 111] loss: 441.09\n",
      "[epoch 112] loss: 439.25\n",
      "[epoch 113] loss: 437.50\n",
      "[epoch 114] loss: 435.78\n",
      "[epoch 115] loss: 434.09\n",
      "[epoch 116] loss: 432.42\n",
      "[epoch 117] loss: 430.80\n",
      "[epoch 118] loss: 429.20\n",
      "[epoch 119] loss: 427.61\n",
      "[epoch 120] loss: 426.04\n",
      "[epoch 121] loss: 424.50\n",
      "[epoch 122] loss: 422.97\n",
      "[epoch 123] loss: 421.46\n",
      "[epoch 124] loss: 419.97\n",
      "[epoch 125] loss: 418.49\n",
      "[epoch 126] loss: 417.04\n",
      "[epoch 127] loss: 415.61\n",
      "[epoch 128] loss: 414.17\n",
      "[epoch 129] loss: 412.77\n",
      "[epoch 130] loss: 411.38\n",
      "[epoch 131] loss: 410.01\n",
      "[epoch 132] loss: 408.65\n",
      "[epoch 133] loss: 407.30\n",
      "[epoch 134] loss: 405.97\n",
      "[epoch 135] loss: 404.66\n",
      "[epoch 136] loss: 403.35\n",
      "[epoch 137] loss: 402.07\n",
      "[epoch 138] loss: 400.80\n",
      "[epoch 139] loss: 399.54\n",
      "[epoch 140] loss: 398.30\n",
      "[epoch 141] loss: 397.07\n",
      "[epoch 142] loss: 395.84\n",
      "[epoch 143] loss: 394.63\n",
      "[epoch 144] loss: 393.43\n",
      "[epoch 145] loss: 392.23\n",
      "[epoch 146] loss: 391.05\n",
      "[epoch 147] loss: 389.88\n",
      "[epoch 148] loss: 388.72\n",
      "[epoch 149] loss: 387.57\n",
      "[epoch 150] loss: 386.45\n",
      "##############################Training model 3##############################\n",
      "[epoch 1] loss: 493512.49\n",
      "[epoch 2] loss: 55556.13\n",
      "[epoch 3] loss: 6679.51\n",
      "[epoch 4] loss: 5579.14\n",
      "[epoch 5] loss: 4811.29\n",
      "[epoch 6] loss: 4163.80\n",
      "[epoch 7] loss: 3633.99\n",
      "[epoch 8] loss: 3212.43\n",
      "[epoch 9] loss: 2784.27\n",
      "[epoch 10] loss: 2352.22\n",
      "[epoch 11] loss: 2074.41\n",
      "[epoch 12] loss: 1856.59\n",
      "[epoch 13] loss: 1652.85\n",
      "[epoch 14] loss: 1512.68\n",
      "[epoch 15] loss: 1407.14\n",
      "[epoch 16] loss: 1319.93\n",
      "[epoch 17] loss: 1246.88\n",
      "[epoch 18] loss: 1185.08\n",
      "[epoch 19] loss: 1132.21\n",
      "[epoch 20] loss: 1086.45\n",
      "[epoch 21] loss: 1046.47\n",
      "[epoch 22] loss: 1011.18\n",
      "[epoch 23] loss: 979.74\n",
      "[epoch 24] loss: 951.50\n",
      "[epoch 25] loss: 925.96\n",
      "[epoch 26] loss: 902.69\n",
      "[epoch 27] loss: 881.33\n",
      "[epoch 28] loss: 861.64\n",
      "[epoch 29] loss: 843.41\n",
      "[epoch 30] loss: 826.46\n",
      "[epoch 31] loss: 810.64\n",
      "[epoch 32] loss: 795.84\n",
      "[epoch 33] loss: 781.97\n",
      "[epoch 34] loss: 768.93\n",
      "[epoch 35] loss: 756.63\n",
      "[epoch 36] loss: 745.02\n",
      "[epoch 37] loss: 734.03\n",
      "[epoch 38] loss: 723.60\n",
      "[epoch 39] loss: 713.69\n",
      "[epoch 40] loss: 704.26\n",
      "[epoch 41] loss: 695.27\n",
      "[epoch 42] loss: 686.71\n",
      "[epoch 43] loss: 678.52\n",
      "[epoch 44] loss: 670.69\n",
      "[epoch 45] loss: 663.20\n",
      "[epoch 46] loss: 656.01\n",
      "[epoch 47] loss: 649.12\n",
      "[epoch 48] loss: 642.50\n",
      "[epoch 49] loss: 636.14\n",
      "[epoch 50] loss: 630.02\n",
      "[epoch 51] loss: 624.13\n",
      "[epoch 52] loss: 618.45\n",
      "[epoch 53] loss: 612.97\n",
      "[epoch 54] loss: 607.69\n",
      "[epoch 55] loss: 602.58\n",
      "[epoch 56] loss: 597.65\n",
      "[epoch 57] loss: 592.88\n",
      "[epoch 58] loss: 588.26\n",
      "[epoch 59] loss: 583.78\n",
      "[epoch 60] loss: 579.44\n",
      "[epoch 61] loss: 575.23\n",
      "[epoch 62] loss: 571.14\n",
      "[epoch 63] loss: 567.16\n",
      "[epoch 64] loss: 563.30\n",
      "[epoch 65] loss: 559.54\n",
      "[epoch 66] loss: 555.88\n",
      "[epoch 67] loss: 552.30\n",
      "[epoch 68] loss: 548.82\n",
      "[epoch 69] loss: 545.42\n",
      "[epoch 70] loss: 542.10\n",
      "[epoch 71] loss: 538.85\n",
      "[epoch 72] loss: 535.68\n",
      "[epoch 73] loss: 532.58\n",
      "[epoch 74] loss: 529.55\n",
      "[epoch 75] loss: 526.57\n",
      "[epoch 76] loss: 523.66\n",
      "[epoch 77] loss: 520.81\n",
      "[epoch 78] loss: 518.02\n",
      "[epoch 79] loss: 515.27\n",
      "[epoch 80] loss: 512.59\n",
      "[epoch 81] loss: 509.95\n",
      "[epoch 82] loss: 507.37\n",
      "[epoch 83] loss: 504.82\n",
      "[epoch 84] loss: 502.33\n",
      "[epoch 85] loss: 499.88\n",
      "[epoch 86] loss: 497.48\n",
      "[epoch 87] loss: 495.11\n",
      "[epoch 88] loss: 492.79\n",
      "[epoch 89] loss: 490.51\n",
      "[epoch 90] loss: 488.26\n",
      "[epoch 91] loss: 486.06\n",
      "[epoch 92] loss: 483.88\n",
      "[epoch 93] loss: 481.74\n",
      "[epoch 94] loss: 479.64\n",
      "[epoch 95] loss: 477.57\n",
      "[epoch 96] loss: 475.54\n",
      "[epoch 97] loss: 473.54\n",
      "[epoch 98] loss: 471.57\n",
      "[epoch 99] loss: 469.63\n",
      "[epoch 100] loss: 467.72\n",
      "[epoch 101] loss: 465.84\n",
      "[epoch 102] loss: 463.99\n",
      "[epoch 103] loss: 462.17\n",
      "[epoch 104] loss: 460.37\n",
      "[epoch 105] loss: 458.60\n",
      "[epoch 106] loss: 456.85\n",
      "[epoch 107] loss: 455.13\n",
      "[epoch 108] loss: 453.44\n",
      "[epoch 109] loss: 451.77\n",
      "[epoch 110] loss: 450.13\n",
      "[epoch 111] loss: 448.50\n",
      "[epoch 112] loss: 446.90\n",
      "[epoch 113] loss: 445.32\n",
      "[epoch 114] loss: 443.76\n",
      "[epoch 115] loss: 442.22\n",
      "[epoch 116] loss: 440.71\n",
      "[epoch 117] loss: 439.21\n",
      "[epoch 118] loss: 437.73\n",
      "[epoch 119] loss: 436.28\n",
      "[epoch 120] loss: 434.84\n",
      "[epoch 121] loss: 433.42\n",
      "[epoch 122] loss: 432.03\n",
      "[epoch 123] loss: 430.64\n",
      "[epoch 124] loss: 429.27\n",
      "[epoch 125] loss: 427.93\n",
      "[epoch 126] loss: 426.60\n",
      "[epoch 127] loss: 425.28\n",
      "[epoch 128] loss: 423.98\n",
      "[epoch 129] loss: 422.70\n",
      "[epoch 130] loss: 421.43\n",
      "[epoch 131] loss: 420.18\n",
      "[epoch 132] loss: 418.93\n",
      "[epoch 133] loss: 417.68\n",
      "[epoch 134] loss: 416.45\n",
      "[epoch 135] loss: 415.25\n",
      "[epoch 136] loss: 414.04\n",
      "[epoch 137] loss: 412.86\n",
      "[epoch 138] loss: 411.69\n",
      "[epoch 139] loss: 410.53\n",
      "[epoch 140] loss: 409.39\n",
      "[epoch 141] loss: 408.25\n",
      "[epoch 142] loss: 407.14\n",
      "[epoch 143] loss: 406.04\n",
      "[epoch 144] loss: 404.96\n",
      "[epoch 145] loss: 403.88\n",
      "[epoch 146] loss: 402.82\n",
      "[epoch 147] loss: 401.77\n",
      "[epoch 148] loss: 400.74\n",
      "[epoch 149] loss: 399.69\n",
      "[epoch 150] loss: 398.65\n",
      "##############################TRAINING TERMINATED##############################\n",
      "Mean method:{'mse': 1.4404681279444769, 'mae': 0.5993829480285645}\n",
      "Custom method:{'mse': 1.4375686230660527, 'mae': 0.5892379787957022}\n",
      "2 / 4 fold\n",
      "##############################Training model 1##############################\n",
      "[epoch 1] loss: 711.49\n",
      "[epoch 2] loss: 707.18\n",
      "[epoch 3] loss: 704.30\n",
      "[epoch 4] loss: 701.91\n",
      "[epoch 5] loss: 699.80\n",
      "[epoch 6] loss: 697.89\n",
      "[epoch 7] loss: 696.16\n",
      "[epoch 8] loss: 694.54\n",
      "[epoch 9] loss: 693.00\n",
      "[epoch 10] loss: 691.57\n",
      "[epoch 11] loss: 690.17\n",
      "[epoch 12] loss: 688.86\n",
      "[epoch 13] loss: 687.60\n",
      "[epoch 14] loss: 686.38\n",
      "[epoch 15] loss: 685.21\n",
      "[epoch 16] loss: 684.07\n",
      "[epoch 17] loss: 682.98\n",
      "[epoch 18] loss: 681.91\n",
      "[epoch 19] loss: 680.88\n",
      "[epoch 20] loss: 679.88\n",
      "[epoch 21] loss: 678.89\n",
      "[epoch 22] loss: 677.92\n",
      "[epoch 23] loss: 676.96\n",
      "[epoch 24] loss: 676.01\n",
      "[epoch 25] loss: 675.10\n",
      "[epoch 26] loss: 674.21\n",
      "[epoch 27] loss: 673.34\n",
      "[epoch 28] loss: 672.47\n",
      "[epoch 29] loss: 671.63\n",
      "[epoch 30] loss: 670.80\n",
      "[epoch 31] loss: 669.99\n",
      "[epoch 32] loss: 669.21\n",
      "[epoch 33] loss: 668.40\n",
      "[epoch 34] loss: 667.64\n",
      "[epoch 35] loss: 666.85\n",
      "[epoch 36] loss: 666.10\n",
      "[epoch 37] loss: 665.35\n",
      "[epoch 38] loss: 664.61\n",
      "[epoch 39] loss: 663.90\n",
      "[epoch 40] loss: 663.17\n",
      "[epoch 41] loss: 662.47\n",
      "[epoch 42] loss: 661.76\n",
      "[epoch 43] loss: 661.07\n",
      "[epoch 44] loss: 660.39\n",
      "[epoch 45] loss: 659.69\n",
      "[epoch 46] loss: 659.00\n",
      "[epoch 47] loss: 658.33\n",
      "[epoch 48] loss: 657.66\n",
      "[epoch 49] loss: 657.00\n",
      "[epoch 50] loss: 656.34\n",
      "[epoch 51] loss: 655.71\n",
      "[epoch 52] loss: 655.07\n",
      "[epoch 53] loss: 654.45\n",
      "[epoch 54] loss: 653.82\n",
      "[epoch 55] loss: 653.20\n",
      "[epoch 56] loss: 652.60\n",
      "[epoch 57] loss: 651.99\n",
      "[epoch 58] loss: 651.37\n",
      "[epoch 59] loss: 650.78\n",
      "[epoch 60] loss: 650.18\n",
      "[epoch 61] loss: 649.59\n",
      "[epoch 62] loss: 649.02\n",
      "[epoch 63] loss: 648.44\n",
      "[epoch 64] loss: 647.86\n",
      "[epoch 65] loss: 647.29\n",
      "[epoch 66] loss: 646.72\n",
      "[epoch 67] loss: 646.17\n",
      "[epoch 68] loss: 645.62\n",
      "[epoch 69] loss: 645.08\n",
      "[epoch 70] loss: 644.53\n",
      "[epoch 71] loss: 643.98\n",
      "[epoch 72] loss: 643.46\n",
      "[epoch 73] loss: 642.92\n",
      "[epoch 74] loss: 642.40\n",
      "[epoch 75] loss: 641.90\n",
      "[epoch 76] loss: 641.37\n",
      "[epoch 77] loss: 640.88\n",
      "[epoch 78] loss: 640.38\n",
      "[epoch 79] loss: 639.86\n",
      "[epoch 80] loss: 639.38\n",
      "[epoch 81] loss: 638.89\n",
      "[epoch 82] loss: 638.39\n",
      "[epoch 83] loss: 637.91\n",
      "[epoch 84] loss: 637.44\n",
      "[epoch 85] loss: 636.96\n",
      "[epoch 86] loss: 636.49\n",
      "[epoch 87] loss: 636.00\n",
      "[epoch 88] loss: 635.48\n",
      "[epoch 89] loss: 635.02\n",
      "[epoch 90] loss: 634.54\n",
      "[epoch 91] loss: 634.04\n",
      "[epoch 92] loss: 633.57\n",
      "[epoch 93] loss: 633.08\n",
      "[epoch 94] loss: 632.59\n",
      "[epoch 95] loss: 632.09\n",
      "[epoch 96] loss: 631.62\n",
      "[epoch 97] loss: 631.11\n",
      "[epoch 98] loss: 630.59\n",
      "[epoch 99] loss: 630.06\n",
      "[epoch 100] loss: 629.56\n",
      "[epoch 101] loss: 629.06\n",
      "[epoch 102] loss: 628.54\n",
      "[epoch 103] loss: 628.02\n",
      "[epoch 104] loss: 627.52\n",
      "[epoch 105] loss: 626.99\n",
      "[epoch 106] loss: 626.50\n",
      "[epoch 107] loss: 626.02\n",
      "[epoch 108] loss: 625.54\n",
      "[epoch 109] loss: 625.05\n",
      "[epoch 110] loss: 624.57\n",
      "[epoch 111] loss: 624.11\n",
      "[epoch 112] loss: 623.62\n",
      "[epoch 113] loss: 623.16\n",
      "[epoch 114] loss: 622.70\n",
      "[epoch 115] loss: 622.23\n",
      "[epoch 116] loss: 621.79\n",
      "[epoch 117] loss: 621.34\n",
      "[epoch 118] loss: 620.87\n",
      "[epoch 119] loss: 620.45\n",
      "[epoch 120] loss: 619.99\n",
      "[epoch 121] loss: 619.55\n",
      "[epoch 122] loss: 619.12\n",
      "[epoch 123] loss: 618.68\n",
      "[epoch 124] loss: 618.27\n",
      "[epoch 125] loss: 617.85\n",
      "[epoch 126] loss: 617.41\n",
      "[epoch 127] loss: 617.01\n",
      "[epoch 128] loss: 616.59\n",
      "[epoch 129] loss: 616.17\n",
      "[epoch 130] loss: 615.79\n",
      "[epoch 131] loss: 615.40\n",
      "[epoch 132] loss: 614.97\n",
      "[epoch 133] loss: 614.58\n",
      "[epoch 134] loss: 614.20\n",
      "[epoch 135] loss: 613.79\n",
      "[epoch 136] loss: 613.43\n",
      "[epoch 137] loss: 612.99\n",
      "[epoch 138] loss: 612.63\n",
      "[epoch 139] loss: 612.24\n",
      "[epoch 140] loss: 611.83\n",
      "[epoch 141] loss: 611.46\n",
      "[epoch 142] loss: 611.09\n",
      "[epoch 143] loss: 610.74\n",
      "[epoch 144] loss: 610.33\n",
      "[epoch 145] loss: 609.96\n",
      "[epoch 146] loss: 609.61\n",
      "[epoch 147] loss: 609.25\n",
      "[epoch 148] loss: 608.87\n",
      "[epoch 149] loss: 608.49\n",
      "[epoch 150] loss: 608.12\n",
      "##############################Training model 2##############################\n",
      "[epoch 1] loss: 443.24\n",
      "[epoch 2] loss: 438.23\n",
      "[epoch 3] loss: 434.40\n",
      "[epoch 4] loss: 431.07\n",
      "[epoch 5] loss: 428.07\n",
      "[epoch 6] loss: 425.33\n",
      "[epoch 7] loss: 422.79\n",
      "[epoch 8] loss: 420.40\n",
      "[epoch 9] loss: 418.15\n",
      "[epoch 10] loss: 416.01\n",
      "[epoch 11] loss: 413.97\n",
      "[epoch 12] loss: 412.00\n",
      "[epoch 13] loss: 410.13\n",
      "[epoch 14] loss: 408.33\n",
      "[epoch 15] loss: 406.57\n",
      "[epoch 16] loss: 404.89\n",
      "[epoch 17] loss: 403.25\n",
      "[epoch 18] loss: 401.67\n",
      "[epoch 19] loss: 400.12\n",
      "[epoch 20] loss: 398.60\n",
      "[epoch 21] loss: 397.11\n",
      "[epoch 22] loss: 395.66\n",
      "[epoch 23] loss: 394.26\n",
      "[epoch 24] loss: 392.86\n",
      "[epoch 25] loss: 391.51\n",
      "[epoch 26] loss: 390.16\n",
      "[epoch 27] loss: 388.83\n",
      "[epoch 28] loss: 387.55\n",
      "[epoch 29] loss: 386.27\n",
      "[epoch 30] loss: 385.03\n",
      "[epoch 31] loss: 383.80\n",
      "[epoch 32] loss: 382.58\n",
      "[epoch 33] loss: 381.39\n",
      "[epoch 34] loss: 380.21\n",
      "[epoch 35] loss: 379.04\n",
      "[epoch 36] loss: 377.90\n",
      "[epoch 37] loss: 376.77\n",
      "[epoch 38] loss: 375.64\n",
      "[epoch 39] loss: 374.53\n",
      "[epoch 40] loss: 373.46\n",
      "[epoch 41] loss: 372.38\n",
      "[epoch 42] loss: 371.30\n",
      "[epoch 43] loss: 370.26\n",
      "[epoch 44] loss: 369.22\n",
      "[epoch 45] loss: 368.18\n",
      "[epoch 46] loss: 367.19\n",
      "[epoch 47] loss: 366.19\n",
      "[epoch 48] loss: 365.16\n",
      "[epoch 49] loss: 364.16\n",
      "[epoch 50] loss: 363.20\n",
      "[epoch 51] loss: 362.22\n",
      "[epoch 52] loss: 361.24\n",
      "[epoch 53] loss: 360.27\n",
      "[epoch 54] loss: 359.26\n",
      "[epoch 55] loss: 358.28\n",
      "[epoch 56] loss: 357.31\n",
      "[epoch 57] loss: 356.35\n",
      "[epoch 58] loss: 355.39\n",
      "[epoch 59] loss: 354.46\n",
      "[epoch 60] loss: 353.53\n",
      "[epoch 61] loss: 352.61\n",
      "[epoch 62] loss: 351.68\n",
      "[epoch 63] loss: 350.78\n",
      "[epoch 64] loss: 349.87\n",
      "[epoch 65] loss: 348.98\n",
      "[epoch 66] loss: 348.08\n",
      "[epoch 67] loss: 347.23\n",
      "[epoch 68] loss: 346.38\n",
      "[epoch 69] loss: 345.50\n",
      "[epoch 70] loss: 344.67\n",
      "[epoch 71] loss: 343.83\n",
      "[epoch 72] loss: 342.99\n",
      "[epoch 73] loss: 342.18\n",
      "[epoch 74] loss: 341.35\n",
      "[epoch 75] loss: 340.55\n",
      "[epoch 76] loss: 339.73\n",
      "[epoch 77] loss: 338.91\n",
      "[epoch 78] loss: 338.12\n",
      "[epoch 79] loss: 337.32\n",
      "[epoch 80] loss: 336.51\n",
      "[epoch 81] loss: 335.72\n",
      "[epoch 82] loss: 334.93\n",
      "[epoch 83] loss: 334.16\n",
      "[epoch 84] loss: 333.37\n",
      "[epoch 85] loss: 332.59\n",
      "[epoch 86] loss: 331.80\n",
      "[epoch 87] loss: 331.03\n",
      "[epoch 88] loss: 330.28\n",
      "[epoch 89] loss: 329.50\n",
      "[epoch 90] loss: 328.76\n",
      "[epoch 91] loss: 328.02\n",
      "[epoch 92] loss: 327.26\n",
      "[epoch 93] loss: 326.55\n",
      "[epoch 94] loss: 325.82\n",
      "[epoch 95] loss: 325.09\n",
      "[epoch 96] loss: 324.36\n",
      "[epoch 97] loss: 323.66\n",
      "[epoch 98] loss: 322.92\n",
      "[epoch 99] loss: 322.20\n",
      "[epoch 100] loss: 321.49\n",
      "[epoch 101] loss: 320.80\n",
      "[epoch 102] loss: 320.07\n",
      "[epoch 103] loss: 319.39\n",
      "[epoch 104] loss: 318.71\n",
      "[epoch 105] loss: 318.02\n",
      "[epoch 106] loss: 317.34\n",
      "[epoch 107] loss: 316.66\n",
      "[epoch 108] loss: 315.98\n",
      "[epoch 109] loss: 315.31\n",
      "[epoch 110] loss: 314.63\n",
      "[epoch 111] loss: 313.97\n",
      "[epoch 112] loss: 313.30\n",
      "[epoch 113] loss: 312.65\n",
      "[epoch 114] loss: 312.01\n",
      "[epoch 115] loss: 311.34\n",
      "[epoch 116] loss: 310.70\n",
      "[epoch 117] loss: 310.03\n",
      "[epoch 118] loss: 309.41\n",
      "[epoch 119] loss: 308.78\n",
      "[epoch 120] loss: 308.13\n",
      "[epoch 121] loss: 307.50\n",
      "[epoch 122] loss: 306.89\n",
      "[epoch 123] loss: 306.25\n",
      "[epoch 124] loss: 305.61\n",
      "[epoch 125] loss: 305.00\n",
      "[epoch 126] loss: 304.38\n",
      "[epoch 127] loss: 303.77\n",
      "[epoch 128] loss: 303.14\n",
      "[epoch 129] loss: 302.54\n",
      "[epoch 130] loss: 301.91\n",
      "[epoch 131] loss: 301.33\n",
      "[epoch 132] loss: 300.71\n",
      "[epoch 133] loss: 300.12\n",
      "[epoch 134] loss: 299.51\n",
      "[epoch 135] loss: 298.90\n",
      "[epoch 136] loss: 298.32\n",
      "[epoch 137] loss: 297.70\n",
      "[epoch 138] loss: 297.11\n",
      "[epoch 139] loss: 296.52\n",
      "[epoch 140] loss: 295.96\n",
      "[epoch 141] loss: 295.36\n",
      "[epoch 142] loss: 294.75\n",
      "[epoch 143] loss: 294.19\n",
      "[epoch 144] loss: 293.61\n",
      "[epoch 145] loss: 293.05\n",
      "[epoch 146] loss: 292.46\n",
      "[epoch 147] loss: 291.90\n",
      "[epoch 148] loss: 291.32\n",
      "[epoch 149] loss: 290.76\n",
      "[epoch 150] loss: 290.22\n",
      "##############################Training model 3##############################\n",
      "[epoch 1] loss: 449.88\n",
      "[epoch 2] loss: 444.41\n",
      "[epoch 3] loss: 440.10\n",
      "[epoch 4] loss: 436.57\n",
      "[epoch 5] loss: 433.50\n",
      "[epoch 6] loss: 430.72\n",
      "[epoch 7] loss: 428.16\n",
      "[epoch 8] loss: 425.78\n",
      "[epoch 9] loss: 423.55\n",
      "[epoch 10] loss: 421.43\n",
      "[epoch 11] loss: 419.40\n",
      "[epoch 12] loss: 417.46\n",
      "[epoch 13] loss: 415.59\n",
      "[epoch 14] loss: 413.80\n",
      "[epoch 15] loss: 412.07\n",
      "[epoch 16] loss: 410.41\n",
      "[epoch 17] loss: 408.77\n",
      "[epoch 18] loss: 407.12\n",
      "[epoch 19] loss: 405.53\n",
      "[epoch 20] loss: 403.98\n",
      "[epoch 21] loss: 402.46\n",
      "[epoch 22] loss: 400.98\n",
      "[epoch 23] loss: 399.50\n",
      "[epoch 24] loss: 398.05\n",
      "[epoch 25] loss: 396.63\n",
      "[epoch 26] loss: 395.24\n",
      "[epoch 27] loss: 393.82\n",
      "[epoch 28] loss: 392.41\n",
      "[epoch 29] loss: 391.02\n",
      "[epoch 30] loss: 389.68\n",
      "[epoch 31] loss: 388.36\n",
      "[epoch 32] loss: 387.10\n",
      "[epoch 33] loss: 385.83\n",
      "[epoch 34] loss: 384.60\n",
      "[epoch 35] loss: 383.37\n",
      "[epoch 36] loss: 382.15\n",
      "[epoch 37] loss: 380.94\n",
      "[epoch 38] loss: 379.75\n",
      "[epoch 39] loss: 378.59\n",
      "[epoch 40] loss: 377.42\n",
      "[epoch 41] loss: 376.28\n",
      "[epoch 42] loss: 375.15\n",
      "[epoch 43] loss: 374.02\n",
      "[epoch 44] loss: 372.93\n",
      "[epoch 45] loss: 371.82\n",
      "[epoch 46] loss: 370.72\n",
      "[epoch 47] loss: 369.68\n",
      "[epoch 48] loss: 368.59\n",
      "[epoch 49] loss: 367.55\n",
      "[epoch 50] loss: 366.48\n",
      "[epoch 51] loss: 365.43\n",
      "[epoch 52] loss: 364.38\n",
      "[epoch 53] loss: 363.34\n",
      "[epoch 54] loss: 362.33\n",
      "[epoch 55] loss: 361.29\n",
      "[epoch 56] loss: 360.28\n",
      "[epoch 57] loss: 359.28\n",
      "[epoch 58] loss: 358.33\n",
      "[epoch 59] loss: 357.35\n",
      "[epoch 60] loss: 356.36\n",
      "[epoch 61] loss: 355.40\n",
      "[epoch 62] loss: 354.44\n",
      "[epoch 63] loss: 353.50\n",
      "[epoch 64] loss: 352.57\n",
      "[epoch 65] loss: 351.61\n",
      "[epoch 66] loss: 350.69\n",
      "[epoch 67] loss: 349.77\n",
      "[epoch 68] loss: 348.82\n",
      "[epoch 69] loss: 347.89\n",
      "[epoch 70] loss: 346.97\n",
      "[epoch 71] loss: 346.06\n",
      "[epoch 72] loss: 345.17\n",
      "[epoch 73] loss: 344.24\n",
      "[epoch 74] loss: 343.34\n",
      "[epoch 75] loss: 342.46\n",
      "[epoch 76] loss: 341.57\n",
      "[epoch 77] loss: 340.70\n",
      "[epoch 78] loss: 339.83\n",
      "[epoch 79] loss: 338.97\n",
      "[epoch 80] loss: 338.10\n",
      "[epoch 81] loss: 337.25\n",
      "[epoch 82] loss: 336.39\n",
      "[epoch 83] loss: 335.54\n",
      "[epoch 84] loss: 334.70\n",
      "[epoch 85] loss: 333.86\n",
      "[epoch 86] loss: 333.05\n",
      "[epoch 87] loss: 332.22\n",
      "[epoch 88] loss: 331.40\n",
      "[epoch 89] loss: 330.57\n",
      "[epoch 90] loss: 329.76\n",
      "[epoch 91] loss: 328.94\n",
      "[epoch 92] loss: 328.13\n",
      "[epoch 93] loss: 327.34\n",
      "[epoch 94] loss: 326.51\n",
      "[epoch 95] loss: 325.77\n",
      "[epoch 96] loss: 324.94\n",
      "[epoch 97] loss: 324.17\n",
      "[epoch 98] loss: 323.34\n",
      "[epoch 99] loss: 322.54\n",
      "[epoch 100] loss: 321.75\n",
      "[epoch 101] loss: 320.97\n",
      "[epoch 102] loss: 320.18\n",
      "[epoch 103] loss: 319.40\n",
      "[epoch 104] loss: 318.60\n",
      "[epoch 105] loss: 317.85\n",
      "[epoch 106] loss: 317.06\n",
      "[epoch 107] loss: 316.29\n",
      "[epoch 108] loss: 315.56\n",
      "[epoch 109] loss: 314.81\n",
      "[epoch 110] loss: 314.04\n",
      "[epoch 111] loss: 313.30\n",
      "[epoch 112] loss: 312.57\n",
      "[epoch 113] loss: 311.79\n",
      "[epoch 114] loss: 311.09\n",
      "[epoch 115] loss: 310.38\n",
      "[epoch 116] loss: 309.63\n",
      "[epoch 117] loss: 308.90\n",
      "[epoch 118] loss: 308.16\n",
      "[epoch 119] loss: 307.45\n",
      "[epoch 120] loss: 306.73\n",
      "[epoch 121] loss: 306.01\n",
      "[epoch 122] loss: 305.31\n",
      "[epoch 123] loss: 304.61\n",
      "[epoch 124] loss: 303.90\n",
      "[epoch 125] loss: 303.24\n",
      "[epoch 126] loss: 302.52\n",
      "[epoch 127] loss: 301.82\n",
      "[epoch 128] loss: 301.12\n",
      "[epoch 129] loss: 300.44\n",
      "[epoch 130] loss: 299.78\n",
      "[epoch 131] loss: 299.08\n",
      "[epoch 132] loss: 298.42\n",
      "[epoch 133] loss: 297.71\n",
      "[epoch 134] loss: 297.05\n",
      "[epoch 135] loss: 296.41\n",
      "[epoch 136] loss: 295.71\n",
      "[epoch 137] loss: 295.06\n",
      "[epoch 138] loss: 294.36\n",
      "[epoch 139] loss: 293.71\n",
      "[epoch 140] loss: 293.05\n",
      "[epoch 141] loss: 292.37\n",
      "[epoch 142] loss: 291.75\n",
      "[epoch 143] loss: 291.08\n",
      "[epoch 144] loss: 290.43\n",
      "[epoch 145] loss: 289.77\n",
      "[epoch 146] loss: 289.10\n",
      "[epoch 147] loss: 288.45\n",
      "[epoch 148] loss: 287.80\n",
      "[epoch 149] loss: 287.18\n",
      "[epoch 150] loss: 286.52\n",
      "##############################TRAINING TERMINATED##############################\n",
      "Mean method:{'mse': 0.6714772957639212, 'mae': 0.514259344909668}\n",
      "Custom method:{'mse': 0.651790063732029, 'mae': 0.5042417500751263}\n",
      "3 / 4 fold\n",
      "##############################Training model 1##############################\n",
      "[epoch 1] loss: 643.04\n",
      "[epoch 2] loss: 639.45\n",
      "[epoch 3] loss: 636.93\n",
      "[epoch 4] loss: 634.88\n",
      "[epoch 5] loss: 633.11\n",
      "[epoch 6] loss: 631.60\n",
      "[epoch 7] loss: 630.20\n",
      "[epoch 8] loss: 628.98\n",
      "[epoch 9] loss: 627.84\n",
      "[epoch 10] loss: 626.78\n",
      "[epoch 11] loss: 625.78\n",
      "[epoch 12] loss: 624.81\n",
      "[epoch 13] loss: 623.92\n",
      "[epoch 14] loss: 623.11\n",
      "[epoch 15] loss: 622.27\n",
      "[epoch 16] loss: 621.51\n",
      "[epoch 17] loss: 620.77\n",
      "[epoch 18] loss: 620.04\n",
      "[epoch 19] loss: 619.34\n",
      "[epoch 20] loss: 618.66\n",
      "[epoch 21] loss: 617.98\n",
      "[epoch 22] loss: 617.38\n",
      "[epoch 23] loss: 616.75\n",
      "[epoch 24] loss: 616.10\n",
      "[epoch 25] loss: 615.48\n",
      "[epoch 26] loss: 614.92\n",
      "[epoch 27] loss: 614.34\n",
      "[epoch 28] loss: 613.80\n",
      "[epoch 29] loss: 613.23\n",
      "[epoch 30] loss: 612.67\n",
      "[epoch 31] loss: 612.16\n",
      "[epoch 32] loss: 611.63\n",
      "[epoch 33] loss: 611.09\n",
      "[epoch 34] loss: 610.58\n",
      "[epoch 35] loss: 610.10\n",
      "[epoch 36] loss: 609.55\n",
      "[epoch 37] loss: 609.09\n",
      "[epoch 38] loss: 608.57\n",
      "[epoch 39] loss: 608.12\n",
      "[epoch 40] loss: 607.67\n",
      "[epoch 41] loss: 607.20\n",
      "[epoch 42] loss: 606.77\n",
      "[epoch 43] loss: 606.28\n",
      "[epoch 44] loss: 605.87\n",
      "[epoch 45] loss: 605.41\n",
      "[epoch 46] loss: 604.95\n",
      "[epoch 47] loss: 604.54\n",
      "[epoch 48] loss: 604.11\n",
      "[epoch 49] loss: 603.67\n",
      "[epoch 50] loss: 603.24\n",
      "[epoch 51] loss: 602.84\n",
      "[epoch 52] loss: 602.42\n",
      "[epoch 53] loss: 602.01\n",
      "[epoch 54] loss: 601.59\n",
      "[epoch 55] loss: 601.17\n",
      "[epoch 56] loss: 600.79\n",
      "[epoch 57] loss: 600.38\n",
      "[epoch 58] loss: 599.96\n",
      "[epoch 59] loss: 599.56\n",
      "[epoch 60] loss: 599.17\n",
      "[epoch 61] loss: 598.78\n",
      "[epoch 62] loss: 598.35\n",
      "[epoch 63] loss: 597.94\n",
      "[epoch 64] loss: 597.57\n",
      "[epoch 65] loss: 597.21\n",
      "[epoch 66] loss: 596.78\n",
      "[epoch 67] loss: 596.36\n",
      "[epoch 68] loss: 596.03\n",
      "[epoch 69] loss: 595.64\n",
      "[epoch 70] loss: 595.25\n",
      "[epoch 71] loss: 594.85\n",
      "[epoch 72] loss: 594.47\n",
      "[epoch 73] loss: 594.07\n",
      "[epoch 74] loss: 593.69\n",
      "[epoch 75] loss: 593.29\n",
      "[epoch 76] loss: 592.90\n",
      "[epoch 77] loss: 592.50\n",
      "[epoch 78] loss: 592.09\n",
      "[epoch 79] loss: 591.70\n",
      "[epoch 80] loss: 591.33\n",
      "[epoch 81] loss: 590.94\n",
      "[epoch 82] loss: 590.53\n",
      "[epoch 83] loss: 590.15\n",
      "[epoch 84] loss: 589.79\n",
      "[epoch 85] loss: 589.36\n",
      "[epoch 86] loss: 589.00\n",
      "[epoch 87] loss: 588.65\n",
      "[epoch 88] loss: 588.30\n",
      "[epoch 89] loss: 587.91\n",
      "[epoch 90] loss: 587.54\n",
      "[epoch 91] loss: 587.20\n",
      "[epoch 92] loss: 586.83\n",
      "[epoch 93] loss: 586.45\n",
      "[epoch 94] loss: 586.11\n",
      "[epoch 95] loss: 585.72\n",
      "[epoch 96] loss: 585.35\n",
      "[epoch 97] loss: 585.02\n",
      "[epoch 98] loss: 584.69\n",
      "[epoch 99] loss: 584.34\n",
      "[epoch 100] loss: 584.01\n",
      "[epoch 101] loss: 583.66\n",
      "[epoch 102] loss: 583.32\n",
      "[epoch 103] loss: 582.90\n",
      "[epoch 104] loss: 582.56\n",
      "[epoch 105] loss: 582.27\n",
      "[epoch 106] loss: 581.92\n",
      "[epoch 107] loss: 581.56\n",
      "[epoch 108] loss: 581.26\n",
      "[epoch 109] loss: 580.93\n",
      "[epoch 110] loss: 580.56\n",
      "[epoch 111] loss: 580.20\n",
      "[epoch 112] loss: 579.90\n",
      "[epoch 113] loss: 579.56\n",
      "[epoch 114] loss: 579.24\n",
      "[epoch 115] loss: 578.94\n",
      "[epoch 116] loss: 578.54\n",
      "[epoch 117] loss: 578.23\n",
      "[epoch 118] loss: 577.88\n",
      "[epoch 119] loss: 577.57\n",
      "[epoch 120] loss: 577.25\n",
      "[epoch 121] loss: 576.87\n",
      "[epoch 122] loss: 576.56\n",
      "[epoch 123] loss: 576.20\n",
      "[epoch 124] loss: 575.88\n",
      "[epoch 125] loss: 575.56\n",
      "[epoch 126] loss: 575.18\n",
      "[epoch 127] loss: 574.86\n",
      "[epoch 128] loss: 574.58\n",
      "[epoch 129] loss: 574.22\n",
      "[epoch 130] loss: 573.89\n",
      "[epoch 131] loss: 573.60\n",
      "[epoch 132] loss: 573.30\n",
      "[epoch 133] loss: 572.96\n",
      "[epoch 134] loss: 572.60\n",
      "[epoch 135] loss: 572.30\n",
      "[epoch 136] loss: 571.99\n",
      "[epoch 137] loss: 571.62\n",
      "[epoch 138] loss: 571.36\n",
      "[epoch 139] loss: 571.00\n",
      "[epoch 140] loss: 570.73\n",
      "[epoch 141] loss: 570.42\n",
      "[epoch 142] loss: 570.05\n",
      "[epoch 143] loss: 569.72\n",
      "[epoch 144] loss: 569.45\n",
      "[epoch 145] loss: 569.10\n",
      "[epoch 146] loss: 568.77\n",
      "[epoch 147] loss: 568.53\n",
      "[epoch 148] loss: 568.20\n",
      "[epoch 149] loss: 567.87\n",
      "[epoch 150] loss: 567.55\n",
      "##############################Training model 2##############################\n",
      "[epoch 1] loss: 342.93\n",
      "[epoch 2] loss: 338.98\n",
      "[epoch 3] loss: 336.02\n",
      "[epoch 4] loss: 333.52\n",
      "[epoch 5] loss: 331.39\n",
      "[epoch 6] loss: 329.43\n",
      "[epoch 7] loss: 327.67\n",
      "[epoch 8] loss: 326.04\n",
      "[epoch 9] loss: 324.53\n",
      "[epoch 10] loss: 323.13\n",
      "[epoch 11] loss: 321.79\n",
      "[epoch 12] loss: 320.53\n",
      "[epoch 13] loss: 319.30\n",
      "[epoch 14] loss: 318.11\n",
      "[epoch 15] loss: 317.01\n",
      "[epoch 16] loss: 315.90\n",
      "[epoch 17] loss: 314.87\n",
      "[epoch 18] loss: 313.84\n",
      "[epoch 19] loss: 312.86\n",
      "[epoch 20] loss: 311.86\n",
      "[epoch 21] loss: 310.93\n",
      "[epoch 22] loss: 310.00\n",
      "[epoch 23] loss: 309.10\n",
      "[epoch 24] loss: 308.17\n",
      "[epoch 25] loss: 307.30\n",
      "[epoch 26] loss: 306.43\n",
      "[epoch 27] loss: 305.58\n",
      "[epoch 28] loss: 304.70\n",
      "[epoch 29] loss: 303.85\n",
      "[epoch 30] loss: 302.92\n",
      "[epoch 31] loss: 302.00\n",
      "[epoch 32] loss: 301.13\n",
      "[epoch 33] loss: 300.28\n",
      "[epoch 34] loss: 299.45\n",
      "[epoch 35] loss: 298.59\n",
      "[epoch 36] loss: 297.78\n",
      "[epoch 37] loss: 296.99\n",
      "[epoch 38] loss: 296.20\n",
      "[epoch 39] loss: 295.43\n",
      "[epoch 40] loss: 294.67\n",
      "[epoch 41] loss: 293.92\n",
      "[epoch 42] loss: 293.17\n",
      "[epoch 43] loss: 292.44\n",
      "[epoch 44] loss: 291.74\n",
      "[epoch 45] loss: 290.98\n",
      "[epoch 46] loss: 290.30\n",
      "[epoch 47] loss: 289.58\n",
      "[epoch 48] loss: 288.89\n",
      "[epoch 49] loss: 288.19\n",
      "[epoch 50] loss: 287.51\n",
      "[epoch 51] loss: 286.82\n",
      "[epoch 52] loss: 286.13\n",
      "[epoch 53] loss: 285.46\n",
      "[epoch 54] loss: 284.80\n",
      "[epoch 55] loss: 284.12\n",
      "[epoch 56] loss: 283.46\n",
      "[epoch 57] loss: 282.80\n",
      "[epoch 58] loss: 282.15\n",
      "[epoch 59] loss: 281.51\n",
      "[epoch 60] loss: 280.85\n",
      "[epoch 61] loss: 280.23\n",
      "[epoch 62] loss: 279.58\n",
      "[epoch 63] loss: 278.97\n",
      "[epoch 64] loss: 278.31\n",
      "[epoch 65] loss: 277.72\n",
      "[epoch 66] loss: 277.07\n",
      "[epoch 67] loss: 276.48\n",
      "[epoch 68] loss: 275.88\n",
      "[epoch 69] loss: 275.27\n",
      "[epoch 70] loss: 274.67\n",
      "[epoch 71] loss: 274.08\n",
      "[epoch 72] loss: 273.46\n",
      "[epoch 73] loss: 272.87\n",
      "[epoch 74] loss: 272.27\n",
      "[epoch 75] loss: 271.68\n",
      "[epoch 76] loss: 271.10\n",
      "[epoch 77] loss: 270.52\n",
      "[epoch 78] loss: 269.93\n",
      "[epoch 79] loss: 269.37\n",
      "[epoch 80] loss: 268.78\n",
      "[epoch 81] loss: 268.21\n",
      "[epoch 82] loss: 267.61\n",
      "[epoch 83] loss: 267.06\n",
      "[epoch 84] loss: 266.49\n",
      "[epoch 85] loss: 265.93\n",
      "[epoch 86] loss: 265.41\n",
      "[epoch 87] loss: 264.84\n",
      "[epoch 88] loss: 264.25\n",
      "[epoch 89] loss: 263.71\n",
      "[epoch 90] loss: 263.18\n",
      "[epoch 91] loss: 262.63\n",
      "[epoch 92] loss: 262.07\n",
      "[epoch 93] loss: 261.53\n",
      "[epoch 94] loss: 261.00\n",
      "[epoch 95] loss: 260.46\n",
      "[epoch 96] loss: 259.96\n",
      "[epoch 97] loss: 259.42\n",
      "[epoch 98] loss: 258.88\n",
      "[epoch 99] loss: 258.39\n",
      "[epoch 100] loss: 257.88\n",
      "[epoch 101] loss: 257.34\n",
      "[epoch 102] loss: 256.83\n",
      "[epoch 103] loss: 256.34\n",
      "[epoch 104] loss: 255.84\n",
      "[epoch 105] loss: 255.33\n",
      "[epoch 106] loss: 254.83\n",
      "[epoch 107] loss: 254.33\n",
      "[epoch 108] loss: 253.84\n",
      "[epoch 109] loss: 253.37\n",
      "[epoch 110] loss: 252.91\n",
      "[epoch 111] loss: 252.41\n",
      "[epoch 112] loss: 251.92\n",
      "[epoch 113] loss: 251.42\n",
      "[epoch 114] loss: 250.96\n",
      "[epoch 115] loss: 250.48\n",
      "[epoch 116] loss: 250.02\n",
      "[epoch 117] loss: 249.53\n",
      "[epoch 118] loss: 249.07\n",
      "[epoch 119] loss: 248.60\n",
      "[epoch 120] loss: 248.15\n",
      "[epoch 121] loss: 247.69\n",
      "[epoch 122] loss: 247.19\n",
      "[epoch 123] loss: 246.74\n",
      "[epoch 124] loss: 246.29\n",
      "[epoch 125] loss: 245.85\n",
      "[epoch 126] loss: 245.36\n",
      "[epoch 127] loss: 244.93\n",
      "[epoch 128] loss: 244.49\n",
      "[epoch 129] loss: 244.06\n",
      "[epoch 130] loss: 243.59\n",
      "[epoch 131] loss: 243.15\n",
      "[epoch 132] loss: 242.73\n",
      "[epoch 133] loss: 242.28\n",
      "[epoch 134] loss: 241.85\n",
      "[epoch 135] loss: 241.39\n",
      "[epoch 136] loss: 240.97\n",
      "[epoch 137] loss: 240.53\n",
      "[epoch 138] loss: 240.10\n",
      "[epoch 139] loss: 239.66\n",
      "[epoch 140] loss: 239.26\n",
      "[epoch 141] loss: 238.82\n",
      "[epoch 142] loss: 238.41\n",
      "[epoch 143] loss: 237.96\n",
      "[epoch 144] loss: 237.57\n",
      "[epoch 145] loss: 237.15\n",
      "[epoch 146] loss: 236.73\n",
      "[epoch 147] loss: 236.33\n",
      "[epoch 148] loss: 235.90\n",
      "[epoch 149] loss: 235.50\n",
      "[epoch 150] loss: 235.10\n",
      "##############################Training model 3##############################\n",
      "[epoch 1] loss: 341.08\n",
      "[epoch 2] loss: 337.09\n",
      "[epoch 3] loss: 334.06\n",
      "[epoch 4] loss: 331.48\n",
      "[epoch 5] loss: 329.21\n",
      "[epoch 6] loss: 327.15\n",
      "[epoch 7] loss: 325.29\n",
      "[epoch 8] loss: 323.57\n",
      "[epoch 9] loss: 321.96\n",
      "[epoch 10] loss: 320.43\n",
      "[epoch 11] loss: 318.97\n",
      "[epoch 12] loss: 317.62\n",
      "[epoch 13] loss: 316.27\n",
      "[epoch 14] loss: 315.01\n",
      "[epoch 15] loss: 313.79\n",
      "[epoch 16] loss: 312.58\n",
      "[epoch 17] loss: 311.43\n",
      "[epoch 18] loss: 310.35\n",
      "[epoch 19] loss: 309.26\n",
      "[epoch 20] loss: 308.18\n",
      "[epoch 21] loss: 307.14\n",
      "[epoch 22] loss: 306.13\n",
      "[epoch 23] loss: 305.14\n",
      "[epoch 24] loss: 304.17\n",
      "[epoch 25] loss: 303.20\n",
      "[epoch 26] loss: 302.25\n",
      "[epoch 27] loss: 301.35\n",
      "[epoch 28] loss: 300.43\n",
      "[epoch 29] loss: 299.51\n",
      "[epoch 30] loss: 298.63\n",
      "[epoch 31] loss: 297.76\n",
      "[epoch 32] loss: 296.91\n",
      "[epoch 33] loss: 296.05\n",
      "[epoch 34] loss: 295.25\n",
      "[epoch 35] loss: 294.42\n",
      "[epoch 36] loss: 293.59\n",
      "[epoch 37] loss: 292.80\n",
      "[epoch 38] loss: 292.00\n",
      "[epoch 39] loss: 291.22\n",
      "[epoch 40] loss: 290.45\n",
      "[epoch 41] loss: 289.68\n",
      "[epoch 42] loss: 288.92\n",
      "[epoch 43] loss: 288.16\n",
      "[epoch 44] loss: 287.44\n",
      "[epoch 45] loss: 286.67\n",
      "[epoch 46] loss: 285.97\n",
      "[epoch 47] loss: 285.24\n",
      "[epoch 48] loss: 284.54\n",
      "[epoch 49] loss: 283.80\n",
      "[epoch 50] loss: 283.11\n",
      "[epoch 51] loss: 282.40\n",
      "[epoch 52] loss: 281.71\n",
      "[epoch 53] loss: 281.03\n",
      "[epoch 54] loss: 280.31\n",
      "[epoch 55] loss: 279.64\n",
      "[epoch 56] loss: 278.97\n",
      "[epoch 57] loss: 278.30\n",
      "[epoch 58] loss: 277.67\n",
      "[epoch 59] loss: 276.98\n",
      "[epoch 60] loss: 276.36\n",
      "[epoch 61] loss: 275.69\n",
      "[epoch 62] loss: 275.06\n",
      "[epoch 63] loss: 274.43\n",
      "[epoch 64] loss: 273.81\n",
      "[epoch 65] loss: 273.16\n",
      "[epoch 66] loss: 272.54\n",
      "[epoch 67] loss: 271.92\n",
      "[epoch 68] loss: 271.32\n",
      "[epoch 69] loss: 270.71\n",
      "[epoch 70] loss: 270.06\n",
      "[epoch 71] loss: 269.51\n",
      "[epoch 72] loss: 268.85\n",
      "[epoch 73] loss: 268.25\n",
      "[epoch 74] loss: 267.65\n",
      "[epoch 75] loss: 267.08\n",
      "[epoch 76] loss: 266.49\n",
      "[epoch 77] loss: 265.90\n",
      "[epoch 78] loss: 265.32\n",
      "[epoch 79] loss: 264.74\n",
      "[epoch 80] loss: 264.18\n",
      "[epoch 81] loss: 263.63\n",
      "[epoch 82] loss: 263.08\n",
      "[epoch 83] loss: 262.50\n",
      "[epoch 84] loss: 261.93\n",
      "[epoch 85] loss: 261.38\n",
      "[epoch 86] loss: 260.86\n",
      "[epoch 87] loss: 260.32\n",
      "[epoch 88] loss: 259.75\n",
      "[epoch 89] loss: 259.21\n",
      "[epoch 90] loss: 258.68\n",
      "[epoch 91] loss: 258.16\n",
      "[epoch 92] loss: 257.65\n",
      "[epoch 93] loss: 257.07\n",
      "[epoch 94] loss: 256.58\n",
      "[epoch 95] loss: 256.04\n",
      "[epoch 96] loss: 255.48\n",
      "[epoch 97] loss: 255.00\n",
      "[epoch 98] loss: 254.48\n",
      "[epoch 99] loss: 253.96\n",
      "[epoch 100] loss: 253.45\n",
      "[epoch 101] loss: 252.93\n",
      "[epoch 102] loss: 252.42\n",
      "[epoch 103] loss: 251.92\n",
      "[epoch 104] loss: 251.43\n",
      "[epoch 105] loss: 250.93\n",
      "[epoch 106] loss: 250.42\n",
      "[epoch 107] loss: 249.95\n",
      "[epoch 108] loss: 249.41\n",
      "[epoch 109] loss: 248.96\n",
      "[epoch 110] loss: 248.46\n",
      "[epoch 111] loss: 247.96\n",
      "[epoch 112] loss: 247.47\n",
      "[epoch 113] loss: 246.99\n",
      "[epoch 114] loss: 246.49\n",
      "[epoch 115] loss: 246.05\n",
      "[epoch 116] loss: 245.54\n",
      "[epoch 117] loss: 245.08\n",
      "[epoch 118] loss: 244.61\n",
      "[epoch 119] loss: 244.14\n",
      "[epoch 120] loss: 243.68\n",
      "[epoch 121] loss: 243.17\n",
      "[epoch 122] loss: 242.78\n",
      "[epoch 123] loss: 242.30\n",
      "[epoch 124] loss: 241.84\n",
      "[epoch 125] loss: 241.35\n",
      "[epoch 126] loss: 240.94\n",
      "[epoch 127] loss: 240.46\n",
      "[epoch 128] loss: 240.03\n",
      "[epoch 129] loss: 239.59\n",
      "[epoch 130] loss: 239.13\n",
      "[epoch 131] loss: 238.69\n",
      "[epoch 132] loss: 238.24\n",
      "[epoch 133] loss: 237.78\n",
      "[epoch 134] loss: 237.37\n",
      "[epoch 135] loss: 236.91\n",
      "[epoch 136] loss: 236.51\n",
      "[epoch 137] loss: 236.05\n",
      "[epoch 138] loss: 235.63\n",
      "[epoch 139] loss: 235.22\n",
      "[epoch 140] loss: 234.80\n",
      "[epoch 141] loss: 234.32\n",
      "[epoch 142] loss: 233.91\n",
      "[epoch 143] loss: 233.51\n",
      "[epoch 144] loss: 233.09\n",
      "[epoch 145] loss: 232.61\n",
      "[epoch 146] loss: 232.25\n",
      "[epoch 147] loss: 231.83\n",
      "[epoch 148] loss: 231.40\n",
      "[epoch 149] loss: 231.00\n",
      "[epoch 150] loss: 230.57\n",
      "##############################TRAINING TERMINATED##############################\n",
      "Mean method:{'mse': 1.0751183099330375, 'mae': 0.49044961387125646}\n",
      "Custom method:{'mse': 1.085343016762427, 'mae': 0.481548548393293}\n",
      "4 / 4 fold\n",
      "##############################Training model 1##############################\n",
      "[epoch 1] loss: 602.12\n",
      "[epoch 2] loss: 598.60\n",
      "[epoch 3] loss: 596.25\n",
      "[epoch 4] loss: 594.37\n",
      "[epoch 5] loss: 592.72\n",
      "[epoch 6] loss: 591.21\n",
      "[epoch 7] loss: 589.89\n",
      "[epoch 8] loss: 588.64\n",
      "[epoch 9] loss: 587.50\n",
      "[epoch 10] loss: 586.52\n",
      "[epoch 11] loss: 585.51\n",
      "[epoch 12] loss: 584.55\n",
      "[epoch 13] loss: 583.66\n",
      "[epoch 14] loss: 582.79\n",
      "[epoch 15] loss: 582.00\n",
      "[epoch 16] loss: 581.22\n",
      "[epoch 17] loss: 580.47\n",
      "[epoch 18] loss: 579.73\n",
      "[epoch 19] loss: 579.08\n",
      "[epoch 20] loss: 578.40\n",
      "[epoch 21] loss: 577.71\n",
      "[epoch 22] loss: 577.06\n",
      "[epoch 23] loss: 576.45\n",
      "[epoch 24] loss: 575.92\n",
      "[epoch 25] loss: 575.23\n",
      "[epoch 26] loss: 574.67\n",
      "[epoch 27] loss: 574.18\n",
      "[epoch 28] loss: 573.56\n",
      "[epoch 29] loss: 573.01\n",
      "[epoch 30] loss: 572.54\n",
      "[epoch 31] loss: 571.96\n",
      "[epoch 32] loss: 571.40\n",
      "[epoch 33] loss: 570.91\n",
      "[epoch 34] loss: 570.47\n",
      "[epoch 35] loss: 569.89\n",
      "[epoch 36] loss: 569.38\n",
      "[epoch 37] loss: 568.91\n",
      "[epoch 38] loss: 568.50\n",
      "[epoch 39] loss: 567.98\n",
      "[epoch 40] loss: 567.54\n",
      "[epoch 41] loss: 567.15\n",
      "[epoch 42] loss: 566.57\n",
      "[epoch 43] loss: 566.12\n",
      "[epoch 44] loss: 565.81\n",
      "[epoch 45] loss: 565.24\n",
      "[epoch 46] loss: 564.91\n",
      "[epoch 47] loss: 564.42\n",
      "[epoch 48] loss: 563.98\n",
      "[epoch 49] loss: 563.61\n",
      "[epoch 50] loss: 563.33\n",
      "[epoch 51] loss: 562.83\n",
      "[epoch 52] loss: 562.42\n",
      "[epoch 53] loss: 562.01\n",
      "[epoch 54] loss: 561.69\n",
      "[epoch 55] loss: 561.21\n",
      "[epoch 56] loss: 560.86\n",
      "[epoch 57] loss: 560.51\n",
      "[epoch 58] loss: 560.09\n",
      "[epoch 59] loss: 559.75\n",
      "[epoch 60] loss: 559.40\n",
      "[epoch 61] loss: 558.96\n",
      "[epoch 62] loss: 558.63\n",
      "[epoch 63] loss: 558.23\n",
      "[epoch 64] loss: 557.90\n",
      "[epoch 65] loss: 557.49\n",
      "[epoch 66] loss: 557.14\n",
      "[epoch 67] loss: 556.77\n",
      "[epoch 68] loss: 556.44\n",
      "[epoch 69] loss: 556.06\n",
      "[epoch 70] loss: 555.70\n",
      "[epoch 71] loss: 555.40\n",
      "[epoch 72] loss: 555.00\n",
      "[epoch 73] loss: 554.62\n",
      "[epoch 74] loss: 554.28\n",
      "[epoch 75] loss: 553.95\n",
      "[epoch 76] loss: 553.57\n",
      "[epoch 77] loss: 553.25\n",
      "[epoch 78] loss: 552.90\n",
      "[epoch 79] loss: 552.56\n",
      "[epoch 80] loss: 552.29\n",
      "[epoch 81] loss: 551.89\n",
      "[epoch 82] loss: 551.59\n",
      "[epoch 83] loss: 551.26\n",
      "[epoch 84] loss: 550.91\n",
      "[epoch 85] loss: 550.59\n",
      "[epoch 86] loss: 550.25\n",
      "[epoch 87] loss: 549.95\n",
      "[epoch 88] loss: 549.61\n",
      "[epoch 89] loss: 549.29\n",
      "[epoch 90] loss: 548.99\n",
      "[epoch 91] loss: 548.65\n",
      "[epoch 92] loss: 548.33\n",
      "[epoch 93] loss: 548.00\n",
      "[epoch 94] loss: 547.68\n",
      "[epoch 95] loss: 547.38\n",
      "[epoch 96] loss: 547.04\n",
      "[epoch 97] loss: 546.73\n",
      "[epoch 98] loss: 546.46\n",
      "[epoch 99] loss: 546.12\n",
      "[epoch 100] loss: 545.81\n",
      "[epoch 101] loss: 545.55\n",
      "[epoch 102] loss: 545.20\n",
      "[epoch 103] loss: 544.97\n",
      "[epoch 104] loss: 544.57\n",
      "[epoch 105] loss: 544.34\n",
      "[epoch 106] loss: 544.00\n",
      "[epoch 107] loss: 543.74\n",
      "[epoch 108] loss: 543.40\n",
      "[epoch 109] loss: 543.16\n",
      "[epoch 110] loss: 542.85\n",
      "[epoch 111] loss: 542.55\n",
      "[epoch 112] loss: 542.29\n",
      "[epoch 113] loss: 542.07\n",
      "[epoch 114] loss: 541.70\n",
      "[epoch 115] loss: 541.50\n",
      "[epoch 116] loss: 541.19\n",
      "[epoch 117] loss: 540.92\n",
      "[epoch 118] loss: 540.65\n",
      "[epoch 119] loss: 540.38\n",
      "[epoch 120] loss: 540.12\n",
      "[epoch 121] loss: 539.80\n",
      "[epoch 122] loss: 539.61\n",
      "[epoch 123] loss: 539.27\n",
      "[epoch 124] loss: 539.02\n",
      "[epoch 125] loss: 538.83\n",
      "[epoch 126] loss: 538.55\n",
      "[epoch 127] loss: 538.25\n",
      "[epoch 128] loss: 537.98\n",
      "[epoch 129] loss: 537.70\n",
      "[epoch 130] loss: 537.40\n",
      "[epoch 131] loss: 537.20\n",
      "[epoch 132] loss: 536.92\n",
      "[epoch 133] loss: 536.67\n",
      "[epoch 134] loss: 536.32\n",
      "[epoch 135] loss: 536.10\n",
      "[epoch 136] loss: 535.79\n",
      "[epoch 137] loss: 535.60\n",
      "[epoch 138] loss: 535.29\n",
      "[epoch 139] loss: 534.99\n",
      "[epoch 140] loss: 534.69\n",
      "[epoch 141] loss: 534.48\n",
      "[epoch 142] loss: 534.18\n",
      "[epoch 143] loss: 533.88\n",
      "[epoch 144] loss: 533.65\n",
      "[epoch 145] loss: 533.32\n",
      "[epoch 146] loss: 533.11\n",
      "[epoch 147] loss: 532.80\n",
      "[epoch 148] loss: 532.59\n",
      "[epoch 149] loss: 532.27\n",
      "[epoch 150] loss: 531.88\n",
      "##############################Training model 2##############################\n",
      "[epoch 1] loss: 281.58\n",
      "[epoch 2] loss: 278.01\n",
      "[epoch 3] loss: 275.42\n",
      "[epoch 4] loss: 273.26\n",
      "[epoch 5] loss: 271.38\n",
      "[epoch 6] loss: 269.70\n",
      "[epoch 7] loss: 268.15\n",
      "[epoch 8] loss: 266.74\n",
      "[epoch 9] loss: 265.41\n",
      "[epoch 10] loss: 264.17\n",
      "[epoch 11] loss: 263.01\n",
      "[epoch 12] loss: 261.90\n",
      "[epoch 13] loss: 260.83\n",
      "[epoch 14] loss: 259.85\n",
      "[epoch 15] loss: 258.87\n",
      "[epoch 16] loss: 257.88\n",
      "[epoch 17] loss: 256.98\n",
      "[epoch 18] loss: 256.13\n",
      "[epoch 19] loss: 255.27\n",
      "[epoch 20] loss: 254.45\n",
      "[epoch 21] loss: 253.64\n",
      "[epoch 22] loss: 252.87\n",
      "[epoch 23] loss: 252.10\n",
      "[epoch 24] loss: 251.35\n",
      "[epoch 25] loss: 250.60\n",
      "[epoch 26] loss: 249.87\n",
      "[epoch 27] loss: 249.21\n",
      "[epoch 28] loss: 248.47\n",
      "[epoch 29] loss: 247.82\n",
      "[epoch 30] loss: 247.18\n",
      "[epoch 31] loss: 246.53\n",
      "[epoch 32] loss: 245.87\n",
      "[epoch 33] loss: 245.24\n",
      "[epoch 34] loss: 244.61\n",
      "[epoch 35] loss: 243.99\n",
      "[epoch 36] loss: 243.38\n",
      "[epoch 37] loss: 242.82\n",
      "[epoch 38] loss: 242.19\n",
      "[epoch 39] loss: 241.62\n",
      "[epoch 40] loss: 241.00\n",
      "[epoch 41] loss: 240.47\n",
      "[epoch 42] loss: 239.91\n",
      "[epoch 43] loss: 239.32\n",
      "[epoch 44] loss: 238.77\n",
      "[epoch 45] loss: 238.23\n",
      "[epoch 46] loss: 237.67\n",
      "[epoch 47] loss: 237.17\n",
      "[epoch 48] loss: 236.62\n",
      "[epoch 49] loss: 236.07\n",
      "[epoch 50] loss: 235.58\n",
      "[epoch 51] loss: 235.05\n",
      "[epoch 52] loss: 234.51\n",
      "[epoch 53] loss: 234.03\n",
      "[epoch 54] loss: 233.51\n",
      "[epoch 55] loss: 233.02\n",
      "[epoch 56] loss: 232.52\n",
      "[epoch 57] loss: 232.01\n",
      "[epoch 58] loss: 231.52\n",
      "[epoch 59] loss: 231.03\n",
      "[epoch 60] loss: 230.57\n",
      "[epoch 61] loss: 230.06\n",
      "[epoch 62] loss: 229.58\n",
      "[epoch 63] loss: 229.15\n",
      "[epoch 64] loss: 228.63\n",
      "[epoch 65] loss: 228.22\n",
      "[epoch 66] loss: 227.71\n",
      "[epoch 67] loss: 227.27\n",
      "[epoch 68] loss: 226.84\n",
      "[epoch 69] loss: 226.40\n",
      "[epoch 70] loss: 225.93\n",
      "[epoch 71] loss: 225.47\n",
      "[epoch 72] loss: 225.05\n",
      "[epoch 73] loss: 224.66\n",
      "[epoch 74] loss: 224.16\n",
      "[epoch 75] loss: 223.76\n",
      "[epoch 76] loss: 223.32\n",
      "[epoch 77] loss: 222.89\n",
      "[epoch 78] loss: 222.49\n",
      "[epoch 79] loss: 222.05\n",
      "[epoch 80] loss: 221.58\n",
      "[epoch 81] loss: 221.18\n",
      "[epoch 82] loss: 220.77\n",
      "[epoch 83] loss: 220.37\n",
      "[epoch 84] loss: 219.97\n",
      "[epoch 85] loss: 219.55\n",
      "[epoch 86] loss: 219.13\n",
      "[epoch 87] loss: 218.72\n",
      "[epoch 88] loss: 218.35\n",
      "[epoch 89] loss: 217.93\n",
      "[epoch 90] loss: 217.53\n",
      "[epoch 91] loss: 217.15\n",
      "[epoch 92] loss: 216.76\n",
      "[epoch 93] loss: 216.33\n",
      "[epoch 94] loss: 215.98\n",
      "[epoch 95] loss: 215.61\n",
      "[epoch 96] loss: 215.24\n",
      "[epoch 97] loss: 214.85\n",
      "[epoch 98] loss: 214.46\n",
      "[epoch 99] loss: 214.07\n",
      "[epoch 100] loss: 213.68\n",
      "[epoch 101] loss: 213.35\n",
      "[epoch 102] loss: 212.97\n",
      "[epoch 103] loss: 212.59\n",
      "[epoch 104] loss: 212.21\n",
      "[epoch 105] loss: 211.83\n",
      "[epoch 106] loss: 211.48\n",
      "[epoch 107] loss: 211.14\n",
      "[epoch 108] loss: 210.77\n",
      "[epoch 109] loss: 210.42\n",
      "[epoch 110] loss: 210.05\n",
      "[epoch 111] loss: 209.68\n",
      "[epoch 112] loss: 209.36\n",
      "[epoch 113] loss: 208.99\n",
      "[epoch 114] loss: 208.61\n",
      "[epoch 115] loss: 208.26\n",
      "[epoch 116] loss: 207.92\n",
      "[epoch 117] loss: 207.52\n",
      "[epoch 118] loss: 207.20\n",
      "[epoch 119] loss: 206.87\n",
      "[epoch 120] loss: 206.52\n",
      "[epoch 121] loss: 206.18\n",
      "[epoch 122] loss: 205.85\n",
      "[epoch 123] loss: 205.49\n",
      "[epoch 124] loss: 205.15\n",
      "[epoch 125] loss: 204.80\n",
      "[epoch 126] loss: 204.44\n",
      "[epoch 127] loss: 204.13\n",
      "[epoch 128] loss: 203.82\n",
      "[epoch 129] loss: 203.48\n",
      "[epoch 130] loss: 203.16\n",
      "[epoch 131] loss: 202.79\n",
      "[epoch 132] loss: 202.48\n",
      "[epoch 133] loss: 202.14\n",
      "[epoch 134] loss: 201.85\n",
      "[epoch 135] loss: 201.50\n",
      "[epoch 136] loss: 201.19\n",
      "[epoch 137] loss: 200.87\n",
      "[epoch 138] loss: 200.55\n",
      "[epoch 139] loss: 200.19\n",
      "[epoch 140] loss: 199.89\n",
      "[epoch 141] loss: 199.58\n",
      "[epoch 142] loss: 199.25\n",
      "[epoch 143] loss: 198.93\n",
      "[epoch 144] loss: 198.62\n",
      "[epoch 145] loss: 198.31\n",
      "[epoch 146] loss: 198.01\n",
      "[epoch 147] loss: 197.67\n",
      "[epoch 148] loss: 197.40\n",
      "[epoch 149] loss: 197.06\n",
      "[epoch 150] loss: 196.77\n",
      "##############################Training model 3##############################\n",
      "[epoch 1] loss: 274.50\n",
      "[epoch 2] loss: 271.12\n",
      "[epoch 3] loss: 268.67\n",
      "[epoch 4] loss: 266.59\n",
      "[epoch 5] loss: 264.74\n",
      "[epoch 6] loss: 263.07\n",
      "[epoch 7] loss: 261.57\n",
      "[epoch 8] loss: 260.17\n",
      "[epoch 9] loss: 258.84\n",
      "[epoch 10] loss: 257.63\n",
      "[epoch 11] loss: 256.47\n",
      "[epoch 12] loss: 255.34\n",
      "[epoch 13] loss: 254.30\n",
      "[epoch 14] loss: 253.26\n",
      "[epoch 15] loss: 252.30\n",
      "[epoch 16] loss: 251.38\n",
      "[epoch 17] loss: 250.43\n",
      "[epoch 18] loss: 249.55\n",
      "[epoch 19] loss: 248.77\n",
      "[epoch 20] loss: 247.92\n",
      "[epoch 21] loss: 247.11\n",
      "[epoch 22] loss: 246.31\n",
      "[epoch 23] loss: 245.54\n",
      "[epoch 24] loss: 244.81\n",
      "[epoch 25] loss: 244.02\n",
      "[epoch 26] loss: 243.31\n",
      "[epoch 27] loss: 242.64\n",
      "[epoch 28] loss: 241.90\n",
      "[epoch 29] loss: 241.26\n",
      "[epoch 30] loss: 240.57\n",
      "[epoch 31] loss: 239.94\n",
      "[epoch 32] loss: 239.28\n",
      "[epoch 33] loss: 238.63\n",
      "[epoch 34] loss: 238.03\n",
      "[epoch 35] loss: 237.39\n",
      "[epoch 36] loss: 236.78\n",
      "[epoch 37] loss: 236.20\n",
      "[epoch 38] loss: 235.61\n",
      "[epoch 39] loss: 235.02\n",
      "[epoch 40] loss: 234.45\n",
      "[epoch 41] loss: 233.85\n",
      "[epoch 42] loss: 233.31\n",
      "[epoch 43] loss: 232.76\n",
      "[epoch 44] loss: 232.20\n",
      "[epoch 45] loss: 231.71\n",
      "[epoch 46] loss: 231.10\n",
      "[epoch 47] loss: 230.62\n",
      "[epoch 48] loss: 230.06\n",
      "[epoch 49] loss: 229.58\n",
      "[epoch 50] loss: 229.04\n",
      "[epoch 51] loss: 228.50\n",
      "[epoch 52] loss: 228.01\n",
      "[epoch 53] loss: 227.47\n",
      "[epoch 54] loss: 227.03\n",
      "[epoch 55] loss: 226.49\n",
      "[epoch 56] loss: 225.99\n",
      "[epoch 57] loss: 225.50\n",
      "[epoch 58] loss: 225.05\n",
      "[epoch 59] loss: 224.57\n",
      "[epoch 60] loss: 224.07\n",
      "[epoch 61] loss: 223.63\n",
      "[epoch 62] loss: 223.13\n",
      "[epoch 63] loss: 222.65\n",
      "[epoch 64] loss: 222.22\n",
      "[epoch 65] loss: 221.75\n",
      "[epoch 66] loss: 221.29\n",
      "[epoch 67] loss: 220.85\n",
      "[epoch 68] loss: 220.39\n",
      "[epoch 69] loss: 219.97\n",
      "[epoch 70] loss: 219.50\n",
      "[epoch 71] loss: 219.06\n",
      "[epoch 72] loss: 218.60\n",
      "[epoch 73] loss: 218.16\n",
      "[epoch 74] loss: 217.77\n",
      "[epoch 75] loss: 217.33\n",
      "[epoch 76] loss: 216.90\n",
      "[epoch 77] loss: 216.46\n",
      "[epoch 78] loss: 216.01\n",
      "[epoch 79] loss: 215.63\n",
      "[epoch 80] loss: 215.19\n",
      "[epoch 81] loss: 214.80\n",
      "[epoch 82] loss: 214.41\n",
      "[epoch 83] loss: 213.97\n",
      "[epoch 84] loss: 213.56\n",
      "[epoch 85] loss: 213.17\n",
      "[epoch 86] loss: 212.77\n",
      "[epoch 87] loss: 212.38\n",
      "[epoch 88] loss: 211.97\n",
      "[epoch 89] loss: 211.57\n",
      "[epoch 90] loss: 211.21\n",
      "[epoch 91] loss: 210.79\n",
      "[epoch 92] loss: 210.39\n",
      "[epoch 93] loss: 210.04\n",
      "[epoch 94] loss: 209.61\n",
      "[epoch 95] loss: 209.26\n",
      "[epoch 96] loss: 208.88\n",
      "[epoch 97] loss: 208.49\n",
      "[epoch 98] loss: 208.09\n",
      "[epoch 99] loss: 207.74\n",
      "[epoch 100] loss: 207.35\n",
      "[epoch 101] loss: 206.96\n",
      "[epoch 102] loss: 206.64\n",
      "[epoch 103] loss: 206.23\n",
      "[epoch 104] loss: 205.91\n",
      "[epoch 105] loss: 205.50\n",
      "[epoch 106] loss: 205.19\n",
      "[epoch 107] loss: 204.79\n",
      "[epoch 108] loss: 204.42\n",
      "[epoch 109] loss: 204.12\n",
      "[epoch 110] loss: 203.69\n",
      "[epoch 111] loss: 203.37\n",
      "[epoch 112] loss: 203.02\n",
      "[epoch 113] loss: 202.70\n",
      "[epoch 114] loss: 202.32\n",
      "[epoch 115] loss: 201.98\n",
      "[epoch 116] loss: 201.66\n",
      "[epoch 117] loss: 201.31\n",
      "[epoch 118] loss: 200.94\n",
      "[epoch 119] loss: 200.63\n",
      "[epoch 120] loss: 200.29\n",
      "[epoch 121] loss: 199.97\n",
      "[epoch 122] loss: 199.63\n",
      "[epoch 123] loss: 199.26\n",
      "[epoch 124] loss: 198.96\n",
      "[epoch 125] loss: 198.60\n",
      "[epoch 126] loss: 198.30\n",
      "[epoch 127] loss: 197.94\n",
      "[epoch 128] loss: 197.65\n",
      "[epoch 129] loss: 197.30\n",
      "[epoch 130] loss: 196.98\n",
      "[epoch 131] loss: 196.65\n",
      "[epoch 132] loss: 196.32\n",
      "[epoch 133] loss: 196.01\n",
      "[epoch 134] loss: 195.69\n",
      "[epoch 135] loss: 195.36\n",
      "[epoch 136] loss: 195.04\n",
      "[epoch 137] loss: 194.75\n",
      "[epoch 138] loss: 194.45\n",
      "[epoch 139] loss: 194.13\n",
      "[epoch 140] loss: 193.85\n",
      "[epoch 141] loss: 193.53\n",
      "[epoch 142] loss: 193.20\n",
      "[epoch 143] loss: 192.90\n",
      "[epoch 144] loss: 192.65\n",
      "[epoch 145] loss: 192.29\n",
      "[epoch 146] loss: 191.97\n",
      "[epoch 147] loss: 191.69\n",
      "[epoch 148] loss: 191.37\n",
      "[epoch 149] loss: 191.11\n",
      "[epoch 150] loss: 190.80\n",
      "##############################TRAINING TERMINATED##############################\n",
      "Mean method:{'mse': 0.8032944483053445, 'mae': 0.4487494323323568}\n",
      "Custom method:{'mse': 0.7996959353199653, 'mae': 0.4359855841716465}\n"
     ]
    }
   ],
   "source": [
    "scores_mean,scores_custom = KFold3(bigbibo,data_X1,data_X2,data_X3,data_y,n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {'mean_method':scores_mean,'custom_method':scores_custom}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kfoldresults.pickle','wb') as f:\n",
    "    pickle.dump(scores,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9975895454866951\n",
      "0.9935994097201185\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores_mean['mse']))\n",
    "print(np.mean(scores_custom['mse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single training no kfold\n",
    "def single_test(bibgbibo,data_X1,data_X2,data_X3,data_y):\n",
    "    train_size = 200\n",
    "    mask = np.full(data_y.shape[0], False)\n",
    "    mask[:train_size] = True\n",
    "    np.random.shuffle(mask)\n",
    "\n",
    "    y_train = data_y[mask]\n",
    "    y_test = data_y[~mask]\n",
    "    X1_train = data_X1[mask]\n",
    "    X1_test = data_X1[~mask]\n",
    "    X2_train = data_X2[mask]\n",
    "    X2_test = data_X2[~mask]\n",
    "    X3_train = data_X3[mask]\n",
    "    X3_test = data_X3[~mask]\n",
    "\n",
    "    bigbibo.fit(X1_train,X2_train,X3_train,y_train)\n",
    "    \n",
    "    bigbibo.set_mean('mean')\n",
    "    y_hat = bigbibo.predict(X1_test,X2_test,X3_test)\n",
    "    score_mean = compute_score(y_test,y_hat)\n",
    "    \n",
    "    bigbibo.set_mean('custom')\n",
    "    y_hat = bigbibo.predict(X1_test,X2_test,X3_test)\n",
    "    score_custom = compute_score(y_test,y_hat)\n",
    "    \n",
    "    return score_mean,score_custom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################Training model 1##############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] loss: 12693.66\n",
      "[epoch 2] loss: 12616.09\n",
      "[epoch 3] loss: 12528.58\n",
      "##############################Training model 2##############################\n",
      "[epoch 1] loss: 12666.94\n",
      "[epoch 2] loss: 12610.31\n",
      "[epoch 3] loss: 12545.70\n",
      "##############################Training model 3##############################\n",
      "[epoch 1] loss: 12745.96\n",
      "[epoch 2] loss: 12696.40\n",
      "[epoch 3] loss: 12639.43\n",
      "##############################TRAINING TERMINATED##############################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'mse': 645.6513731255138, 'mae': 25.16708791091442},\n",
       " {'mse': 644.775093078498, 'mae': 25.149303449651683})"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_test(bigbibo,data_X1,data_X2,data_X3,data_y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
