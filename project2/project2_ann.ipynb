{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Project 2 - Neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.stats\n",
    "import sklearn.metrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# For neural net part\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import k_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content\n",
    "0. [Import data](#import)\n",
    "1. [Data preprocessing](#preprocess) <br>\n",
    "    1.1 [Load *n* data](#load_n_data) <br>\n",
    "    1.2 [Remove outliers](#outliers) <br>\n",
    "    1.3 [Standardize data](#std) <br>\n",
    "    1.4 [PCA](#pca) <br>\n",
    "2. [PyTorch NN](#pytorch) <br>\n",
    "    2.1 [Functions](#p_functions) <br>\n",
    "    2.2 [Architecture 1](#p_architecture1) <br>\n",
    "    2.3 [Architecture 2](#p_architecture2) <br>\n",
    "    2.4 [Architecture 3](#p_architecture3) <br>\n",
    "3. [Keras NN](#keras) <br>\n",
    "    3.1 [Architecture 1](#k_architecture1) <br>\n",
    "    3.2 [Architecture 2](#k_architecture2) <br>\n",
    "4. [Main](#main) <br>\n",
    "    4.1 [PyTorch pipeline](#main_pytorch) <br>\n",
    "    4.2 [Keras pipeline](#main_keras) <br>\n",
    "5. [Statistics](#stats) <br>\n",
    "    5.1 [IQR vs *n*](#iqr_vs_n) <br>\n",
    "6. [Figures](#figures) <br>\n",
    "    6.1 [Distributions](#fig_dist) <br>\n",
    "    6.2 [IQR](#fig_iqr) <br>\n",
    "    6.3 [Feature distributions](#fig_feature_dist) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import data\n",
    "<a id='import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './data'\n",
    "\n",
    "files = os.listdir(data_folder)  \n",
    "X_files = [filename for filename in files if (filename.endswith('.npy') and ('rsr' in filename))]\n",
    "y_files = [filename for filename in files if (filename.endswith('.npy') and ('chemical_shielding' in filename))]\n",
    "\n",
    "\n",
    "X_tot = np.load(data_folder + '/' + X_files[1], mmap_mode='r')\n",
    "y_tot = np.load(data_folder + '/' + y_files[0], mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38514, 14400)\n",
      "(38514,)\n"
     ]
    }
   ],
   "source": [
    "print(X_tot.shape)\n",
    "print(y_tot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing\n",
    "<a id='preprocess'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load *n* data\n",
    "<a id='load_n_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(n_samples, tot_data_x=X_tot, tot_data_y=y_tot, iqr=False):\n",
    "    if iqr:\n",
    "        tot_data_x, tot_data_y = remove_outliers(tot_data_x, tot_data_y)\n",
    "    \n",
    "    data_len = tot_data_x.shape[0]\n",
    "    mask_data = np.random.permutation(data_len)[:n_samples]\n",
    "\n",
    "    data_X = tot_data_x[mask_data]\n",
    "    data_y = tot_data_y[mask_data]\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_train_test(n_samples, tot_data_x=X_tot, tot_data_y=y_tot, iqr=False):\n",
    "    data_X, data_y = load_data(n_samples, tot_data_x, tot_data_y, iqr=iqr)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size = 0.2)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Remove outliers\n",
    "<a id='outliers'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*c.f.* `data_preprocessing.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(X_data, y_data):\n",
    "    ''' aims at removing all rows whose label (i.e. shielding) is considered as outlier.\n",
    "    output:\n",
    "     - X_filtered\n",
    "     - y_filtered\n",
    "    '''\n",
    "    q1, q3 = np.percentile(y_data, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - (iqr * 1.5)\n",
    "    upper_bound = q3 + (iqr * 1.5)\n",
    "    \n",
    "    assert(q1 != q3), 'Q1 and Q3 have the same value'\n",
    "    \n",
    "    idx = np.where((y_data > lower_bound) & (y_data < upper_bound))\n",
    "    X_filtered = X_data[idx]\n",
    "    y_filtered = y_data[idx]\n",
    "    \n",
    "    assert(X_filtered.shape[0] == y_filtered.shape[0]), 'Problem'\n",
    "    \n",
    "    # alternative\n",
    "    #dists[(np.where((dists >= r) & (dists <= r + dr)))]\n",
    "    \n",
    "    return X_filtered, y_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distributions(data):\n",
    "    fig, ax = plt.subplots(figsize=(6,7))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.title('Distribution of the shielding')\n",
    "    sns.distplot(data).set(xlim=(-5,65),ylim=(0,0.14))\n",
    "    plt.subplot(2,1,2)\n",
    "    sns.boxplot(x=data, linewidth=1.5).set(xlabel='Shielding', xlim=(-5,65))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_distributions(y_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered, y_filtered = remove_outliers(X_tot, y_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_distributions(y_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1176 outliers\n"
     ]
    }
   ],
   "source": [
    "print('There are {} outliers'.format(y_tot.shape[0] - y_filtered.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Standardize data\n",
    "<a id='std'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*c.f.* `data_preprocessing.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_scaler(scaler, X_train, X_test):\n",
    "    \"\"\"\n",
    "    Function to scale outside of the pipeline.\n",
    "    \"\"\"\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 PCA\n",
    "<a id='pca'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PCA(n_comp, X_train):\n",
    "    \"\"\"\n",
    "    displays the 'elbow' of the PCA, ie the screeplot\"\"\"\n",
    "    pca = PCA(n_components = n_comp)\n",
    "    pca.fit(X_train)\n",
    "    plt.figure(1)\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('Cumulative explained variance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_PCA(X_train, X_test, n):\n",
    "    \"\"\"\n",
    "    Useful method to reduce train/test sets outside of the pipeline\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n)\n",
    "    # Note on fit_transform: internally, it just calls first fit() and then transform() on the same data.\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch NN\n",
    "<a id='pytorch'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Functions\n",
    "<a id='p_functions'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, mini_batch_size, monitor_loss=False):\n",
    "    '''Train the model using Mini-batch SGD'''\n",
    "    \n",
    "    criterion = nn.MSELoss() #regression task\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-4) #1e-4 normalement\n",
    "    nb_epochs = 150\n",
    "    \n",
    "    # Monitor loss\n",
    "    losses = []\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            sum_loss += loss.item() #compute loss for each mini batch for 1 epoch\n",
    "            \n",
    "            optimizer.step()\n",
    "        \n",
    "        # Monitor loss\n",
    "        losses.append(sum_loss)\n",
    "        \n",
    "        print('[epoch {:d}] loss: {:0.2f}'.format(e+1, sum_loss))\n",
    "    \n",
    "    if monitor_loss:\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pred(model, data_input):\n",
    "    '''Given a trained model, output the prediction corresponding to data_input'''\n",
    "    y_hat = model(data_input)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(y_actual, y_pred):\n",
    "    mse = mean_squared_error(y_actual, y_pred)\n",
    "    mae = mean_absolute_error(y_actual, y_pred)\n",
    "    r2 = r2_score(y_actual, y_pred)\n",
    "    #print(\"Obtained MSE on test set %2.2f \" % mse)\n",
    "    #print(\"Obtained MAE on test set %2.2f \" % mae)\n",
    "    return mse, mae, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Architecture 1\n",
    "<a id='p_architecture1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n,100)\n",
    "        self.fc2 = nn.Linear(100,1)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Architecture 2\n",
    "<a id='p_architecture2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply add dropout layers\n",
    "Net_2 = nn.Sequential(nn.Linear(14400, 100), nn.ReLU(),\n",
    "                      #nn.Dropout(),\n",
    "                      nn.Linear(100, 50), nn.ReLU(),\n",
    "                      nn.Dropout(p=.2),\n",
    "                      nn.Linear(50, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test this one with dropout\n",
    "class Net_2(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(Net_2, self).__init__()\n",
    "        self.fc1 = nn.Linear(n,100)\n",
    "        self.fc2 = nn.Linear(100,1)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Architecture 3\n",
    "<a id='p_architecture3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_3(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(Net_3, self).__init__()\n",
    "        self.fc1 = nn.Linear(n,100)\n",
    "        self.fc2 = nn.Linear(100,1)\n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_4(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(Net_4, self).__init__()\n",
    "        self.fc1 = nn.Linear(n,100)\n",
    "        self.fc2 = nn.Linear(100,50)\n",
    "        self.fc3 = nn.Linear(50,50)\n",
    "        self.fc4 = nn.Linear(50,1)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = F.leaky_relu(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_5(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(Net_5, self).__init__()\n",
    "        self.fc1 = nn.Linear(n,100)\n",
    "        self.fc2 = nn.Linear(100,50)\n",
    "        self.fc3 = nn.Linear(50,50)\n",
    "        self.fc4 = nn.Linear(50,50)\n",
    "        self.fc5 = nn.Linear(50,1)\n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = F.leaky_relu(self.fc4(x))\n",
    "        x = F.leaky_relu(self.fc5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras NN\n",
    "<a id='keras'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Architecture 1\n",
    "<a id='k_architecture1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_model_2(input_data, metric):\n",
    "    '''return a model based on the input_data size (i.e. nb of features)\n",
    "       metric: 'mean_absolute_error' OR 'mean_square_error'\n",
    "    '''\n",
    "    # instantiate \n",
    "    model = Sequential()\n",
    "\n",
    "    # input Layer\n",
    "    model.add(Dense(50, kernel_initializer='normal',input_dim = input_data.shape[1], activation='relu')) # test with 50 or 100 here\n",
    "\n",
    "    # output Layer\n",
    "    model.add(Dense(1, kernel_initializer='normal',activation='relu')) # change last activation 'linear', 'sigmoid'\n",
    "\n",
    "    # compile the network\n",
    "    model.compile(loss=metric, optimizer='adam', metrics=[metric])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Architecture 2\n",
    "<a id='k_architecture2'></a>\n",
    "\n",
    "_c.f._ `k_models.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main\n",
    "<a id='main'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data prior feeding it to the mode\n",
    "def load_process_data(n, iqr=False, pca=False, scale=False):\n",
    "    # load all datas\n",
    "    # filter (iqr)\n",
    "    if iqr:\n",
    "        # remove outliers\n",
    "    # select n samples and split in train-test sets\n",
    "    # perform pca\n",
    "    if pca:\n",
    "        # do pca\n",
    "    # normalize \n",
    "    if scale:\n",
    "        # do normalization\n",
    "    #return X_train, X_test, y_train, y_test\n",
    "    raise NotImplementedError "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How man PCA components do we need?\n",
    "plot_PCA(100, X_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">40 components to explain 95% of the total variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 PyTorch pipeline\n",
    "<a id='main_pytorch'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_iqr = True\n",
    "apply_scaler_ = False\n",
    "apply_pca = False\n",
    "\n",
    "nb_input_neurons = 14400\n",
    "\n",
    "if apply_iqr:\n",
    "    X_train, X_test, y_train, y_test = load_data_train_test(12000, X_tot, y_tot, iqr=apply_iqr)\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = load_data_train_test(12000, X_tot, y_tot) #38514\n",
    "\n",
    "if apply_scaler_:\n",
    "    min_max = MinMaxScaler()\n",
    "    X_train, X_test = apply_scaler(min_max, X_train, X_test)\n",
    "    \n",
    "if apply_pca:\n",
    "    n = 80\n",
    "    X_train, X_test = do_PCA(X_train, X_test, n)\n",
    "    nb_input_neurons = n\n",
    "\n",
    "train_input = torch.Tensor(X_train)\n",
    "test_input = torch.Tensor(X_test)\n",
    "train_target = torch.Tensor(y_train.reshape(len(y_train), 1))\n",
    "test_target = torch.Tensor(y_test.reshape(len(y_test), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] loss: 176932.25\n",
      "[epoch 2] loss: 3867.05\n",
      "[epoch 3] loss: 2556.41\n",
      "[epoch 4] loss: 1799.85\n",
      "[epoch 5] loss: 1359.80\n",
      "[epoch 6] loss: 1122.91\n",
      "[epoch 7] loss: 981.16\n",
      "[epoch 8] loss: 888.16\n",
      "[epoch 9] loss: 821.82\n",
      "[epoch 10] loss: 771.12\n",
      "[epoch 11] loss: 730.33\n",
      "[epoch 12] loss: 696.36\n",
      "[epoch 13] loss: 667.45\n",
      "[epoch 14] loss: 642.32\n",
      "[epoch 15] loss: 620.05\n",
      "[epoch 16] loss: 600.56\n",
      "[epoch 17] loss: 583.12\n",
      "[epoch 18] loss: 567.44\n",
      "[epoch 19] loss: 553.22\n",
      "[epoch 20] loss: 540.16\n",
      "[epoch 21] loss: 528.07\n",
      "[epoch 22] loss: 516.26\n",
      "[epoch 23] loss: 505.37\n",
      "[epoch 24] loss: 495.33\n",
      "[epoch 25] loss: 485.95\n",
      "[epoch 26] loss: 477.12\n",
      "[epoch 27] loss: 468.63\n",
      "[epoch 28] loss: 460.65\n",
      "[epoch 29] loss: 452.96\n",
      "[epoch 30] loss: 445.87\n",
      "[epoch 31] loss: 439.09\n",
      "[epoch 32] loss: 432.60\n",
      "[epoch 33] loss: 426.42\n",
      "[epoch 34] loss: 420.48\n",
      "[epoch 35] loss: 414.70\n",
      "[epoch 36] loss: 409.24\n",
      "[epoch 37] loss: 404.05\n",
      "[epoch 38] loss: 399.00\n",
      "[epoch 39] loss: 394.11\n",
      "[epoch 40] loss: 389.35\n",
      "[epoch 41] loss: 384.88\n",
      "[epoch 42] loss: 380.57\n",
      "[epoch 43] loss: 376.23\n",
      "[epoch 44] loss: 372.23\n",
      "[epoch 45] loss: 368.22\n",
      "[epoch 46] loss: 364.42\n",
      "[epoch 47] loss: 360.75\n",
      "[epoch 48] loss: 357.20\n",
      "[epoch 49] loss: 353.79\n",
      "[epoch 50] loss: 350.29\n",
      "[epoch 51] loss: 347.06\n",
      "[epoch 52] loss: 343.80\n",
      "[epoch 53] loss: 340.73\n",
      "[epoch 54] loss: 337.63\n",
      "[epoch 55] loss: 334.73\n",
      "[epoch 56] loss: 331.92\n",
      "[epoch 57] loss: 329.17\n",
      "[epoch 58] loss: 326.48\n",
      "[epoch 59] loss: 323.83\n",
      "[epoch 60] loss: 321.19\n",
      "[epoch 61] loss: 318.61\n",
      "[epoch 62] loss: 316.15\n",
      "[epoch 63] loss: 313.75\n",
      "[epoch 64] loss: 311.34\n",
      "[epoch 65] loss: 309.04\n",
      "[epoch 66] loss: 306.68\n",
      "[epoch 67] loss: 304.46\n",
      "[epoch 68] loss: 302.26\n",
      "[epoch 69] loss: 300.08\n",
      "[epoch 70] loss: 297.97\n",
      "[epoch 71] loss: 295.86\n",
      "[epoch 72] loss: 293.80\n",
      "[epoch 73] loss: 291.78\n",
      "[epoch 74] loss: 289.89\n",
      "[epoch 75] loss: 287.89\n",
      "[epoch 76] loss: 285.92\n",
      "[epoch 77] loss: 284.13\n",
      "[epoch 78] loss: 282.26\n",
      "[epoch 79] loss: 280.47\n",
      "[epoch 80] loss: 278.70\n",
      "[epoch 81] loss: 277.02\n",
      "[epoch 82] loss: 275.30\n",
      "[epoch 83] loss: 273.58\n",
      "[epoch 84] loss: 271.83\n",
      "[epoch 85] loss: 270.27\n",
      "[epoch 86] loss: 268.57\n",
      "[epoch 87] loss: 266.95\n",
      "[epoch 88] loss: 265.36\n",
      "[epoch 89] loss: 263.79\n",
      "[epoch 90] loss: 262.40\n",
      "[epoch 91] loss: 260.82\n",
      "[epoch 92] loss: 259.42\n",
      "[epoch 93] loss: 257.77\n",
      "[epoch 94] loss: 256.38\n",
      "[epoch 95] loss: 254.91\n",
      "[epoch 96] loss: 253.34\n",
      "[epoch 97] loss: 252.02\n",
      "[epoch 98] loss: 250.49\n",
      "[epoch 99] loss: 249.11\n",
      "[epoch 100] loss: 247.70\n",
      "[epoch 101] loss: 246.24\n",
      "[epoch 102] loss: 244.73\n",
      "[epoch 103] loss: 243.56\n",
      "[epoch 104] loss: 242.30\n",
      "[epoch 105] loss: 240.88\n",
      "[epoch 106] loss: 239.55\n",
      "[epoch 107] loss: 238.24\n",
      "[epoch 108] loss: 237.12\n",
      "[epoch 109] loss: 235.73\n",
      "[epoch 110] loss: 234.50\n",
      "[epoch 111] loss: 233.28\n",
      "[epoch 112] loss: 232.06\n",
      "[epoch 113] loss: 231.03\n",
      "[epoch 114] loss: 229.74\n",
      "[epoch 115] loss: 228.57\n",
      "[epoch 116] loss: 227.31\n",
      "[epoch 117] loss: 226.28\n",
      "[epoch 118] loss: 225.08\n",
      "[epoch 119] loss: 224.03\n",
      "[epoch 120] loss: 222.90\n",
      "[epoch 121] loss: 221.77\n",
      "[epoch 122] loss: 220.48\n",
      "[epoch 123] loss: 219.31\n",
      "[epoch 124] loss: 218.07\n",
      "[epoch 125] loss: 217.07\n",
      "[epoch 126] loss: 215.91\n",
      "[epoch 127] loss: 214.86\n",
      "[epoch 128] loss: 213.87\n",
      "[epoch 129] loss: 212.82\n",
      "[epoch 130] loss: 211.72\n",
      "[epoch 131] loss: 210.89\n",
      "[epoch 132] loss: 209.87\n",
      "[epoch 133] loss: 208.66\n",
      "[epoch 134] loss: 207.70\n",
      "[epoch 135] loss: 206.90\n",
      "[epoch 136] loss: 205.82\n",
      "[epoch 137] loss: 204.88\n",
      "[epoch 138] loss: 203.85\n",
      "[epoch 139] loss: 202.88\n",
      "[epoch 140] loss: 201.93\n",
      "[epoch 141] loss: 201.04\n",
      "[epoch 142] loss: 200.05\n",
      "[epoch 143] loss: 199.27\n",
      "[epoch 144] loss: 198.23\n",
      "[epoch 145] loss: 197.39\n",
      "[epoch 146] loss: 196.48\n",
      "[epoch 147] loss: 195.51\n",
      "[epoch 148] loss: 194.79\n",
      "[epoch 149] loss: 194.05\n",
      "[epoch 150] loss: 193.07\n"
     ]
    }
   ],
   "source": [
    "mini_batch_size = 10\n",
    "model = Net_5(nb_input_neurons) \n",
    "losses = train_model(model, train_input, train_target, mini_batch_size, monitor_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.54 \n",
      "MAE: 0.54 \n",
      "r2: 0.94\n"
     ]
    }
   ],
   "source": [
    "#model.train(False)\n",
    "\n",
    "#Make predictions\n",
    "y_hat = compute_pred(model, test_input)\n",
    "\n",
    "#Compute score\n",
    "mse_nn, mae_nn, r2_nn = compute_score(y_test, y_hat.detach().numpy())\n",
    "\n",
    "print('MSE: {:0.2f} \\nMAE: {:0.2f} \\nr2: {:0.2f}'.format(mse_nn, mae_nn, r2_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('y_hat_all_dataset2_net3.pkl', 'wb') as f:\n",
    "    pickle.dump(y_hat, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37338, 14400)\n",
      "(37338,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "mse_storage = []\n",
    "mae_storage = []\n",
    "r2_storage = []\n",
    "\n",
    "mini_batch_size = 10\n",
    "nb_input_neurons = 14400\n",
    "\n",
    "# Load data, remove outliers but do not split yet into train and test\n",
    "X, y = load_data(38514, tot_data_x=X_tot, tot_data_y=y_tot, iqr=True)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37000, 14400)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:-338].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:-338]\n",
    "y = y[:-338]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "[epoch 1] loss: 195956.91\n",
      "[epoch 2] loss: 5717.32\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "\n",
    "for idx, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(\"FOLD {}\".format(idx+1))\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    train_input = torch.Tensor(X_train)\n",
    "    test_input = torch.Tensor(X_test)\n",
    "    train_target = torch.Tensor(y_train.reshape(len(y_train), 1))\n",
    "    test_target = torch.Tensor(y_test.reshape(len(y_test), 1))\n",
    "    \n",
    "    model = Net_4(nb_input_neurons) \n",
    "    losses = train_model(model, train_input, train_target, mini_batch_size, monitor_loss=True)\n",
    "\n",
    "    #Make predictions\n",
    "    y_hat = compute_pred(model, test_input)\n",
    "    #Compute score\n",
    "    mse_nn, mae_nn, r2_nn = compute_score(y_test, y_hat.detach().numpy())\n",
    "    \n",
    "    mse_storage.append(mse_nn)\n",
    "    mae_storage.append(mae_nn)\n",
    "    r2_storage.append(r2_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('metrics_all_dataset2_net4_4fold.pkl', 'wb') as f:\n",
    "    pickle.dump([mse_storage, mae_storage, r2_storage], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.42 +/- 0.0069 \n",
      "MAE: 0.48 +/- 0.0047 \n",
      "R2: 0.95 +/- 0.0008\n"
     ]
    }
   ],
   "source": [
    "print('MSE: {:0.2f} +/- {:0.4f} \\nMAE: {:0.2f} +/- {:0.4f} \\nR2: {:0.2f} +/- {:0.4f}'.format(np.mean(mse_storage), np.std(mse_storage), np.mean(mae_storage), np.std(mae_storage), np.mean(r2_storage), np.std(r2_storage)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Keras pipeline\n",
    "<a id='main_keras'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1280 samples, validate on 320 samples\n",
      "Epoch 1/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 20.5533 - mean_absolute_error: 20.5534 - val_loss: 10.6779 - val_mean_absolute_error: 10.6779\n",
      "Epoch 2/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 3.4412 - mean_absolute_error: 3.4412 - val_loss: 2.1116 - val_mean_absolute_error: 2.1116\n",
      "Epoch 3/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.8856 - mean_absolute_error: 1.8856 - val_loss: 1.7832 - val_mean_absolute_error: 1.7832\n",
      "Epoch 4/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.6338 - mean_absolute_error: 1.6338 - val_loss: 1.6364 - val_mean_absolute_error: 1.6364\n",
      "Epoch 5/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.4684 - mean_absolute_error: 1.4684 - val_loss: 1.4507 - val_mean_absolute_error: 1.4507\n",
      "Epoch 6/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.3487 - mean_absolute_error: 1.3487 - val_loss: 1.3620 - val_mean_absolute_error: 1.3620\n",
      "Epoch 7/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.2593 - mean_absolute_error: 1.2593 - val_loss: 1.3047 - val_mean_absolute_error: 1.3047\n",
      "Epoch 8/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.1838 - mean_absolute_error: 1.1838 - val_loss: 1.2502 - val_mean_absolute_error: 1.2502\n",
      "Epoch 9/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.1258 - mean_absolute_error: 1.1258 - val_loss: 1.2066 - val_mean_absolute_error: 1.2066\n",
      "Epoch 10/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.0773 - mean_absolute_error: 1.0773 - val_loss: 1.1537 - val_mean_absolute_error: 1.1537\n",
      "Epoch 11/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.0469 - mean_absolute_error: 1.0469 - val_loss: 1.1248 - val_mean_absolute_error: 1.1248\n",
      "Epoch 12/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.0024 - mean_absolute_error: 1.0024 - val_loss: 1.0928 - val_mean_absolute_error: 1.0928\n",
      "Epoch 13/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.9663 - mean_absolute_error: 0.9663 - val_loss: 1.0636 - val_mean_absolute_error: 1.0636\n",
      "Epoch 14/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.9486 - mean_absolute_error: 0.9486 - val_loss: 1.0322 - val_mean_absolute_error: 1.0322\n",
      "Epoch 15/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.9272 - mean_absolute_error: 0.9272 - val_loss: 1.0828 - val_mean_absolute_error: 1.0828\n",
      "Epoch 16/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.9107 - mean_absolute_error: 0.9107 - val_loss: 1.0040 - val_mean_absolute_error: 1.0040\n",
      "Epoch 17/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8827 - mean_absolute_error: 0.8827 - val_loss: 0.9901 - val_mean_absolute_error: 0.9901\n",
      "Epoch 18/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8781 - mean_absolute_error: 0.8781 - val_loss: 0.9749 - val_mean_absolute_error: 0.9749\n",
      "Epoch 19/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8446 - mean_absolute_error: 0.8446 - val_loss: 0.9781 - val_mean_absolute_error: 0.9781\n",
      "Epoch 20/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8415 - mean_absolute_error: 0.8415 - val_loss: 0.9896 - val_mean_absolute_error: 0.9896\n",
      "Epoch 21/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8250 - mean_absolute_error: 0.8250 - val_loss: 0.9458 - val_mean_absolute_error: 0.9458\n",
      "Epoch 22/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8086 - mean_absolute_error: 0.8086 - val_loss: 0.9559 - val_mean_absolute_error: 0.9559\n",
      "Epoch 23/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8053 - mean_absolute_error: 0.8053 - val_loss: 0.9622 - val_mean_absolute_error: 0.9622\n",
      "Epoch 24/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7821 - mean_absolute_error: 0.7821 - val_loss: 0.9340 - val_mean_absolute_error: 0.9340\n",
      "Epoch 25/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7826 - mean_absolute_error: 0.7826 - val_loss: 0.9283 - val_mean_absolute_error: 0.9283\n",
      "Epoch 26/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7803 - mean_absolute_error: 0.7803 - val_loss: 0.9226 - val_mean_absolute_error: 0.9226\n",
      "Epoch 27/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7768 - mean_absolute_error: 0.7768 - val_loss: 0.9319 - val_mean_absolute_error: 0.9319\n",
      "Epoch 28/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7615 - mean_absolute_error: 0.7615 - val_loss: 0.9995 - val_mean_absolute_error: 0.9995\n",
      "Epoch 29/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7684 - mean_absolute_error: 0.7684 - val_loss: 0.9637 - val_mean_absolute_error: 0.9637\n",
      "Epoch 30/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7622 - mean_absolute_error: 0.7622 - val_loss: 0.9131 - val_mean_absolute_error: 0.9131\n",
      "Epoch 31/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7485 - mean_absolute_error: 0.7485 - val_loss: 0.9188 - val_mean_absolute_error: 0.9188\n",
      "Epoch 32/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7424 - mean_absolute_error: 0.7424 - val_loss: 0.9017 - val_mean_absolute_error: 0.9017\n",
      "Epoch 33/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7434 - mean_absolute_error: 0.7434 - val_loss: 0.9331 - val_mean_absolute_error: 0.9331\n",
      "Epoch 34/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7264 - mean_absolute_error: 0.7264 - val_loss: 0.9186 - val_mean_absolute_error: 0.9186\n",
      "Epoch 35/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7198 - mean_absolute_error: 0.7198 - val_loss: 0.9513 - val_mean_absolute_error: 0.9513\n",
      "Epoch 36/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7331 - mean_absolute_error: 0.7331 - val_loss: 0.9123 - val_mean_absolute_error: 0.9123\n",
      "Epoch 37/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7221 - mean_absolute_error: 0.7221 - val_loss: 0.9004 - val_mean_absolute_error: 0.9004\n",
      "Epoch 38/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7271 - mean_absolute_error: 0.7271 - val_loss: 0.8990 - val_mean_absolute_error: 0.8990\n",
      "Epoch 39/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7113 - mean_absolute_error: 0.7113 - val_loss: 0.9280 - val_mean_absolute_error: 0.9280\n",
      "Epoch 40/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7045 - mean_absolute_error: 0.7045 - val_loss: 0.9209 - val_mean_absolute_error: 0.9209\n",
      "Epoch 41/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6925 - mean_absolute_error: 0.6925 - val_loss: 0.8963 - val_mean_absolute_error: 0.8963\n",
      "Epoch 42/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6879 - mean_absolute_error: 0.6879 - val_loss: 0.9023 - val_mean_absolute_error: 0.9023\n",
      "Epoch 43/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6798 - mean_absolute_error: 0.6798 - val_loss: 0.9094 - val_mean_absolute_error: 0.9094\n",
      "Epoch 44/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6825 - mean_absolute_error: 0.6825 - val_loss: 0.9322 - val_mean_absolute_error: 0.9322 - loss: 0.6855 - mean_absolut\n",
      "Epoch 45/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6736 - mean_absolute_error: 0.6736 - val_loss: 0.9072 - val_mean_absolute_error: 0.9072\n",
      "Epoch 46/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6767 - mean_absolute_error: 0.6767 - val_loss: 0.9403 - val_mean_absolute_error: 0.9403\n",
      "Epoch 47/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6700 - mean_absolute_error: 0.6700 - val_loss: 0.9227 - val_mean_absolute_error: 0.9227\n",
      "Epoch 48/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6590 - mean_absolute_error: 0.6590 - val_loss: 0.9006 - val_mean_absolute_error: 0.9006\n",
      "Epoch 49/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6623 - mean_absolute_error: 0.6623 - val_loss: 0.9027 - val_mean_absolute_error: 0.9027\n",
      "Epoch 50/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6640 - mean_absolute_error: 0.6640 - val_loss: 0.8982 - val_mean_absolute_error: 0.8982\n",
      "Epoch 51/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6573 - mean_absolute_error: 0.6573 - val_loss: 0.8995 - val_mean_absolute_error: 0.8995\n",
      "Epoch 52/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6508 - mean_absolute_error: 0.6508 - val_loss: 0.8898 - val_mean_absolute_error: 0.8898\n",
      "Epoch 53/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6592 - mean_absolute_error: 0.6592 - val_loss: 0.9011 - val_mean_absolute_error: 0.9011\n",
      "Epoch 54/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6454 - mean_absolute_error: 0.6454 - val_loss: 0.9154 - val_mean_absolute_error: 0.9154\n",
      "Epoch 55/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6478 - mean_absolute_error: 0.6478 - val_loss: 0.8987 - val_mean_absolute_error: 0.8987\n",
      "Epoch 56/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6349 - mean_absolute_error: 0.6349 - val_loss: 0.8987 - val_mean_absolute_error: 0.8987\n",
      "Epoch 57/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6553 - mean_absolute_error: 0.6553 - val_loss: 0.9073 - val_mean_absolute_error: 0.9073\n",
      "Epoch 58/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6306 - mean_absolute_error: 0.6306 - val_loss: 0.9502 - val_mean_absolute_error: 0.9502\n",
      "Epoch 59/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6256 - mean_absolute_error: 0.6256 - val_loss: 0.8995 - val_mean_absolute_error: 0.8995\n",
      "Epoch 60/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6298 - mean_absolute_error: 0.6298 - val_loss: 0.8885 - val_mean_absolute_error: 0.8885\n",
      "Epoch 61/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6302 - mean_absolute_error: 0.6302 - val_loss: 0.8880 - val_mean_absolute_error: 0.8880\n",
      "Epoch 62/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6201 - mean_absolute_error: 0.6201 - val_loss: 0.8977 - val_mean_absolute_error: 0.8977\n",
      "Epoch 63/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6212 - mean_absolute_error: 0.6212 - val_loss: 0.9090 - val_mean_absolute_error: 0.9090\n",
      "Epoch 64/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6221 - mean_absolute_error: 0.6221 - val_loss: 0.9033 - val_mean_absolute_error: 0.9033\n",
      "Epoch 65/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6105 - mean_absolute_error: 0.6105 - val_loss: 0.9020 - val_mean_absolute_error: 0.9020\n",
      "Epoch 66/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6172 - mean_absolute_error: 0.6172 - val_loss: 0.8895 - val_mean_absolute_error: 0.8895\n",
      "Epoch 67/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6110 - mean_absolute_error: 0.6110 - val_loss: 0.8952 - val_mean_absolute_error: 0.8952\n",
      "Epoch 68/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6133 - mean_absolute_error: 0.6133 - val_loss: 0.8986 - val_mean_absolute_error: 0.8986\n",
      "Epoch 69/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6068 - mean_absolute_error: 0.6068 - val_loss: 0.8976 - val_mean_absolute_error: 0.8976\n",
      "Epoch 70/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6164 - mean_absolute_error: 0.6164 - val_loss: 0.9015 - val_mean_absolute_error: 0.9015\n",
      "Epoch 71/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6087 - mean_absolute_error: 0.6087 - val_loss: 0.9093 - val_mean_absolute_error: 0.9093\n",
      "Epoch 72/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6060 - mean_absolute_error: 0.6060 - val_loss: 0.9320 - val_mean_absolute_error: 0.9320\n",
      "Epoch 73/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6069 - mean_absolute_error: 0.6069 - val_loss: 0.8818 - val_mean_absolute_error: 0.8818\n",
      "Epoch 74/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6012 - mean_absolute_error: 0.6012 - val_loss: 0.8895 - val_mean_absolute_error: 0.8895\n",
      "Epoch 75/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5948 - mean_absolute_error: 0.5948 - val_loss: 0.9510 - val_mean_absolute_error: 0.9510\n",
      "Epoch 76/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6030 - mean_absolute_error: 0.6030 - val_loss: 0.8884 - val_mean_absolute_error: 0.8884\n",
      "Epoch 77/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5905 - mean_absolute_error: 0.5905 - val_loss: 0.9292 - val_mean_absolute_error: 0.9292\n",
      "Epoch 78/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5869 - mean_absolute_error: 0.5869 - val_loss: 0.8999 - val_mean_absolute_error: 0.8999\n",
      "Epoch 79/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5858 - mean_absolute_error: 0.5858 - val_loss: 0.9045 - val_mean_absolute_error: 0.9045\n",
      "Epoch 80/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5877 - mean_absolute_error: 0.5877 - val_loss: 0.9128 - val_mean_absolute_error: 0.9128\n",
      "Epoch 81/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5888 - mean_absolute_error: 0.5888 - val_loss: 0.9007 - val_mean_absolute_error: 0.9007\n",
      "Epoch 82/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5773 - mean_absolute_error: 0.5773 - val_loss: 0.8913 - val_mean_absolute_error: 0.8913\n",
      "Epoch 83/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5893 - mean_absolute_error: 0.5893 - val_loss: 0.9104 - val_mean_absolute_error: 0.9104\n",
      "Epoch 84/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5784 - mean_absolute_error: 0.5784 - val_loss: 0.9112 - val_mean_absolute_error: 0.9112\n",
      "Epoch 85/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5794 - mean_absolute_error: 0.5794 - val_loss: 0.9158 - val_mean_absolute_error: 0.9158\n",
      "Epoch 86/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5762 - mean_absolute_error: 0.5762 - val_loss: 0.9056 - val_mean_absolute_error: 0.9056\n",
      "Epoch 87/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5746 - mean_absolute_error: 0.5746 - val_loss: 0.9235 - val_mean_absolute_error: 0.9235\n",
      "Epoch 88/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5826 - mean_absolute_error: 0.5826 - val_loss: 0.9050 - val_mean_absolute_error: 0.9050\n",
      "Epoch 89/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5721 - mean_absolute_error: 0.5721 - val_loss: 0.9014 - val_mean_absolute_error: 0.9014\n",
      "Epoch 90/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5667 - mean_absolute_error: 0.5667 - val_loss: 0.9011 - val_mean_absolute_error: 0.9011\n",
      "Epoch 91/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5798 - mean_absolute_error: 0.5798 - val_loss: 0.9099 - val_mean_absolute_error: 0.9099\n",
      "Epoch 92/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5689 - mean_absolute_error: 0.5689 - val_loss: 0.9198 - val_mean_absolute_error: 0.9198\n",
      "Epoch 93/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5734 - mean_absolute_error: 0.5734 - val_loss: 0.8973 - val_mean_absolute_error: 0.8973\n",
      "Epoch 94/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5832 - mean_absolute_error: 0.5832 - val_loss: 0.9035 - val_mean_absolute_error: 0.9035\n",
      "Epoch 95/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5598 - mean_absolute_error: 0.5598 - val_loss: 0.8960 - val_mean_absolute_error: 0.8960\n",
      "Epoch 96/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5584 - mean_absolute_error: 0.5584 - val_loss: 0.8992 - val_mean_absolute_error: 0.8992\n",
      "Epoch 97/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5649 - mean_absolute_error: 0.5649 - val_loss: 0.9141 - val_mean_absolute_error: 0.9141\n",
      "Epoch 98/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5523 - mean_absolute_error: 0.5523 - val_loss: 0.9010 - val_mean_absolute_error: 0.9010\n",
      "Epoch 99/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5598 - mean_absolute_error: 0.5598 - val_loss: 0.9065 - val_mean_absolute_error: 0.9065\n",
      "Epoch 100/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5583 - mean_absolute_error: 0.5583 - val_loss: 0.9037 - val_mean_absolute_error: 0.9037\n",
      "Train on 1280 samples, validate on 320 samples\n",
      "Epoch 1/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 18.8452 - val_loss: 4.8843\n",
      "Epoch 2/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.3295 - val_loss: 1.9703\n",
      "Epoch 3/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7548 - val_loss: 1.6349\n",
      "Epoch 4/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5050 - val_loss: 1.5179\n",
      "Epoch 5/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.3417 - val_loss: 1.3470\n",
      "Epoch 6/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.2435 - val_loss: 1.2814\n",
      "Epoch 7/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.1624 - val_loss: 1.2135\n",
      "Epoch 8/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.0929 - val_loss: 1.1724\n",
      "Epoch 9/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.0303 - val_loss: 1.1081\n",
      "Epoch 10/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.9942 - val_loss: 1.0730\n",
      "Epoch 11/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.9517 - val_loss: 1.0579\n",
      "Epoch 12/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.9239 - val_loss: 1.0167\n",
      "Epoch 13/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.9062 - val_loss: 0.9932\n",
      "Epoch 14/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.8659 - val_loss: 0.9700\n",
      "Epoch 15/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.8520 - val_loss: 0.9597\n",
      "Epoch 16/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.8371 - val_loss: 0.9447\n",
      "Epoch 17/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.8404 - val_loss: 0.9816\n",
      "Epoch 18/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.8127 - val_loss: 0.9249\n",
      "Epoch 19/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7918 - val_loss: 0.9186\n",
      "Epoch 20/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7891 - val_loss: 0.9090\n",
      "Epoch 21/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7738 - val_loss: 0.9247\n",
      "Epoch 22/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7743 - val_loss: 0.9096\n",
      "Epoch 23/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7558 - val_loss: 0.9220\n",
      "Epoch 24/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7526 - val_loss: 0.8966\n",
      "Epoch 25/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7420 - val_loss: 0.9184\n",
      "Epoch 26/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7467 - val_loss: 0.9166\n",
      "Epoch 27/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7442 - val_loss: 0.8978\n",
      "Epoch 28/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7223 - val_loss: 0.9736\n",
      "Epoch 29/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7249 - val_loss: 0.9099\n",
      "Epoch 30/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7342 - val_loss: 0.9018\n",
      "Epoch 31/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7155 - val_loss: 0.9038\n",
      "Epoch 32/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7000 - val_loss: 0.9975\n",
      "Epoch 33/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7012 - val_loss: 0.9412\n",
      "Epoch 34/100\n",
      "1280/1280 [==============================] - ETA: 0s - loss: 0.694 - 4s 3ms/step - loss: 0.6972 - val_loss: 0.9703\n",
      "Epoch 35/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6858 - val_loss: 0.8939\n",
      "Epoch 36/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6753 - val_loss: 0.9570\n",
      "Epoch 37/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6838 - val_loss: 0.8904\n",
      "Epoch 38/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6713 - val_loss: 0.8973\n",
      "Epoch 39/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6640 - val_loss: 0.9024\n",
      "Epoch 40/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6539 - val_loss: 0.8935\n",
      "Epoch 41/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6496 - val_loss: 0.9160\n",
      "Epoch 42/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6515 - val_loss: 0.8943\n",
      "Epoch 43/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6604 - val_loss: 0.8869\n",
      "Epoch 44/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6407 - val_loss: 0.8865\n",
      "Epoch 45/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6523 - val_loss: 0.9133\n",
      "Epoch 46/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6403 - val_loss: 0.9136\n",
      "Epoch 47/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6299 - val_loss: 0.8866\n",
      "Epoch 48/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6220 - val_loss: 0.9075\n",
      "Epoch 49/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6228 - val_loss: 0.8990\n",
      "Epoch 50/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6243 - val_loss: 0.8925\n",
      "Epoch 51/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6345 - val_loss: 0.8851\n",
      "Epoch 52/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6146 - val_loss: 0.8885\n",
      "Epoch 53/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6098 - val_loss: 0.9135\n",
      "Epoch 54/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6050 - val_loss: 0.9256\n",
      "Epoch 55/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6026 - val_loss: 0.8935\n",
      "Epoch 56/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6097 - val_loss: 0.8855\n",
      "Epoch 57/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6011 - val_loss: 0.9168\n",
      "Epoch 58/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5927 - val_loss: 0.8822\n",
      "Epoch 59/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5980 - val_loss: 0.8858\n",
      "Epoch 60/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5879 - val_loss: 0.8794\n",
      "Epoch 61/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6019 - val_loss: 0.8807\n",
      "Epoch 62/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5886 - val_loss: 0.8682\n",
      "Epoch 63/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5831 - val_loss: 0.8810\n",
      "Epoch 64/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5827 - val_loss: 0.9095\n",
      "Epoch 65/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5827 - val_loss: 0.8965\n",
      "Epoch 66/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5763 - val_loss: 0.8906\n",
      "Epoch 67/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5685 - val_loss: 0.9132\n",
      "Epoch 68/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5721 - val_loss: 0.8764\n",
      "Epoch 69/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5777 - val_loss: 0.8837\n",
      "Epoch 70/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5801 - val_loss: 0.9155\n",
      "Epoch 71/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5817 - val_loss: 0.8808\n",
      "Epoch 72/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5718 - val_loss: 0.9199\n",
      "Epoch 73/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5659 - val_loss: 0.9320\n",
      "Epoch 74/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5639 - val_loss: 0.8825\n",
      "Epoch 75/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5630 - val_loss: 0.8947\n",
      "Epoch 76/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5505 - val_loss: 0.8850\n",
      "Epoch 77/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5594 - val_loss: 0.9336\n",
      "Epoch 78/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5656 - val_loss: 0.9256\n",
      "Epoch 79/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5480 - val_loss: 0.8779\n",
      "Epoch 80/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5472 - val_loss: 0.8944\n",
      "Epoch 81/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5412 - val_loss: 0.8974\n",
      "Epoch 82/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5406 - val_loss: 0.9095\n",
      "Epoch 83/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5404 - val_loss: 0.9141\n",
      "Epoch 84/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5386 - val_loss: 0.8927\n",
      "Epoch 85/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5339 - val_loss: 0.8939\n",
      "Epoch 86/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5542 - val_loss: 0.8851\n",
      "Epoch 87/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5330 - val_loss: 0.9109\n",
      "Epoch 88/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5290 - val_loss: 0.8923\n",
      "Epoch 89/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5273 - val_loss: 0.9155\n",
      "Epoch 90/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5279 - val_loss: 0.9020\n",
      "Epoch 91/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5239 - val_loss: 0.8970\n",
      "Epoch 92/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5209 - val_loss: 0.8979\n",
      "Epoch 93/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5192 - val_loss: 0.9030\n",
      "Epoch 94/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5181 - val_loss: 0.9405\n",
      "Epoch 95/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5284 - val_loss: 0.9256\n",
      "Epoch 96/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5191 - val_loss: 0.9009\n",
      "Epoch 97/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5153 - val_loss: 0.9028\n",
      "Epoch 98/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5029 - val_loss: 0.8929\n",
      "Epoch 99/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5145 - val_loss: 0.9048\n",
      "Epoch 100/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5070 - val_loss: 0.9202\n",
      "Train on 1280 samples, validate on 320 samples\n",
      "Epoch 1/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 21.9362 - val_loss: 21.0831\n",
      "Epoch 2/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 10.6694 - val_loss: 4.9718\n",
      "Epoch 3/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 4.1598 - val_loss: 3.9340\n",
      "Epoch 4/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 3.2858 - val_loss: 3.5508\n",
      "Epoch 5/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.9754 - val_loss: 2.8293\n",
      "Epoch 6/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.9384 - val_loss: 2.2671\n",
      "Epoch 7/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.6124 - val_loss: 2.2294\n",
      "Epoch 8/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.5956 - val_loss: 1.9236\n",
      "Epoch 9/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.6133 - val_loss: 1.2556\n",
      "Epoch 10/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.4611 - val_loss: 1.4869\n",
      "Epoch 11/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.5565 - val_loss: 1.2181\n",
      "Epoch 12/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.3193 - val_loss: 2.0742\n",
      "Epoch 13/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.4152 - val_loss: 1.5683\n",
      "Epoch 14/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.3199 - val_loss: 1.9536\n",
      "Epoch 15/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.2937 - val_loss: 1.5653\n",
      "Epoch 16/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.3405 - val_loss: 1.2393\n",
      "Epoch 17/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.2545 - val_loss: 1.1195\n",
      "Epoch 18/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.1344 - val_loss: 1.1949\n",
      "Epoch 19/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.2316 - val_loss: 1.1902\n",
      "Epoch 20/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.0960 - val_loss: 1.5547\n",
      "Epoch 21/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.1313 - val_loss: 1.4119\n",
      "Epoch 22/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.0559 - val_loss: 1.0102\n",
      "Epoch 23/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.1752 - val_loss: 1.2884\n",
      "Epoch 24/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.1634 - val_loss: 1.2553\n",
      "Epoch 25/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9911 - val_loss: 1.1318\n",
      "Epoch 26/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.0460 - val_loss: 1.2042\n",
      "Epoch 27/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.0200 - val_loss: 1.0775\n",
      "Epoch 28/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.0328 - val_loss: 1.0665\n",
      "Epoch 29/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9516 - val_loss: 1.0490\n",
      "Epoch 30/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.0006 - val_loss: 0.9345\n",
      "Epoch 31/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9600 - val_loss: 1.1783\n",
      "Epoch 32/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9171 - val_loss: 1.1449\n",
      "Epoch 33/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9090 - val_loss: 0.9420\n",
      "Epoch 34/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9830 - val_loss: 0.9518\n",
      "Epoch 35/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8611 - val_loss: 0.9754\n",
      "Epoch 36/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9569 - val_loss: 1.0691\n",
      "Epoch 37/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8941 - val_loss: 0.9009\n",
      "Epoch 38/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9785 - val_loss: 1.0169\n",
      "Epoch 39/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8517 - val_loss: 1.3498\n",
      "Epoch 40/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8961 - val_loss: 1.1343\n",
      "Epoch 41/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8304 - val_loss: 0.9606\n",
      "Epoch 42/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8578 - val_loss: 1.0208\n",
      "Epoch 43/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8504 - val_loss: 0.9355\n",
      "Epoch 44/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8637 - val_loss: 1.4083\n",
      "Epoch 45/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8517 - val_loss: 1.3353\n",
      "Epoch 46/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8388 - val_loss: 0.9508\n",
      "Epoch 47/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8552 - val_loss: 1.1281\n",
      "Epoch 48/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8456 - val_loss: 0.9294\n",
      "Epoch 49/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8169 - val_loss: 1.0934\n",
      "Epoch 50/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8308 - val_loss: 1.1383\n",
      "Epoch 51/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7614 - val_loss: 0.9391\n",
      "Epoch 52/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7698 - val_loss: 0.8672\n",
      "Epoch 53/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7587 - val_loss: 1.6818\n",
      "Epoch 54/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7704 - val_loss: 1.4286\n",
      "Epoch 55/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7162 - val_loss: 0.9472\n",
      "Epoch 56/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7757 - val_loss: 0.9156\n",
      "Epoch 57/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7820 - val_loss: 1.0793\n",
      "Epoch 58/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7642 - val_loss: 0.9951\n",
      "Epoch 59/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8157 - val_loss: 0.9489\n",
      "Epoch 60/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7290 - val_loss: 1.1018\n",
      "Epoch 61/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6372 - val_loss: 1.0664\n",
      "Epoch 62/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7434 - val_loss: 0.9636\n",
      "Epoch 63/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6715 - val_loss: 0.9984\n",
      "Epoch 64/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7047 - val_loss: 1.1442\n",
      "Epoch 65/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7028 - val_loss: 0.9360\n",
      "Epoch 66/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6986 - val_loss: 0.8796\n",
      "Epoch 67/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7504 - val_loss: 0.9445\n",
      "Epoch 68/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6265 - val_loss: 1.0664\n",
      "Epoch 69/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6449 - val_loss: 0.8823\n",
      "Epoch 70/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7221 - val_loss: 1.0630\n",
      "Epoch 71/100\n",
      "1280/1280 [==============================] - 3s 3ms/step - loss: 1.7181 - val_loss: 0.9589\n",
      "Epoch 72/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6745 - val_loss: 1.2331\n",
      "Epoch 73/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6898 - val_loss: 1.0802\n",
      "Epoch 74/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5732 - val_loss: 0.9860\n",
      "Epoch 75/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5998 - val_loss: 0.9259\n",
      "Epoch 76/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7093 - val_loss: 0.9657\n",
      "Epoch 77/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5907 - val_loss: 1.0717\n",
      "Epoch 78/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5309 - val_loss: 0.9039\n",
      "Epoch 79/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6774 - val_loss: 0.9090\n",
      "Epoch 80/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6279 - val_loss: 0.9330\n",
      "Epoch 81/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6408 - val_loss: 0.8968\n",
      "Epoch 82/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5419 - val_loss: 1.0997\n",
      "Epoch 83/100\n",
      "1280/1280 [==============================] - 3s 3ms/step - loss: 1.6346 - val_loss: 0.8592\n",
      "Epoch 84/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5298 - val_loss: 1.0378\n",
      "Epoch 85/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5996 - val_loss: 0.8719\n",
      "Epoch 86/100\n",
      "1280/1280 [==============================] - 3s 3ms/step - loss: 1.5407 - val_loss: 0.9061\n",
      "Epoch 87/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6536 - val_loss: 0.9594\n",
      "Epoch 88/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5870 - val_loss: 0.9579\n",
      "Epoch 89/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6459 - val_loss: 0.8652\n",
      "Epoch 90/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5481 - val_loss: 0.9130\n",
      "Epoch 91/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5439 - val_loss: 0.9201\n",
      "Epoch 92/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6152 - val_loss: 0.9055\n",
      "Epoch 93/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5744 - val_loss: 0.8613\n",
      "Epoch 94/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5480 - val_loss: 0.9054\n",
      "Epoch 95/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5386 - val_loss: 0.8726\n",
      "Epoch 96/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5859 - val_loss: 1.0245\n",
      "Epoch 97/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6000 - val_loss: 1.2286\n",
      "Epoch 98/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6297 - val_loss: 0.9892\n",
      "Epoch 99/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5165 - val_loss: 0.9072\n",
      "Epoch 100/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5887 - val_loss: 0.9081\n"
     ]
    }
   ],
   "source": [
    "models = [k_models.model_5, k_models.model_6, k_models.model_7]\n",
    "\n",
    "model_names = [] #[m.__name__ for m in models]\n",
    "epochs_list = []\n",
    "mse_list = []\n",
    "mae_list = []\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_data_train_test(2000, X_tot, y_tot)\n",
    "\n",
    "nb_epochs = [100]\n",
    "for m in models:\n",
    "    for e in nb_epochs:\n",
    "        network = m(X_train, 'mean_absolute_error')\n",
    "        network.fit(X_train, y_train, epochs=e, batch_size=10, validation_split=0.2)\n",
    "\n",
    "        y_hat = network.predict(X_test)\n",
    "        mse_nn, mae_nn = compute_score(y_test, y_hat)\n",
    "\n",
    "        model_names.append(m.__name__)\n",
    "        epochs_list.append(e)\n",
    "        mse_list.append(mse_nn)\n",
    "        mae_list.append(mae_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            model_5     model_6     model_7 \n",
      "\n",
      " Epochs  100         100         100        \n",
      "\n",
      " MSE       1.01402     0.944624    1.2443   \n",
      "\n",
      " MAE       0.768377    0.754768    0.866663 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([model_names, ['Epochs']+epochs_list, ['MSE']+mse_list, ['MAE']+mae_list], headers=\"firstrow\", tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick tests\n",
    "apply_pca = False\n",
    "apply_iqr = True\n",
    "\n",
    "if apply_iqr:\n",
    "    X_train, X_test, y_train, y_test = load_data_train_test(12000, X_tot, y_tot, iqr=apply_iqr)\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = load_data_train_test(12000, X_tot, y_tot)\n",
    "\n",
    "if apply_pca:\n",
    "    n = 100\n",
    "    X_train, X_test = do_PCA(X_train, X_test, n)\n",
    "    nb_input_neurons = n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7680 samples, validate on 1920 samples\n",
      "Epoch 1/150\n",
      "7680/7680 [==============================] - 27s 3ms/step - loss: 4.3517 - val_loss: 1.1326\n",
      "Epoch 2/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 1.0146 - val_loss: 0.8708\n",
      "Epoch 3/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 0.8197 - val_loss: 0.7872\n",
      "Epoch 4/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.7380 - val_loss: 0.7370\n",
      "Epoch 5/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.6953 - val_loss: 0.6850\n",
      "Epoch 6/150\n",
      "7680/7680 [==============================] - 28s 4ms/step - loss: 0.6662 - val_loss: 0.6687\n",
      "Epoch 7/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 0.6464 - val_loss: 0.6513\n",
      "Epoch 8/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.6306 - val_loss: 0.6370\n",
      "Epoch 9/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.6140 - val_loss: 0.6369\n",
      "Epoch 10/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 0.6005 - val_loss: 0.6448\n",
      "Epoch 11/150\n",
      "7680/7680 [==============================] - 24s 3ms/step - loss: 0.5937 - val_loss: 0.6185\n",
      "Epoch 12/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.5808 - val_loss: 0.6438\n",
      "Epoch 13/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.5702 - val_loss: 0.6016\n",
      "Epoch 14/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.5661 - val_loss: 0.6178\n",
      "Epoch 15/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.5595 - val_loss: 0.5951\n",
      "Epoch 16/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 0.5458 - val_loss: 0.5860\n",
      "Epoch 17/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.5400 - val_loss: 0.6213\n",
      "Epoch 18/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.5352 - val_loss: 0.5938\n",
      "Epoch 19/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.5322 - val_loss: 0.5697\n",
      "Epoch 20/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.5291 - val_loss: 0.5731\n",
      "Epoch 21/150\n",
      "7680/7680 [==============================] - 25s 3ms/step - loss: 0.5247 - val_loss: 0.5779\n",
      "Epoch 22/150\n",
      "7680/7680 [==============================] - 26s 3ms/step - loss: 0.5207 - val_loss: 0.5709\n",
      "Epoch 23/150\n",
      "7680/7680 [==============================] - 26s 3ms/step - loss: 0.5111 - val_loss: 0.5703\n",
      "Epoch 24/150\n",
      "7680/7680 [==============================] - 26s 3ms/step - loss: 0.5122 - val_loss: 0.5708\n",
      "Epoch 25/150\n",
      "7680/7680 [==============================] - 24s 3ms/step - loss: 0.5086 - val_loss: 0.5652\n",
      "Epoch 26/150\n",
      "7680/7680 [==============================] - 25s 3ms/step - loss: 0.5010 - val_loss: 0.5613\n",
      "Epoch 27/150\n",
      "7680/7680 [==============================] - 27s 3ms/step - loss: 0.4959 - val_loss: 0.5688\n",
      "Epoch 28/150\n",
      "7680/7680 [==============================] - 25s 3ms/step - loss: 0.5056 - val_loss: 0.5712\n",
      "Epoch 29/150\n",
      "7680/7680 [==============================] - 32s 4ms/step - loss: 0.4928 - val_loss: 0.5826\n",
      "Epoch 30/150\n",
      "7680/7680 [==============================] - 27s 4ms/step - loss: 0.4942 - val_loss: 0.5526\n",
      "Epoch 31/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4893 - val_loss: 0.5643\n",
      "Epoch 32/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4875 - val_loss: 0.5527\n",
      "Epoch 33/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4889 - val_loss: 0.5541\n",
      "Epoch 34/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4843 - val_loss: 0.5538\n",
      "Epoch 35/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4794 - val_loss: 0.5501\n",
      "Epoch 36/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4793 - val_loss: 0.5533\n",
      "Epoch 37/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4794 - val_loss: 0.5496\n",
      "Epoch 38/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4790 - val_loss: 0.5770\n",
      "Epoch 39/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4691 - val_loss: 0.5657\n",
      "Epoch 40/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4670 - val_loss: 0.5523\n",
      "Epoch 41/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4724 - val_loss: 0.5714\n",
      "Epoch 42/150\n",
      "7680/7680 [==============================] - 24s 3ms/step - loss: 0.4634 - val_loss: 0.5437\n",
      "Epoch 43/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4632 - val_loss: 0.5801\n",
      "Epoch 44/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4629 - val_loss: 0.5658\n",
      "Epoch 45/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4655 - val_loss: 0.5407\n",
      "Epoch 46/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4596 - val_loss: 0.5432\n",
      "Epoch 47/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4570 - val_loss: 0.5400\n",
      "Epoch 48/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4590 - val_loss: 0.5716\n",
      "Epoch 49/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4620 - val_loss: 0.6188\n",
      "Epoch 50/150\n",
      "7680/7680 [==============================] - 25s 3ms/step - loss: 0.4519 - val_loss: 0.5896\n",
      "Epoch 51/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4595 - val_loss: 0.5511\n",
      "Epoch 52/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4500 - val_loss: 0.5539\n",
      "Epoch 53/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4443 - val_loss: 0.5684\n",
      "Epoch 54/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4489 - val_loss: 0.5926\n",
      "Epoch 55/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4452 - val_loss: 0.5668\n",
      "Epoch 56/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4410 - val_loss: 0.5497\n",
      "Epoch 57/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4469 - val_loss: 0.5646\n",
      "Epoch 58/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4499 - val_loss: 0.5402\n",
      "Epoch 59/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4437 - val_loss: 0.5362\n",
      "Epoch 60/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4432 - val_loss: 0.5575\n",
      "Epoch 61/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4388 - val_loss: 0.5347\n",
      "Epoch 62/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4412 - val_loss: 0.5401\n",
      "Epoch 63/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4405 - val_loss: 0.5477\n",
      "Epoch 64/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4344 - val_loss: 0.5390\n",
      "Epoch 65/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4376 - val_loss: 0.5747\n",
      "Epoch 66/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4343 - val_loss: 0.5600\n",
      "Epoch 67/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4296 - val_loss: 0.5437\n",
      "Epoch 68/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4306 - val_loss: 0.5831\n",
      "Epoch 69/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4345 - val_loss: 0.6191\n",
      "Epoch 70/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4385 - val_loss: 0.5409\n",
      "Epoch 71/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4301 - val_loss: 0.5396\n",
      "Epoch 72/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4304 - val_loss: 0.5517\n",
      "Epoch 73/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4284 - val_loss: 0.5441\n",
      "Epoch 74/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4322 - val_loss: 0.5373\n",
      "Epoch 75/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4269 - val_loss: 0.5354\n",
      "Epoch 76/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4273 - val_loss: 0.5365\n",
      "Epoch 77/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4269 - val_loss: 0.5409\n",
      "Epoch 78/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4241 - val_loss: 0.5347\n",
      "Epoch 79/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4297 - val_loss: 0.5357\n",
      "Epoch 80/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4233 - val_loss: 0.5425\n",
      "Epoch 81/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4249 - val_loss: 0.5616\n",
      "Epoch 82/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4247 - val_loss: 0.5373\n",
      "Epoch 83/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4194 - val_loss: 0.5848\n",
      "Epoch 84/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4280 - val_loss: 0.5360\n",
      "Epoch 85/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4175 - val_loss: 0.5304\n",
      "Epoch 86/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4266 - val_loss: 0.5533\n",
      "Epoch 87/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4179 - val_loss: 0.5327\n",
      "Epoch 88/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4223 - val_loss: 0.5325\n",
      "Epoch 89/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4133 - val_loss: 0.5910\n",
      "Epoch 90/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4191 - val_loss: 0.5628\n",
      "Epoch 91/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4160 - val_loss: 0.5758\n",
      "Epoch 92/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4141 - val_loss: 0.5253\n",
      "Epoch 93/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4142 - val_loss: 0.5455\n",
      "Epoch 94/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 0.4177 - val_loss: 0.5700\n",
      "Epoch 95/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4138 - val_loss: 0.5318\n",
      "Epoch 96/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4152 - val_loss: 0.5523\n",
      "Epoch 97/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4158 - val_loss: 0.5749\n",
      "Epoch 98/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4095 - val_loss: 0.5516\n",
      "Epoch 99/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4130 - val_loss: 0.5440\n",
      "Epoch 100/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4109 - val_loss: 0.5351\n",
      "Epoch 101/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4167 - val_loss: 0.5329\n",
      "Epoch 102/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4066 - val_loss: 0.5391\n",
      "Epoch 103/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4071 - val_loss: 0.5538\n",
      "Epoch 104/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4097 - val_loss: 0.5388\n",
      "Epoch 105/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4115 - val_loss: 0.5352\n",
      "Epoch 106/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4086 - val_loss: 0.5520\n",
      "Epoch 107/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4156 - val_loss: 0.5346\n",
      "Epoch 108/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4067 - val_loss: 0.5387\n",
      "Epoch 109/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4106 - val_loss: 0.5554\n",
      "Epoch 110/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4053 - val_loss: 0.5339\n",
      "Epoch 111/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4091 - val_loss: 0.5254\n",
      "Epoch 112/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4096 - val_loss: 0.5432\n",
      "Epoch 113/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4046 - val_loss: 0.5603\n",
      "Epoch 114/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4054 - val_loss: 0.5843\n",
      "Epoch 115/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4117 - val_loss: 0.5337\n",
      "Epoch 116/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4076 - val_loss: 0.5339\n",
      "Epoch 117/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4081 - val_loss: 0.5346\n",
      "Epoch 118/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4041 - val_loss: 0.5430\n",
      "Epoch 119/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4039 - val_loss: 0.5296\n",
      "Epoch 120/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4007 - val_loss: 0.5362\n",
      "Epoch 121/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4099 - val_loss: 0.5773\n",
      "Epoch 122/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4040 - val_loss: 0.6434\n",
      "Epoch 123/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4040 - val_loss: 0.5409\n",
      "Epoch 124/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4082 - val_loss: 0.5318\n",
      "Epoch 125/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4047 - val_loss: 0.5287\n",
      "Epoch 126/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4008 - val_loss: 0.5337\n",
      "Epoch 127/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4019 - val_loss: 0.5473\n",
      "Epoch 128/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.3997 - val_loss: 0.5272\n",
      "Epoch 129/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.3977 - val_loss: 0.5296\n",
      "Epoch 130/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4032 - val_loss: 0.5574\n",
      "Epoch 131/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.3975 - val_loss: 0.5309\n",
      "Epoch 132/150\n",
      "7680/7680 [==============================] - 25s 3ms/step - loss: 0.3988 - val_loss: 0.5904\n",
      "Epoch 133/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4003 - val_loss: 0.5401\n",
      "Epoch 134/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3983 - val_loss: 0.5326\n",
      "Epoch 135/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3963 - val_loss: 0.5345\n",
      "Epoch 136/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4005 - val_loss: 0.5383\n",
      "Epoch 137/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3957 - val_loss: 0.5797\n",
      "Epoch 138/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4038 - val_loss: 0.5557\n",
      "Epoch 139/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3976 - val_loss: 0.5414\n",
      "Epoch 140/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3942 - val_loss: 0.5449\n",
      "Epoch 141/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3957 - val_loss: 0.5513\n",
      "Epoch 142/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3944 - val_loss: 0.5498\n",
      "Epoch 143/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 0.3943 - val_loss: 0.5328\n",
      "Epoch 144/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3936 - val_loss: 0.5315\n",
      "Epoch 145/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3965 - val_loss: 0.5308\n",
      "Epoch 146/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3920 - val_loss: 0.5547\n",
      "Epoch 147/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3971 - val_loss: 0.5512\n",
      "Epoch 148/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3891 - val_loss: 0.5330\n",
      "Epoch 149/150\n",
      "7680/7680 [==============================] - 24s 3ms/step - loss: 0.4047 - val_loss: 0.5488\n",
      "Epoch 150/150\n",
      "7680/7680 [==============================] - 24s 3ms/step - loss: 0.3939 - val_loss: 0.5453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2bd418eae10>"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_ann = k_models.model_8(X_train, 'mean_absolute_error') \n",
    "k_ann.fit(X_train, y_train, epochs=150, batch_size=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.58 \n",
      "MAE: 0.57\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "y_hat = k_ann.predict(X_test)\n",
    "\n",
    "# compute score\n",
    "mse_nn, mae_nn, _ = compute_score(y_test, y_hat)\n",
    "print('MSE: {:.2f} \\nMAE: {:.2f}'.format(mse_nn, mae_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('y_hat_12000_dataset2_model8.pkl', 'wb') as f:\n",
    "    pickle.dump(y_hat, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results tracking**\n",
    "\n",
    "Model    | MSE  | MAE  | PCA | IQR |    n     | dataset | *n* PCs | Epochs | Scaling | \n",
    "---      | ---  | ---  | --- | --- |   ---    |   ---   |   ---   |   ---  |   ---   |\n",
    "model_6  | 0.93 | 0.70 | Yes | Yes |  20000   |   2     |   40    |   150  |   No    |\n",
    "model_6  | 0.85 | 0.67 | No  | Yes |   4000   |   2     |    -    |   150  |   No    |\n",
    "model_6  | 0.78 | 0.64 | Yes | Yes |  20000   |   2     |   60    |   150  |   No    | \n",
    "model_6  | 0.75 | 0.63 | Yes | Yes |  25000   |   2     |   70    |   200  |   No    |\n",
    "model_6  | 0.67 | 0.60 | No  | Yes |   8000   |   2     |    -    |   150  |   No    |\n",
    "model_6  | 0.75 | 0.62 | Yes | Yes |  25000   |   2     |  100    |   150  |   No    |\n",
    "model_6  | 0.60 | 0.55 | No  | Yes |  12000   |   2     |    -    |   150  |   No    |\n",
    "Net_3    | 1.01 | 0.74 | Yes | Yes |   5000   |   2     |   80    |   100  |   No    |\n",
    "Net_3    | 1.03 | 0.73 | Yes | Yes |   8000   |   2     |   80    |   150  |   No    |\n",
    "Net_3    | 0.65 | 0.60 | No  | Yes |   8000   |   2     |    -    |   150  |   No    |\n",
    "Net_3    | 0.52 | 0.54 | No  | Yes |  12000   |   2     |    -    |   150  |   No    |\n",
    "model_8  | 0.58 | 0.57 | No  | Yes |  12000   |   2     |    -    |   150  |   No    |\n",
    "Net_3    | 0.57 | 0.56 | No  | Yes |  12000   |   3     |    -    |   150  |   No    |\n",
    "Net_3    | 0.65 | 0.62 | No  | Yes |  12000   |   2     |    -    |   150  |   Yes   |\n",
    "Net_3    | 1.00 | 0.76 | Yes | Yes |  12000   |   2     |   80    |   150  |   Yes   |\n",
    "Net_3    | 0.86 | 0.69 | Yes | Yes |  25000   |   2     |   80    |   150  |   Yes   |\n",
    "Net_4    | 0.47 | 0.52 | No  | Yes |  12000   |   2     |   -     |   150  |   Yes   |\n",
    "Net_5    | 0.54 | 0.54 | No  | Yes |  12000   |   2     |   -     |   150  |   Yes   |\n",
    "\n",
    "Note: \n",
    "- Larger increase in performances when using more PCs (say 60).\n",
    "- Obviously computationally less demanding\n",
    "- Will the increase in number of samples without PCA perform better? YES it does.\n",
    "- Loss evolution indicates that 150 epochs are not necessary (try with 120)\n",
    "- Plateau reached using larger number of PCs\n",
    "- $r^2 = 0.95$ for Net_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stats\n",
    "<a id='stats'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 IQR vs. sample number\n",
    "<a id='iqr_vs_n'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I should do a function but j'ai la flemme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 287us/step - loss: 20.0163 - val_loss: 7.9898\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 2.1726 - val_loss: 1.1877\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 1.0646 - val_loss: 0.9910\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.9258 - val_loss: 0.8876\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.8713 - val_loss: 0.8659\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.8536 - val_loss: 0.8499\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.8411 - val_loss: 0.8589\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.8331 - val_loss: 0.8565\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.8326 - val_loss: 0.8517\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.8289 - val_loss: 0.8619\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.8315 - val_loss: 0.8580\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.8284 - val_loss: 0.8839\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.8276 - val_loss: 0.8578\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 185us/step - loss: 0.8262 - val_loss: 0.8565\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 187us/step - loss: 0.8234 - val_loss: 0.8550\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 185us/step - loss: 0.8218 - val_loss: 0.8637\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.8203 - val_loss: 0.8654\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 195us/step - loss: 0.8249 - val_loss: 0.8537\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 181us/step - loss: 0.8196 - val_loss: 0.8522\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.8175 - val_loss: 0.8494\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 181us/step - loss: 0.8189 - val_loss: 0.8510\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.8130 - val_loss: 0.8459\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.8115 - val_loss: 0.8501\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.8087 - val_loss: 0.8495\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.8069 - val_loss: 0.8427\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.8080 - val_loss: 0.8414\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.8061 - val_loss: 0.8522\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.8016 - val_loss: 0.8409\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7983 - val_loss: 0.8705\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7991 - val_loss: 0.8329\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7937 - val_loss: 0.8308\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7925 - val_loss: 0.8290\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7902 - val_loss: 0.8265\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7882 - val_loss: 0.8243\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7854 - val_loss: 0.8280\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7852 - val_loss: 0.8237\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7822 - val_loss: 0.8217\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7798 - val_loss: 0.8380\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7824 - val_loss: 0.8184\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7768 - val_loss: 0.8152\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7756 - val_loss: 0.8149\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7689 - val_loss: 0.8147\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7663 - val_loss: 0.8097\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7664 - val_loss: 0.8070\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7636 - val_loss: 0.8069\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7603 - val_loss: 0.8057\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7607 - val_loss: 0.8039\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 185us/step - loss: 0.7592 - val_loss: 0.7992\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 194us/step - loss: 0.7594 - val_loss: 0.7998\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7536 - val_loss: 0.8180\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7510 - val_loss: 0.7949\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7557 - val_loss: 0.7951\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7491 - val_loss: 0.8020\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 199us/step - loss: 0.7492 - val_loss: 0.7872\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7476 - val_loss: 0.8102\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7464 - val_loss: 0.7925\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7436 - val_loss: 0.7915\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7381 - val_loss: 0.7885\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7400 - val_loss: 0.7872\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7419 - val_loss: 0.7938\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7356 - val_loss: 0.7853\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7365 - val_loss: 0.7846\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 179us/step - loss: 0.7343 - val_loss: 0.7871\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7346 - val_loss: 0.7882\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7333 - val_loss: 0.7803\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7310 - val_loss: 0.7772\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7293 - val_loss: 0.7733\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7275 - val_loss: 0.7920\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 187us/step - loss: 0.7246 - val_loss: 0.7810\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.7224 - val_loss: 0.7719\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7225 - val_loss: 0.7775\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.7231 - val_loss: 0.7733\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 181us/step - loss: 0.7203 - val_loss: 0.8159\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 200us/step - loss: 0.7181 - val_loss: 0.7725\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 193us/step - loss: 0.7240 - val_loss: 0.7762\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 194us/step - loss: 0.7181 - val_loss: 0.7717\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 242us/step - loss: 0.7193 - val_loss: 0.7742\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7169 - val_loss: 0.7762\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7129 - val_loss: 0.7686\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.7127 - val_loss: 0.7704\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.7105 - val_loss: 0.7700\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7133 - val_loss: 0.7780\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 179us/step - loss: 0.7084 - val_loss: 0.7688\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7110 - val_loss: 0.7686\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7097 - val_loss: 0.7681\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7073 - val_loss: 0.7755\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7085 - val_loss: 0.7650\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7098 - val_loss: 0.7698\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7076 - val_loss: 0.7663\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7056 - val_loss: 0.7771\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7043 - val_loss: 0.7711\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7024 - val_loss: 0.7642\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7011 - val_loss: 0.7725\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7073 - val_loss: 0.7868\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6984 - val_loss: 0.7672\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6978 - val_loss: 0.7613\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7003 - val_loss: 0.7802\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 181us/step - loss: 0.6998 - val_loss: 0.7655\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6970 - val_loss: 0.7676\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6975 - val_loss: 0.7742\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6971 - val_loss: 0.7676\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 290us/step - loss: 0.6976 - val_loss: 0.7651\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6936 - val_loss: 0.7629\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 181us/step - loss: 0.6944 - val_loss: 0.7562\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6928 - val_loss: 0.7575\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6892 - val_loss: 0.7613\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6892 - val_loss: 0.7596\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6894 - val_loss: 0.7568\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6911 - val_loss: 0.7557\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6886 - val_loss: 0.7734\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6874 - val_loss: 0.7612\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6899 - val_loss: 0.7583\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6887 - val_loss: 0.7786\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6887 - val_loss: 0.7578\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6875 - val_loss: 0.7518\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6893 - val_loss: 0.7562\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6857 - val_loss: 0.7545\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6835 - val_loss: 0.7568\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6860 - val_loss: 0.7538\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6793 - val_loss: 0.7582\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6794 - val_loss: 0.7717\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6840 - val_loss: 0.7573\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6808 - val_loss: 0.7567\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6827 - val_loss: 0.7575\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6801 - val_loss: 0.7563\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6802 - val_loss: 0.7571\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6777 - val_loss: 0.7591\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6772 - val_loss: 0.7639\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6757 - val_loss: 0.7538\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6734 - val_loss: 0.7735\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6756 - val_loss: 0.7574\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6735 - val_loss: 0.7533\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6786 - val_loss: 0.7531\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6747 - val_loss: 0.7676\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6720 - val_loss: 0.7564\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6738 - val_loss: 0.7556\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6726 - val_loss: 0.7531\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6784 - val_loss: 0.7657\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6740 - val_loss: 0.7525\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6715 - val_loss: 0.7631\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6683 - val_loss: 0.7489\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6728 - val_loss: 0.7548\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6671 - val_loss: 0.7529\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6688 - val_loss: 0.7497\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6678 - val_loss: 0.7816\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6677 - val_loss: 0.7643\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6688 - val_loss: 0.7523\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6664 - val_loss: 0.7489\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6676 - val_loss: 0.7442\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6698 - val_loss: 0.7546\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 283us/step - loss: 19.3701 - val_loss: 6.9150\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 1.9228 - val_loss: 1.1749\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.9935 - val_loss: 0.9364\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.8667 - val_loss: 0.8616\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.8181 - val_loss: 0.8320\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.8010 - val_loss: 0.8137\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7988 - val_loss: 0.8149\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7931 - val_loss: 0.7984\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7862 - val_loss: 0.7959\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7832 - val_loss: 0.7950\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7842 - val_loss: 0.7864\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7836 - val_loss: 0.8022\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7811 - val_loss: 0.7964\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7806 - val_loss: 0.7806\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7789 - val_loss: 0.7863\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7792 - val_loss: 0.7783\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7791 - val_loss: 0.7799\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7734 - val_loss: 0.7794\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7730 - val_loss: 0.7960\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7726 - val_loss: 0.7797\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7689 - val_loss: 0.7761\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7668 - val_loss: 0.7899\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7674 - val_loss: 0.7779\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7643 - val_loss: 0.7777\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7626 - val_loss: 0.7904\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7618 - val_loss: 0.7931\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7587 - val_loss: 0.7688\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7585 - val_loss: 0.7702\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7534 - val_loss: 0.7835\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7512 - val_loss: 0.7709\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7492 - val_loss: 0.7689\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7480 - val_loss: 0.7678\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7433 - val_loss: 0.7657\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7455 - val_loss: 0.7666\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7446 - val_loss: 0.7634\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7410 - val_loss: 0.7612\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7428 - val_loss: 0.7678\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7379 - val_loss: 0.7668\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7348 - val_loss: 0.7871\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7332 - val_loss: 0.7616\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7339 - val_loss: 0.7652\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7306 - val_loss: 0.7578\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7289 - val_loss: 0.7963\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7244 - val_loss: 0.7657\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7252 - val_loss: 0.7548\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7216 - val_loss: 0.7577\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7184 - val_loss: 0.7582\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7198 - val_loss: 0.7561\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7203 - val_loss: 0.7525\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7170 - val_loss: 0.7527\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7104 - val_loss: 0.7541\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7128 - val_loss: 0.7531\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7093 - val_loss: 0.7517\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7079 - val_loss: 0.7461\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7096 - val_loss: 0.7571\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7073 - val_loss: 0.7503\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7075 - val_loss: 0.7561\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7059 - val_loss: 0.7535\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7004 - val_loss: 0.7467\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7027 - val_loss: 0.7602\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6979 - val_loss: 0.7496\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6977 - val_loss: 0.7556\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7021 - val_loss: 0.7689\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6972 - val_loss: 0.7480\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6977 - val_loss: 0.7519\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6946 - val_loss: 0.7658\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6931 - val_loss: 0.7476\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6903 - val_loss: 0.7632\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6906 - val_loss: 0.7381\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6894 - val_loss: 0.7357\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6864 - val_loss: 0.7491\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6889 - val_loss: 0.7496\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6861 - val_loss: 0.7377\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6919 - val_loss: 0.7333\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6831 - val_loss: 0.7348\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6826 - val_loss: 0.7397\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6812 - val_loss: 0.7363\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6822 - val_loss: 0.7403\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6843 - val_loss: 0.7367\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6813 - val_loss: 0.7324\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6813 - val_loss: 0.7370\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6753 - val_loss: 0.7468\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6811 - val_loss: 0.7401\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6757 - val_loss: 0.7393\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6739 - val_loss: 0.7640\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6788 - val_loss: 0.7538\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6697 - val_loss: 0.7385\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6766 - val_loss: 0.7312\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6704 - val_loss: 0.7349\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6713 - val_loss: 0.7531\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6700 - val_loss: 0.7435\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6670 - val_loss: 0.7422\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6674 - val_loss: 0.7719\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6683 - val_loss: 0.7400\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6687 - val_loss: 0.7332\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6666 - val_loss: 0.7624\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6684 - val_loss: 0.7370\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6634 - val_loss: 0.7358\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6681 - val_loss: 0.7452\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6612 - val_loss: 0.7359\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6605 - val_loss: 0.7346\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6655 - val_loss: 0.7470\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6599 - val_loss: 0.7355\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6605 - val_loss: 0.7521\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6580 - val_loss: 0.7387\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6574 - val_loss: 0.7390\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6629 - val_loss: 0.7389\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6606 - val_loss: 0.7344\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6557 - val_loss: 0.7405\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6573 - val_loss: 0.7315\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6563 - val_loss: 0.7335\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6559 - val_loss: 0.7377\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6526 - val_loss: 0.7294\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6535 - val_loss: 0.7329\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6564 - val_loss: 0.7298\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6555 - val_loss: 0.7542\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6526 - val_loss: 0.7366\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6541 - val_loss: 0.7349\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6524 - val_loss: 0.7389\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6499 - val_loss: 0.7287\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6484 - val_loss: 0.7331\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6497 - val_loss: 0.7356\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6508 - val_loss: 0.7487\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6539 - val_loss: 0.7456\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6466 - val_loss: 0.7594\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6475 - val_loss: 0.7361\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6466 - val_loss: 0.7350\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6441 - val_loss: 0.7354\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6461 - val_loss: 0.7383\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6482 - val_loss: 0.7433\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6460 - val_loss: 0.7444\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6480 - val_loss: 0.7378\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6457 - val_loss: 0.7406\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6427 - val_loss: 0.7324\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6451 - val_loss: 0.7337\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6481 - val_loss: 0.7448\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6438 - val_loss: 0.7329\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6444 - val_loss: 0.7388\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6419 - val_loss: 0.7507\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6449 - val_loss: 0.7522\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6423 - val_loss: 0.7406\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6435 - val_loss: 0.7725\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6462 - val_loss: 0.7420\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6400 - val_loss: 0.7397\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6386 - val_loss: 0.7389\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6383 - val_loss: 0.7509\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6427 - val_loss: 0.7373\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6416 - val_loss: 0.7449\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6401 - val_loss: 0.7384\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6386 - val_loss: 0.7363\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 290us/step - loss: 20.2210 - val_loss: 9.1806\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 2.4785 - val_loss: 1.1528\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 1.0547 - val_loss: 0.9443\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.9072 - val_loss: 0.8665\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 180us/step - loss: 0.8487 - val_loss: 0.8495\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.8220 - val_loss: 0.8276\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.8148 - val_loss: 0.8285\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.8117 - val_loss: 0.8282\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.8065 - val_loss: 0.8261\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.8019 - val_loss: 0.8238\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7996 - val_loss: 0.8220\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7975 - val_loss: 0.8246\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7955 - val_loss: 0.8202\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7975 - val_loss: 0.8359\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7929 - val_loss: 0.8301\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7901 - val_loss: 0.8166\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7858 - val_loss: 0.8155\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7877 - val_loss: 0.8204\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7839 - val_loss: 0.8121\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7780 - val_loss: 0.8101\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7748 - val_loss: 0.8232\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7725 - val_loss: 0.8072\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7703 - val_loss: 0.8250\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7667 - val_loss: 0.8027\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7615 - val_loss: 0.8040\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7602 - val_loss: 0.7934\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7594 - val_loss: 0.8174\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7560 - val_loss: 0.7924\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7555 - val_loss: 0.7986\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7558 - val_loss: 0.7949\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7444 - val_loss: 0.8059\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7443 - val_loss: 0.7863\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7411 - val_loss: 0.7848\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7390 - val_loss: 0.7812\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7337 - val_loss: 0.7884\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7325 - val_loss: 0.7770\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7321 - val_loss: 0.7774\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7307 - val_loss: 0.7795\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7267 - val_loss: 0.7668\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7228 - val_loss: 0.7720\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7244 - val_loss: 0.7679\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7210 - val_loss: 0.7720\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7177 - val_loss: 0.7640\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7162 - val_loss: 0.7632\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7114 - val_loss: 0.7588\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7155 - val_loss: 0.7695\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7082 - val_loss: 0.7577\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7086 - val_loss: 0.7664\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7050 - val_loss: 0.7569\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7050 - val_loss: 0.7607\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7030 - val_loss: 0.7590\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6985 - val_loss: 0.7541\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6992 - val_loss: 0.7589\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6965 - val_loss: 0.7535\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6922 - val_loss: 0.7460\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6962 - val_loss: 0.7491\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6928 - val_loss: 0.7526\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6892 - val_loss: 0.7462\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6869 - val_loss: 0.7505\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6853 - val_loss: 0.7482\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6864 - val_loss: 0.7465\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6850 - val_loss: 0.7468\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6823 - val_loss: 0.7595\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6819 - val_loss: 0.7518\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6808 - val_loss: 0.7420\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6806 - val_loss: 0.7501\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6782 - val_loss: 0.7453\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6755 - val_loss: 0.7387\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6761 - val_loss: 0.7399\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6726 - val_loss: 0.7448\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6786 - val_loss: 0.7358\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6762 - val_loss: 0.7419\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6718 - val_loss: 0.7456\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6711 - val_loss: 0.7362\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6684 - val_loss: 0.7365\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6664 - val_loss: 0.7437\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6706 - val_loss: 0.7336\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6663 - val_loss: 0.7382\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6681 - val_loss: 0.7376\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6675 - val_loss: 0.7319\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6649 - val_loss: 0.7348\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6594 - val_loss: 0.7371\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6641 - val_loss: 0.7414\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6584 - val_loss: 0.7391\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 179us/step - loss: 0.6601 - val_loss: 0.7325\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6602 - val_loss: 0.7377\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6635 - val_loss: 0.7399\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6587 - val_loss: 0.7476\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6613 - val_loss: 0.7373\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6582 - val_loss: 0.7438\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6544 - val_loss: 0.7347\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6557 - val_loss: 0.7385\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6565 - val_loss: 0.7328\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6534 - val_loss: 0.7372\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6532 - val_loss: 0.7439\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6505 - val_loss: 0.7422\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6503 - val_loss: 0.7404\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6533 - val_loss: 0.7618\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6498 - val_loss: 0.7329\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6469 - val_loss: 0.7348\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6542 - val_loss: 0.7329\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6481 - val_loss: 0.7501\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6509 - val_loss: 0.7549\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6482 - val_loss: 0.7363\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6473 - val_loss: 0.7325\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6469 - val_loss: 0.7330\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6524 - val_loss: 0.7426\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6499 - val_loss: 0.7315\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6523 - val_loss: 0.7431\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6457 - val_loss: 0.7388\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6418 - val_loss: 0.7424\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6421 - val_loss: 0.7320\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6423 - val_loss: 0.7372\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6420 - val_loss: 0.7280\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6401 - val_loss: 0.7297\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6416 - val_loss: 0.7311\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6430 - val_loss: 0.7275\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6399 - val_loss: 0.7363\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6435 - val_loss: 0.7315\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6400 - val_loss: 0.7333\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6388 - val_loss: 0.7464\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6388 - val_loss: 0.7255\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6404 - val_loss: 0.7520\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6395 - val_loss: 0.7275\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6367 - val_loss: 0.7422\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6378 - val_loss: 0.7290\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6383 - val_loss: 0.7362\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6374 - val_loss: 0.7368\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6375 - val_loss: 0.7298\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6441 - val_loss: 0.7461\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6340 - val_loss: 0.7364\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6344 - val_loss: 0.7414\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6360 - val_loss: 0.7353\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6335 - val_loss: 0.7321\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6334 - val_loss: 0.7292\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6329 - val_loss: 0.7259\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6308 - val_loss: 0.7310\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6338 - val_loss: 0.7237\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6321 - val_loss: 0.7295\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6302 - val_loss: 0.7306\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6309 - val_loss: 0.7300\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6311 - val_loss: 0.7269\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6309 - val_loss: 0.7255\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6285 - val_loss: 0.7309\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6287 - val_loss: 0.7238\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6298 - val_loss: 0.7674\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6303 - val_loss: 0.7312\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6297 - val_loss: 0.7328\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6263 - val_loss: 0.7454\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6312 - val_loss: 0.7286\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 286us/step - loss: 20.3486 - val_loss: 9.8560\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 2.6836 - val_loss: 1.2067\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 1.0500 - val_loss: 0.9389\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.8858 - val_loss: 0.8425\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.8228 - val_loss: 0.8246\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.8013 - val_loss: 0.8010\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7957 - val_loss: 0.7925\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7903 - val_loss: 0.7902\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7872 - val_loss: 0.8064\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7872 - val_loss: 0.8018\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7901 - val_loss: 0.8072\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7833 - val_loss: 0.8179\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7826 - val_loss: 0.8005\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7881 - val_loss: 0.7929\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7847 - val_loss: 0.7950\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7842 - val_loss: 0.7971\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7792 - val_loss: 0.7948\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7788 - val_loss: 0.7907\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7754 - val_loss: 0.8025\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7749 - val_loss: 0.7893\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7737 - val_loss: 0.7919\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7750 - val_loss: 0.7897\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7699 - val_loss: 0.7884\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7720 - val_loss: 0.7876\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7681 - val_loss: 0.7883\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7652 - val_loss: 0.7858\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7613 - val_loss: 0.7867\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7619 - val_loss: 0.7864\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7637 - val_loss: 0.7848\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7577 - val_loss: 0.7934\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7607 - val_loss: 0.7855\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7543 - val_loss: 0.8215\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7529 - val_loss: 0.7806\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7493 - val_loss: 0.7800\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7483 - val_loss: 0.7807\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7463 - val_loss: 0.7878\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7424 - val_loss: 0.8172\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7467 - val_loss: 0.7976\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7428 - val_loss: 0.7845\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7374 - val_loss: 0.7758\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7341 - val_loss: 0.7862\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7345 - val_loss: 0.7741\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7316 - val_loss: 0.7804\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7301 - val_loss: 0.7744\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7292 - val_loss: 0.7702\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7261 - val_loss: 0.8116\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7232 - val_loss: 0.7782\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7247 - val_loss: 0.7784\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7188 - val_loss: 0.7748\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7166 - val_loss: 0.7764\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7190 - val_loss: 0.7740\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7137 - val_loss: 0.7763\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7103 - val_loss: 0.7693\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7115 - val_loss: 0.7645\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7099 - val_loss: 0.7782\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7098 - val_loss: 0.7661\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7062 - val_loss: 0.7619\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7011 - val_loss: 0.7675\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.7047 - val_loss: 0.7675\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6991 - val_loss: 0.7703\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7008 - val_loss: 0.7613\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6967 - val_loss: 0.7649\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6955 - val_loss: 0.7660\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6950 - val_loss: 0.7613\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6928 - val_loss: 0.8051\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6922 - val_loss: 0.7669\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6898 - val_loss: 0.7643\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6888 - val_loss: 0.7585\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6848 - val_loss: 0.7485\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6854 - val_loss: 0.7492\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6854 - val_loss: 0.7515\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6870 - val_loss: 0.7557\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6823 - val_loss: 0.7467\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6792 - val_loss: 0.7526\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6828 - val_loss: 0.7533\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6761 - val_loss: 0.7513\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6822 - val_loss: 0.7530\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6746 - val_loss: 0.7554\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6720 - val_loss: 0.7558\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6710 - val_loss: 0.7500\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6710 - val_loss: 0.7423\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6680 - val_loss: 0.7568\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6709 - val_loss: 0.7465\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6685 - val_loss: 0.7454\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6674 - val_loss: 0.7492\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6626 - val_loss: 0.7409\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6611 - val_loss: 0.7461\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6618 - val_loss: 0.7399\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6639 - val_loss: 0.7523\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6592 - val_loss: 0.7510\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6586 - val_loss: 0.7436\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6584 - val_loss: 0.7364\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6558 - val_loss: 0.7494\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6575 - val_loss: 0.7411\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6591 - val_loss: 0.7397\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6530 - val_loss: 0.7420\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6511 - val_loss: 0.7498\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6517 - val_loss: 0.7383\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6498 - val_loss: 0.7428\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6502 - val_loss: 0.7454\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6501 - val_loss: 0.7500\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6468 - val_loss: 0.7421\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6484 - val_loss: 0.7352\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6438 - val_loss: 0.7512\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6478 - val_loss: 0.7488\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6450 - val_loss: 0.7486\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6449 - val_loss: 0.7486\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6445 - val_loss: 0.7398\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6414 - val_loss: 0.7368\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6422 - val_loss: 0.7372\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6442 - val_loss: 0.7493\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6401 - val_loss: 0.7413\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6380 - val_loss: 0.7479\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6384 - val_loss: 0.7510\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6350 - val_loss: 0.7637\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6363 - val_loss: 0.7369\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6346 - val_loss: 0.7415\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6314 - val_loss: 0.7510\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6346 - val_loss: 0.7412\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6328 - val_loss: 0.7350\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6326 - val_loss: 0.7474\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6308 - val_loss: 0.7497\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6326 - val_loss: 0.7336\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6295 - val_loss: 0.7435\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6289 - val_loss: 0.7551\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6260 - val_loss: 0.7859\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6290 - val_loss: 0.7391\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6285 - val_loss: 0.7375\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6296 - val_loss: 0.7440\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6239 - val_loss: 0.7508\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6288 - val_loss: 0.7415\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6249 - val_loss: 0.7420\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6284 - val_loss: 0.7324\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6246 - val_loss: 0.7499\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6241 - val_loss: 0.7399\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6229 - val_loss: 0.7347\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6196 - val_loss: 0.7424\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6200 - val_loss: 0.7394\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6204 - val_loss: 0.7378\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6183 - val_loss: 0.7461\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6202 - val_loss: 0.7425\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6219 - val_loss: 0.7550\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6201 - val_loss: 0.7394\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6175 - val_loss: 0.7700\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6156 - val_loss: 0.7534\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6134 - val_loss: 0.7562\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6128 - val_loss: 0.7374\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6185 - val_loss: 0.7423\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6161 - val_loss: 0.7542\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6104 - val_loss: 0.7563\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 201us/step - loss: 5.9219 - val_loss: 0.8550\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.8129 - val_loss: 0.8282\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.8026 - val_loss: 0.7981\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7992 - val_loss: 0.7967\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7927 - val_loss: 0.7876\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7854 - val_loss: 0.7774\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7760 - val_loss: 0.7798\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7674 - val_loss: 0.7789\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7575 - val_loss: 0.7588\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7486 - val_loss: 0.7674\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7421 - val_loss: 0.7458\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7358 - val_loss: 0.7518\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.7298 - val_loss: 0.7325\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.7228 - val_loss: 0.7308\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.7169 - val_loss: 0.7304\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7118 - val_loss: 0.7207\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7065 - val_loss: 0.7333\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7017 - val_loss: 0.7359\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6972 - val_loss: 0.7206\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6934 - val_loss: 0.7126\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6881 - val_loss: 0.7153\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6867 - val_loss: 0.7130\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6817 - val_loss: 0.7070\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6823 - val_loss: 0.7016\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6785 - val_loss: 0.7086\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6770 - val_loss: 0.7058\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6740 - val_loss: 0.7004\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6700 - val_loss: 0.6975\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6673 - val_loss: 0.6950\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6642 - val_loss: 0.7061\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6618 - val_loss: 0.6936\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6608 - val_loss: 0.7000\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6569 - val_loss: 0.7067\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6560 - val_loss: 0.6917\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6522 - val_loss: 0.6954\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6514 - val_loss: 0.6960\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6511 - val_loss: 0.6919\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6483 - val_loss: 0.6892\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6477 - val_loss: 0.6884\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6455 - val_loss: 0.6922\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6426 - val_loss: 0.6910\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6430 - val_loss: 0.7009\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6409 - val_loss: 0.6996\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6390 - val_loss: 0.6855\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6390 - val_loss: 0.6840\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6375 - val_loss: 0.6814\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6352 - val_loss: 0.6813\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6349 - val_loss: 0.6855\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6328 - val_loss: 0.6831\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6351 - val_loss: 0.6832\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6332 - val_loss: 0.6805\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6308 - val_loss: 0.6986\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6302 - val_loss: 0.6823\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6274 - val_loss: 0.6984\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6282 - val_loss: 0.6782\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6272 - val_loss: 0.6863\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6251 - val_loss: 0.6778\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6235 - val_loss: 0.6794\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6235 - val_loss: 0.6762\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6229 - val_loss: 0.6780\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6209 - val_loss: 0.6829\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6221 - val_loss: 0.6735\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6221 - val_loss: 0.6817\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6209 - val_loss: 0.6815\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6189 - val_loss: 0.6737\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6184 - val_loss: 0.6737\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6179 - val_loss: 0.6826\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6201 - val_loss: 0.6791\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6168 - val_loss: 0.6738\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6163 - val_loss: 0.6751\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6144 - val_loss: 0.6948\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6159 - val_loss: 0.6793\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6134 - val_loss: 0.6763\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6139 - val_loss: 0.6733\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6130 - val_loss: 0.6746\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6131 - val_loss: 0.6836\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6113 - val_loss: 0.6766\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6113 - val_loss: 0.6779\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6109 - val_loss: 0.6835\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6090 - val_loss: 0.6691\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6074 - val_loss: 0.6748\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6086 - val_loss: 0.6842\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6075 - val_loss: 0.6709\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6070 - val_loss: 0.6726\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6066 - val_loss: 0.6747\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6066 - val_loss: 0.6785\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6059 - val_loss: 0.6914\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6072 - val_loss: 0.6765\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6053 - val_loss: 0.6703\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6028 - val_loss: 0.6706\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6038 - val_loss: 0.6800\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6035 - val_loss: 0.6777\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6033 - val_loss: 0.6682\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6027 - val_loss: 0.6698\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6019 - val_loss: 0.6694\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6014 - val_loss: 0.6809\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6012 - val_loss: 0.6769\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6006 - val_loss: 0.6767\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6026 - val_loss: 0.6760\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6027 - val_loss: 0.6756\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5989 - val_loss: 0.6703\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6001 - val_loss: 0.6776\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5974 - val_loss: 0.6771\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5986 - val_loss: 0.6859\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5999 - val_loss: 0.6891\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5986 - val_loss: 0.6687\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5979 - val_loss: 0.6771\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5976 - val_loss: 0.6760\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5972 - val_loss: 0.6743\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5974 - val_loss: 0.6737\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5949 - val_loss: 0.6686\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5967 - val_loss: 0.6871\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5957 - val_loss: 0.6811\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5950 - val_loss: 0.6798\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5954 - val_loss: 0.6676\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5946 - val_loss: 0.6742\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5960 - val_loss: 0.6711\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5956 - val_loss: 0.6772\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5937 - val_loss: 0.6741\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 178us/step - loss: 0.5947 - val_loss: 0.6707\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5935 - val_loss: 0.6787\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5939 - val_loss: 0.6877\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5948 - val_loss: 0.6701\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5920 - val_loss: 0.6708\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5940 - val_loss: 0.6755\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5902 - val_loss: 0.6735\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5952 - val_loss: 0.6681\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5926 - val_loss: 0.6762\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5917 - val_loss: 0.6748\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5927 - val_loss: 0.6728\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5899 - val_loss: 0.6697\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5908 - val_loss: 0.6752\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5909 - val_loss: 0.6834\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5906 - val_loss: 0.6915\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5894 - val_loss: 0.6723\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5892 - val_loss: 0.6655\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5909 - val_loss: 0.6737\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5880 - val_loss: 0.6708\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5932 - val_loss: 0.6770\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5902 - val_loss: 0.6756\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5898 - val_loss: 0.6863\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5914 - val_loss: 0.6713\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5887 - val_loss: 0.6848\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5896 - val_loss: 0.6685\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5915 - val_loss: 0.6701\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5893 - val_loss: 0.6741\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5886 - val_loss: 0.6769\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5881 - val_loss: 0.6663\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5882 - val_loss: 0.6678\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5860 - val_loss: 0.6743\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 202us/step - loss: 5.9861 - val_loss: 0.8867\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.8206 - val_loss: 0.8384\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.8068 - val_loss: 0.8295\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.8025 - val_loss: 0.8309\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7986 - val_loss: 0.8306\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7914 - val_loss: 0.8223\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7881 - val_loss: 0.8121\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7849 - val_loss: 0.8287\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7807 - val_loss: 0.8014\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7740 - val_loss: 0.7997\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7689 - val_loss: 0.7937\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7626 - val_loss: 0.7955\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7616 - val_loss: 0.7920\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7563 - val_loss: 0.7803\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7533 - val_loss: 0.8222\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7502 - val_loss: 0.7857\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7495 - val_loss: 0.7736\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7443 - val_loss: 0.7834\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7409 - val_loss: 0.7804\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7404 - val_loss: 0.7718\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7341 - val_loss: 0.7647\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7335 - val_loss: 0.7639\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7315 - val_loss: 0.7590\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7289 - val_loss: 0.7623\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7280 - val_loss: 0.7585\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7263 - val_loss: 0.7561\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7230 - val_loss: 0.7811\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7199 - val_loss: 0.7757\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7195 - val_loss: 0.7525\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7193 - val_loss: 0.7553\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7158 - val_loss: 0.7715\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7138 - val_loss: 0.7641\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7114 - val_loss: 0.7571\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7117 - val_loss: 0.7517\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7099 - val_loss: 0.7559\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7075 - val_loss: 0.7495\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7066 - val_loss: 0.7446\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7059 - val_loss: 0.7450\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7043 - val_loss: 0.7424\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7010 - val_loss: 0.7696\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7001 - val_loss: 0.7518\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6968 - val_loss: 0.7395\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6942 - val_loss: 0.7370\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6934 - val_loss: 0.7373\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6913 - val_loss: 0.7330\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6896 - val_loss: 0.7417\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6876 - val_loss: 0.7355\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6871 - val_loss: 0.7416\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6843 - val_loss: 0.7329\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6839 - val_loss: 0.7931\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6827 - val_loss: 0.7298\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6810 - val_loss: 0.7275\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6819 - val_loss: 0.7303\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6789 - val_loss: 0.7385\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6780 - val_loss: 0.7306\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6763 - val_loss: 0.7376\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6759 - val_loss: 0.7366\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6741 - val_loss: 0.7238\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6722 - val_loss: 0.7290\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6726 - val_loss: 0.7187\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6723 - val_loss: 0.7240\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6684 - val_loss: 0.7258\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6708 - val_loss: 0.7253\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6703 - val_loss: 0.7244\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6666 - val_loss: 0.7242\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6679 - val_loss: 0.7248\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6660 - val_loss: 0.7191\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6666 - val_loss: 0.7209\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6673 - val_loss: 0.7221\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6654 - val_loss: 0.7251\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6671 - val_loss: 0.7130\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6648 - val_loss: 0.7188\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6648 - val_loss: 0.7132\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6624 - val_loss: 0.7237\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6642 - val_loss: 0.7246\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6629 - val_loss: 0.7197\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6627 - val_loss: 0.7099\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6615 - val_loss: 0.7102\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6612 - val_loss: 0.7228\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6618 - val_loss: 0.7088\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6604 - val_loss: 0.7185\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6615 - val_loss: 0.7207\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6589 - val_loss: 0.7175\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6593 - val_loss: 0.7143\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6596 - val_loss: 0.7318\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6573 - val_loss: 0.7176\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6586 - val_loss: 0.7105\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6590 - val_loss: 0.7257\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6570 - val_loss: 0.7220\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6573 - val_loss: 0.7118\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6580 - val_loss: 0.7078\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6547 - val_loss: 0.7158\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6559 - val_loss: 0.7123\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6552 - val_loss: 0.7144\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6548 - val_loss: 0.7034\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6545 - val_loss: 0.7192\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6533 - val_loss: 0.7234\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6544 - val_loss: 0.7271\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6546 - val_loss: 0.7114\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6535 - val_loss: 0.7148\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6532 - val_loss: 0.7092\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6535 - val_loss: 0.7172\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6523 - val_loss: 0.7119\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6523 - val_loss: 0.7044\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6531 - val_loss: 0.7065\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6517 - val_loss: 0.7206\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6508 - val_loss: 0.7154\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6522 - val_loss: 0.7056\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6520 - val_loss: 0.7147\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6524 - val_loss: 0.7106\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6516 - val_loss: 0.7020\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6502 - val_loss: 0.7033\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6502 - val_loss: 0.7149\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6497 - val_loss: 0.7249\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6508 - val_loss: 0.7048\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6494 - val_loss: 0.7114\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6505 - val_loss: 0.7033\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 178us/step - loss: 0.6503 - val_loss: 0.7096\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6482 - val_loss: 0.7069\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 180us/step - loss: 0.6489 - val_loss: 0.7111\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6491 - val_loss: 0.7457\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6503 - val_loss: 0.7118\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6485 - val_loss: 0.7067\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6485 - val_loss: 0.7275\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6489 - val_loss: 0.7024\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6491 - val_loss: 0.7074\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6493 - val_loss: 0.7193\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6470 - val_loss: 0.7449\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6474 - val_loss: 0.7099\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6471 - val_loss: 0.7207\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6470 - val_loss: 0.7352\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6467 - val_loss: 0.7132\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6479 - val_loss: 0.7074\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6473 - val_loss: 0.7069\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6459 - val_loss: 0.7070\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6452 - val_loss: 0.7160\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6467 - val_loss: 0.7123\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6480 - val_loss: 0.7032\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6453 - val_loss: 0.7053\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6451 - val_loss: 0.7131\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6480 - val_loss: 0.7044\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6435 - val_loss: 0.7037\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6439 - val_loss: 0.7010\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6462 - val_loss: 0.7146\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6440 - val_loss: 0.7101\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6444 - val_loss: 0.7173\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6451 - val_loss: 0.7064\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6448 - val_loss: 0.7058\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6445 - val_loss: 0.7041\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6424 - val_loss: 0.7156\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 199us/step - loss: 6.3541 - val_loss: 0.8404\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.8040 - val_loss: 0.7933\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.7906 - val_loss: 0.7867\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7859 - val_loss: 0.7834\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7786 - val_loss: 0.7790\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7740 - val_loss: 0.7714\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7650 - val_loss: 0.8286\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7585 - val_loss: 0.7612\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7483 - val_loss: 0.7638\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.7421 - val_loss: 0.7481\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.7350 - val_loss: 0.7491\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7288 - val_loss: 0.7634\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7223 - val_loss: 0.7351\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.7157 - val_loss: 0.7311\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7105 - val_loss: 0.7238\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7058 - val_loss: 0.7404\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7041 - val_loss: 0.7261\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6972 - val_loss: 0.7165\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6975 - val_loss: 0.7098\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6894 - val_loss: 0.7301\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6878 - val_loss: 0.7088\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6862 - val_loss: 0.7044\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6843 - val_loss: 0.7020\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6795 - val_loss: 0.6981\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6794 - val_loss: 0.7387\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6767 - val_loss: 0.6993\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6741 - val_loss: 0.6992\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6725 - val_loss: 0.6966\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6698 - val_loss: 0.6986\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6683 - val_loss: 0.7105\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6664 - val_loss: 0.6982\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6649 - val_loss: 0.6933\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6626 - val_loss: 0.6952\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6611 - val_loss: 0.7010\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6629 - val_loss: 0.7004\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6597 - val_loss: 0.7101\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6591 - val_loss: 0.7004\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6575 - val_loss: 0.7078\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6557 - val_loss: 0.6912\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6548 - val_loss: 0.6912\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6532 - val_loss: 0.6853\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6521 - val_loss: 0.6892\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6503 - val_loss: 0.6904\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6518 - val_loss: 0.6850\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6491 - val_loss: 0.6879\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6474 - val_loss: 0.6870\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6457 - val_loss: 0.6818\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6438 - val_loss: 0.6795\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6456 - val_loss: 0.6838\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6443 - val_loss: 0.6831\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6416 - val_loss: 0.6939\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6426 - val_loss: 0.7040\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6385 - val_loss: 0.6886\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6387 - val_loss: 0.6894\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6353 - val_loss: 0.6831\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6359 - val_loss: 0.6829\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6347 - val_loss: 0.6831\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6355 - val_loss: 0.6809\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6335 - val_loss: 0.6815\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6323 - val_loss: 0.6967\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6323 - val_loss: 0.6876\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6320 - val_loss: 0.6805\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6316 - val_loss: 0.6794\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6307 - val_loss: 0.6860\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6278 - val_loss: 0.6747\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6287 - val_loss: 0.6774\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6265 - val_loss: 0.6772\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6265 - val_loss: 0.6821\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6248 - val_loss: 0.6785\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6245 - val_loss: 0.6786\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6247 - val_loss: 0.6857\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6223 - val_loss: 0.6840\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6222 - val_loss: 0.6862\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6234 - val_loss: 0.6834\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6218 - val_loss: 0.6752\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6192 - val_loss: 0.6799\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6196 - val_loss: 0.6897\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6164 - val_loss: 0.7007\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6193 - val_loss: 0.6780\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6167 - val_loss: 0.7053\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6175 - val_loss: 0.6789\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6150 - val_loss: 0.6731\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6152 - val_loss: 0.6845\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6154 - val_loss: 0.6942\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6134 - val_loss: 0.6813\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6148 - val_loss: 0.6828\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6162 - val_loss: 0.6752\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6149 - val_loss: 0.6759\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6124 - val_loss: 0.6797\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6121 - val_loss: 0.6801\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6111 - val_loss: 0.6753\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6112 - val_loss: 0.6765\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6114 - val_loss: 0.6766\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6110 - val_loss: 0.6811\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6108 - val_loss: 0.6710\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6091 - val_loss: 0.6706\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6118 - val_loss: 0.6720\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6099 - val_loss: 0.6765\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6089 - val_loss: 0.6840\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6087 - val_loss: 0.6781\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6074 - val_loss: 0.6734\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6080 - val_loss: 0.6704\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6069 - val_loss: 0.6747\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6059 - val_loss: 0.6721\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6059 - val_loss: 0.6713\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6051 - val_loss: 0.6721\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6069 - val_loss: 0.6831\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6067 - val_loss: 0.6810\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6055 - val_loss: 0.6849\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6055 - val_loss: 0.6727\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6034 - val_loss: 0.6822\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6041 - val_loss: 0.6762\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6051 - val_loss: 0.6746\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6025 - val_loss: 0.6739\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6049 - val_loss: 0.6715\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6052 - val_loss: 0.6705\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6011 - val_loss: 0.6716\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6019 - val_loss: 0.6957\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6027 - val_loss: 0.6699\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6038 - val_loss: 0.6765\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6004 - val_loss: 0.6687\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6000 - val_loss: 0.6877\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6002 - val_loss: 0.6821\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6016 - val_loss: 0.6658\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6005 - val_loss: 0.6933\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6003 - val_loss: 0.6667\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5968 - val_loss: 0.6764\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5990 - val_loss: 0.6704\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5982 - val_loss: 0.6705\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5990 - val_loss: 0.6749\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5986 - val_loss: 0.6886\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5996 - val_loss: 0.6778\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5970 - val_loss: 0.6768\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5985 - val_loss: 0.6714\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5957 - val_loss: 0.6729\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5972 - val_loss: 0.6619\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5973 - val_loss: 0.6890\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5983 - val_loss: 0.6680\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.5964 - val_loss: 0.6639\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.5959 - val_loss: 0.6797\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5950 - val_loss: 0.6742\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5961 - val_loss: 0.6708\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5954 - val_loss: 0.6900\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5944 - val_loss: 0.6617\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5946 - val_loss: 0.6726\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5951 - val_loss: 0.6653\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5959 - val_loss: 0.6623\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5941 - val_loss: 0.6747\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5948 - val_loss: 0.6622\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5962 - val_loss: 0.6677\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 206us/step - loss: 6.0403 - val_loss: 0.8998\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.8163 - val_loss: 0.8320\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.7985 - val_loss: 0.8351\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7920 - val_loss: 0.8169\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7887 - val_loss: 0.8147\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7829 - val_loss: 0.8041\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7744 - val_loss: 0.8122\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7697 - val_loss: 0.7940\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7618 - val_loss: 0.7860\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7542 - val_loss: 0.7849\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7493 - val_loss: 0.7812\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.7446 - val_loss: 0.7728\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7407 - val_loss: 0.7684\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7338 - val_loss: 0.7672\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7298 - val_loss: 0.7793\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7266 - val_loss: 0.7742\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7224 - val_loss: 0.7540\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7194 - val_loss: 0.7663\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7148 - val_loss: 0.7520\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7106 - val_loss: 0.7530\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.7077 - val_loss: 0.7556\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7061 - val_loss: 0.7625\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7030 - val_loss: 0.7528\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7009 - val_loss: 0.7441\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6984 - val_loss: 0.7430\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6967 - val_loss: 0.7490\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6929 - val_loss: 0.7582\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6910 - val_loss: 0.7440\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6912 - val_loss: 0.7439\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6864 - val_loss: 0.7595\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6878 - val_loss: 0.7417\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6861 - val_loss: 0.7428\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6839 - val_loss: 0.7613\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6807 - val_loss: 0.7522\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6822 - val_loss: 0.7363\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6783 - val_loss: 0.7352\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6804 - val_loss: 0.7616\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6788 - val_loss: 0.7339\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6778 - val_loss: 0.7422\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6752 - val_loss: 0.7346\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6755 - val_loss: 0.7360\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6724 - val_loss: 0.7343\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6738 - val_loss: 0.7337\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6713 - val_loss: 0.7316\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6711 - val_loss: 0.7454\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6693 - val_loss: 0.7498\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6685 - val_loss: 0.7321\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6677 - val_loss: 0.7314\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6676 - val_loss: 0.7395\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6691 - val_loss: 0.7310\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6667 - val_loss: 0.7284\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6669 - val_loss: 0.7466\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6651 - val_loss: 0.7385\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6645 - val_loss: 0.7397\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6632 - val_loss: 0.7327\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6638 - val_loss: 0.7346\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6620 - val_loss: 0.7328\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6626 - val_loss: 0.7272\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6624 - val_loss: 0.7296\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6614 - val_loss: 0.7281\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6624 - val_loss: 0.7436\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6612 - val_loss: 0.7944\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6604 - val_loss: 0.7292\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6582 - val_loss: 0.7261\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6603 - val_loss: 0.7291\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6598 - val_loss: 0.7259\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6564 - val_loss: 0.7273\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6580 - val_loss: 0.7377\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6588 - val_loss: 0.7267\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6590 - val_loss: 0.7288\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6574 - val_loss: 0.7272\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6569 - val_loss: 0.7528\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6553 - val_loss: 0.7216\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6550 - val_loss: 0.7260\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6564 - val_loss: 0.7263\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6550 - val_loss: 0.7219\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6556 - val_loss: 0.7277\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6539 - val_loss: 0.7389\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6529 - val_loss: 0.7500\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6524 - val_loss: 0.7182\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6524 - val_loss: 0.7312\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6550 - val_loss: 0.7203\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6529 - val_loss: 0.7227\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6523 - val_loss: 0.7309\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6545 - val_loss: 0.7187\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6516 - val_loss: 0.7330\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6519 - val_loss: 0.7281\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6514 - val_loss: 0.7262\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6506 - val_loss: 0.7243\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6523 - val_loss: 0.7311\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6515 - val_loss: 0.7252\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6526 - val_loss: 0.7205\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6511 - val_loss: 0.7228\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6498 - val_loss: 0.7341\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6529 - val_loss: 0.7554\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6505 - val_loss: 0.7191\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6490 - val_loss: 0.7165\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6486 - val_loss: 0.7211\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6510 - val_loss: 0.7639\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6486 - val_loss: 0.7227\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6512 - val_loss: 0.7189\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6512 - val_loss: 0.7228\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6480 - val_loss: 0.7265\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6486 - val_loss: 0.7231\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6487 - val_loss: 0.7253\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6489 - val_loss: 0.7162\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6473 - val_loss: 0.7344\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6454 - val_loss: 0.7214\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6480 - val_loss: 0.7241\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6465 - val_loss: 0.7270\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6449 - val_loss: 0.7206\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6468 - val_loss: 0.7163\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6459 - val_loss: 0.7172\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6457 - val_loss: 0.7203\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6459 - val_loss: 0.7405\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6461 - val_loss: 0.7177\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6475 - val_loss: 0.7233\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6455 - val_loss: 0.7240\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6467 - val_loss: 0.7212\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6462 - val_loss: 0.7311\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6431 - val_loss: 0.7241\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6454 - val_loss: 0.7230\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6450 - val_loss: 0.7221\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6447 - val_loss: 0.7178\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6445 - val_loss: 0.7188\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6450 - val_loss: 0.7181\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6414 - val_loss: 0.7200\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6458 - val_loss: 0.7163\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6433 - val_loss: 0.7186\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6426 - val_loss: 0.7187\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6433 - val_loss: 0.7627\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6422 - val_loss: 0.7183\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6442 - val_loss: 0.7142\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6422 - val_loss: 0.7179\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6449 - val_loss: 0.7242\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6419 - val_loss: 0.7166\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6429 - val_loss: 0.7169\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6430 - val_loss: 0.7145\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6420 - val_loss: 0.7323\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6421 - val_loss: 0.7249\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6439 - val_loss: 0.7184\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6411 - val_loss: 0.7194\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6446 - val_loss: 0.7154\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6422 - val_loss: 0.7214\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6425 - val_loss: 0.7227\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6436 - val_loss: 0.7203\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6414 - val_loss: 0.7255\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6426 - val_loss: 0.7337\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6410 - val_loss: 0.7257\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6408 - val_loss: 0.7174\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 284us/step - loss: 20.1978 - val_loss: 7.6535\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 1.9330 - val_loss: 1.0316\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.9175 - val_loss: 0.8456\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.8101 - val_loss: 0.7994\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7757 - val_loss: 0.7804\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7606 - val_loss: 0.7688\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7583 - val_loss: 0.7695\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7543 - val_loss: 0.7700\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7549 - val_loss: 0.7682\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7477 - val_loss: 0.7662\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7479 - val_loss: 0.7628\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7486 - val_loss: 0.7630\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7483 - val_loss: 0.7843\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7496 - val_loss: 0.7879\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7460 - val_loss: 0.7664\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7443 - val_loss: 0.7656\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7414 - val_loss: 0.7602\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7429 - val_loss: 0.7669\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7420 - val_loss: 0.7761\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7350 - val_loss: 0.7704\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7360 - val_loss: 0.7776\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7384 - val_loss: 0.7580\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7328 - val_loss: 0.7688\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7313 - val_loss: 0.7604\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7308 - val_loss: 0.7720\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7286 - val_loss: 0.7564\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7277 - val_loss: 0.7557\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7248 - val_loss: 0.7889\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7295 - val_loss: 0.7533\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7257 - val_loss: 0.7588\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7228 - val_loss: 0.7500\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7198 - val_loss: 0.7595\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7262 - val_loss: 0.7666\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7215 - val_loss: 0.7573\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7129 - val_loss: 0.7498\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7163 - val_loss: 0.7630\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7155 - val_loss: 0.7479\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7099 - val_loss: 0.7518\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7081 - val_loss: 0.7442\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7093 - val_loss: 0.7621\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7064 - val_loss: 0.7510\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7051 - val_loss: 0.7414\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7074 - val_loss: 0.7471\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7043 - val_loss: 0.7611\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7013 - val_loss: 0.7413\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7010 - val_loss: 0.7511\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6988 - val_loss: 0.7409\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6994 - val_loss: 0.7385\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6979 - val_loss: 0.7470\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6972 - val_loss: 0.7383\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6944 - val_loss: 0.7422\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6926 - val_loss: 0.7415\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6932 - val_loss: 0.7417\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6879 - val_loss: 0.7521\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6915 - val_loss: 0.7560\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6871 - val_loss: 0.7361\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6875 - val_loss: 0.7575\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6853 - val_loss: 0.7618\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6837 - val_loss: 0.7367\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6852 - val_loss: 0.7383\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6870 - val_loss: 0.7390\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6798 - val_loss: 0.7354\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6825 - val_loss: 0.7390\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6797 - val_loss: 0.7358\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6816 - val_loss: 0.7442\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6762 - val_loss: 0.7473\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6784 - val_loss: 0.7470\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6776 - val_loss: 0.7386\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6762 - val_loss: 0.7354\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6749 - val_loss: 0.7311\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6736 - val_loss: 0.7365\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6769 - val_loss: 0.7370\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6715 - val_loss: 0.7330\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6724 - val_loss: 0.7334\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6696 - val_loss: 0.7441\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6714 - val_loss: 0.7325\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6670 - val_loss: 0.7370\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6669 - val_loss: 0.7315\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6711 - val_loss: 0.7544\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6685 - val_loss: 0.7520\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6725 - val_loss: 0.7314\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6685 - val_loss: 0.7406\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6661 - val_loss: 0.7467\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6669 - val_loss: 0.7338\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6641 - val_loss: 0.7316\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6617 - val_loss: 0.7322\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6644 - val_loss: 0.7303\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6633 - val_loss: 0.7275\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6636 - val_loss: 0.7463\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6639 - val_loss: 0.7340\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6602 - val_loss: 0.7338\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6570 - val_loss: 0.7399\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6587 - val_loss: 0.7267\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6602 - val_loss: 0.7390\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6584 - val_loss: 0.7528\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6543 - val_loss: 0.7265\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6602 - val_loss: 0.7376\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6582 - val_loss: 0.7345\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6565 - val_loss: 0.7256\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6555 - val_loss: 0.7224\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6521 - val_loss: 0.7281\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6556 - val_loss: 0.7399\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6553 - val_loss: 0.7186\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6569 - val_loss: 0.7302\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6563 - val_loss: 0.7303\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6528 - val_loss: 0.7282\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6528 - val_loss: 0.7248\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6522 - val_loss: 0.7329\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6512 - val_loss: 0.7318\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6557 - val_loss: 0.7255\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6497 - val_loss: 0.7231\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6500 - val_loss: 0.7244\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6509 - val_loss: 0.7334\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6483 - val_loss: 0.7275\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6476 - val_loss: 0.7408\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6513 - val_loss: 0.7380\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6497 - val_loss: 0.7232\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6450 - val_loss: 0.7208\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6488 - val_loss: 0.7283\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6463 - val_loss: 0.7249\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6443 - val_loss: 0.7182\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6485 - val_loss: 0.7325\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6463 - val_loss: 0.7222\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6475 - val_loss: 0.7211\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6437 - val_loss: 0.7202\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6402 - val_loss: 0.7280\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6434 - val_loss: 0.7440\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6448 - val_loss: 0.7301\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6413 - val_loss: 0.7165\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6425 - val_loss: 0.7253\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6412 - val_loss: 0.7200\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6432 - val_loss: 0.7200\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6426 - val_loss: 0.7203\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6407 - val_loss: 0.7167\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6390 - val_loss: 0.7170\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6408 - val_loss: 0.7212\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6443 - val_loss: 0.7225\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6417 - val_loss: 0.7173\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6403 - val_loss: 0.7132\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6385 - val_loss: 0.7189\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6399 - val_loss: 0.7356\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6419 - val_loss: 0.7173\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6384 - val_loss: 0.7218\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6376 - val_loss: 0.7248\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6390 - val_loss: 0.7145\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6382 - val_loss: 0.7256\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6409 - val_loss: 0.7195\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6391 - val_loss: 0.7153\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6369 - val_loss: 0.7140\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6359 - val_loss: 0.7163\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 288us/step - loss: 20.8391 - val_loss: 10.2801\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 2.6515 - val_loss: 1.0911\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.9787 - val_loss: 0.8604\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.8418 - val_loss: 0.8170\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7892 - val_loss: 0.7620\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7678 - val_loss: 0.7471\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7578 - val_loss: 0.7408\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7552 - val_loss: 0.7383\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7506 - val_loss: 0.7450\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7512 - val_loss: 0.7402\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7526 - val_loss: 0.7516\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7497 - val_loss: 0.7420\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7485 - val_loss: 0.7419\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7518 - val_loss: 0.7415\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7444 - val_loss: 0.7430\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7457 - val_loss: 0.7537\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7447 - val_loss: 0.7434\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7446 - val_loss: 0.7522\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7460 - val_loss: 0.7689\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7475 - val_loss: 0.7356\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7448 - val_loss: 0.7667\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7465 - val_loss: 0.7736\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7427 - val_loss: 0.7354\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7404 - val_loss: 0.7329\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7423 - val_loss: 0.7403\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7424 - val_loss: 0.7341\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7391 - val_loss: 0.7366\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7402 - val_loss: 0.7340\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7381 - val_loss: 0.7392\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7371 - val_loss: 0.7284\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7385 - val_loss: 0.7314\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7352 - val_loss: 0.7324\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7359 - val_loss: 0.7347\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7353 - val_loss: 0.7269\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7319 - val_loss: 0.7335\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7345 - val_loss: 0.7286\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7341 - val_loss: 0.7470\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7323 - val_loss: 0.7243\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7303 - val_loss: 0.7268\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7299 - val_loss: 0.7666\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7303 - val_loss: 0.7295\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7298 - val_loss: 0.7325\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7315 - val_loss: 0.7211\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7324 - val_loss: 0.7180\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7259 - val_loss: 0.7182\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7264 - val_loss: 0.7166\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7253 - val_loss: 0.7163\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7240 - val_loss: 0.7405\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7235 - val_loss: 0.7146\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7270 - val_loss: 0.7161\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7212 - val_loss: 0.7363\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7251 - val_loss: 0.7185\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7214 - val_loss: 0.7181\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7178 - val_loss: 0.7161\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7198 - val_loss: 0.7181\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7186 - val_loss: 0.7073\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7199 - val_loss: 0.7181\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7182 - val_loss: 0.7071\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7188 - val_loss: 0.7387\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7152 - val_loss: 0.7148\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7201 - val_loss: 0.7134\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7188 - val_loss: 0.7153\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7133 - val_loss: 0.7118\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7164 - val_loss: 0.7068\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7160 - val_loss: 0.7175\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7134 - val_loss: 0.7065\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7092 - val_loss: 0.7089\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7103 - val_loss: 0.7108\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7102 - val_loss: 0.7271\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7086 - val_loss: 0.7054\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7079 - val_loss: 0.7060\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7078 - val_loss: 0.6974\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7072 - val_loss: 0.7003\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7058 - val_loss: 0.7282\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7053 - val_loss: 0.7070\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7014 - val_loss: 0.7167\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7049 - val_loss: 0.6984\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7004 - val_loss: 0.7017\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6979 - val_loss: 0.7078\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6999 - val_loss: 0.6988\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6974 - val_loss: 0.7022\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6989 - val_loss: 0.6902\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6997 - val_loss: 0.6978\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6960 - val_loss: 0.7014\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6948 - val_loss: 0.6996\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6935 - val_loss: 0.6950\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6933 - val_loss: 0.6895\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6945 - val_loss: 0.6974\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6886 - val_loss: 0.6926\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6885 - val_loss: 0.6929\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6907 - val_loss: 0.6989\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6896 - val_loss: 0.6868\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6877 - val_loss: 0.7061\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6903 - val_loss: 0.7015\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6876 - val_loss: 0.6929\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6877 - val_loss: 0.6886\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6843 - val_loss: 0.7086\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6821 - val_loss: 0.6908\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6862 - val_loss: 0.6903\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6824 - val_loss: 0.6893\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6813 - val_loss: 0.6885\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6813 - val_loss: 0.6915\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6810 - val_loss: 0.7046\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6790 - val_loss: 0.6850\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6820 - val_loss: 0.7101\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6769 - val_loss: 0.6907\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6791 - val_loss: 0.6809\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6765 - val_loss: 0.6804\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6782 - val_loss: 0.6878\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6733 - val_loss: 0.6899\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6761 - val_loss: 0.6983\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6732 - val_loss: 0.7035\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6756 - val_loss: 0.6908\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6763 - val_loss: 0.6822\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6714 - val_loss: 0.6931\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6744 - val_loss: 0.6999\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6729 - val_loss: 0.7108\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6782 - val_loss: 0.6892\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6695 - val_loss: 0.6889\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6719 - val_loss: 0.7074\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6693 - val_loss: 0.6889\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6709 - val_loss: 0.7016\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6725 - val_loss: 0.6852\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6639 - val_loss: 0.6896\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6674 - val_loss: 0.6931\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6660 - val_loss: 0.6968\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6723 - val_loss: 0.6897\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6651 - val_loss: 0.6834\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6677 - val_loss: 0.6890\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6686 - val_loss: 0.6920\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6627 - val_loss: 0.6853\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6643 - val_loss: 0.6879\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6639 - val_loss: 0.6779\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6642 - val_loss: 0.6891\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6624 - val_loss: 0.6849\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6622 - val_loss: 0.6852\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6620 - val_loss: 0.6810\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6628 - val_loss: 0.6930\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.6645 - val_loss: 0.6811\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6598 - val_loss: 0.7125\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6660 - val_loss: 0.6817\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6630 - val_loss: 0.6832\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6620 - val_loss: 0.6752\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6599 - val_loss: 0.6869\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6588 - val_loss: 0.6914\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6587 - val_loss: 0.6857\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6615 - val_loss: 0.6828\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6555 - val_loss: 0.6857\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6579 - val_loss: 0.6845\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6571 - val_loss: 0.6832\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 287us/step - loss: 20.3716 - val_loss: 8.4471\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 2.1711 - val_loss: 0.9850\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.9229 - val_loss: 0.8421\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.8185 - val_loss: 0.7982\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7787 - val_loss: 0.7718\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7654 - val_loss: 0.7540\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 180us/step - loss: 0.7542 - val_loss: 0.7724\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.7548 - val_loss: 0.7583\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.7503 - val_loss: 0.7621\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7507 - val_loss: 0.7529\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7489 - val_loss: 0.7577\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7462 - val_loss: 0.7540\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7450 - val_loss: 0.7859\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7444 - val_loss: 0.7695\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7464 - val_loss: 0.7593\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 182us/step - loss: 0.7427 - val_loss: 0.7531\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7409 - val_loss: 0.7748\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.7400 - val_loss: 0.7786\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7346 - val_loss: 0.7758\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7369 - val_loss: 0.7552\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7335 - val_loss: 0.7505\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7325 - val_loss: 0.7607\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7287 - val_loss: 0.7679\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7285 - val_loss: 0.7489\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7248 - val_loss: 0.7462\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7234 - val_loss: 0.7397\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7290 - val_loss: 0.7436\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7211 - val_loss: 0.7444\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7178 - val_loss: 0.7478\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7179 - val_loss: 0.7421\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7165 - val_loss: 0.7435\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7162 - val_loss: 0.7338\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7091 - val_loss: 0.7515\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 180us/step - loss: 0.7097 - val_loss: 0.7356\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.7074 - val_loss: 0.7310\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7044 - val_loss: 0.7434\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7105 - val_loss: 0.7279\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.7002 - val_loss: 0.7420\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6979 - val_loss: 0.7326\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6965 - val_loss: 0.7302\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6954 - val_loss: 0.7349\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6933 - val_loss: 0.7315\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6912 - val_loss: 0.7349\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6890 - val_loss: 0.7266\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6895 - val_loss: 0.7420\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6862 - val_loss: 0.7219\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6836 - val_loss: 0.7243\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6870 - val_loss: 0.7449\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6832 - val_loss: 0.7255\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6822 - val_loss: 0.7331\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6773 - val_loss: 0.7206\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6801 - val_loss: 0.7170\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6734 - val_loss: 0.7169\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6734 - val_loss: 0.7198\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6740 - val_loss: 0.7220\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6706 - val_loss: 0.7305\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6723 - val_loss: 0.7128\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.6689 - val_loss: 0.7181\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6684 - val_loss: 0.7190\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6654 - val_loss: 0.7248\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6656 - val_loss: 0.7168\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6673 - val_loss: 0.7154\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6614 - val_loss: 0.7160\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 179us/step - loss: 0.6613 - val_loss: 0.7250\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6604 - val_loss: 0.7230\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6609 - val_loss: 0.7206\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6587 - val_loss: 0.7145\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6537 - val_loss: 0.7346\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6593 - val_loss: 0.7172\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6574 - val_loss: 0.7121\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6580 - val_loss: 0.7161\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6542 - val_loss: 0.7212\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6556 - val_loss: 0.7153\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6548 - val_loss: 0.7128\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6507 - val_loss: 0.7123\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6530 - val_loss: 0.7164\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6542 - val_loss: 0.7235\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6488 - val_loss: 0.7190\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6558 - val_loss: 0.7284\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6507 - val_loss: 0.7113\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6455 - val_loss: 0.7141\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6453 - val_loss: 0.7310\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6473 - val_loss: 0.7283\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6427 - val_loss: 0.7164\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6414 - val_loss: 0.7194\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6441 - val_loss: 0.7108\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 180us/step - loss: 0.6443 - val_loss: 0.7172\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6426 - val_loss: 0.7163\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6446 - val_loss: 0.7121\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6384 - val_loss: 0.7381\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6408 - val_loss: 0.7158\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6401 - val_loss: 0.7091\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6382 - val_loss: 0.7131\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6416 - val_loss: 0.7099\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6379 - val_loss: 0.7132\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6363 - val_loss: 0.7111\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6375 - val_loss: 0.7431\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.6357 - val_loss: 0.7413\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6373 - val_loss: 0.7155\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6348 - val_loss: 0.7145\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6361 - val_loss: 0.7121\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6322 - val_loss: 0.7107\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6335 - val_loss: 0.7088\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6360 - val_loss: 0.7136\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.6309 - val_loss: 0.7085\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6360 - val_loss: 0.7049\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6332 - val_loss: 0.7100\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6314 - val_loss: 0.7117\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6332 - val_loss: 0.7139\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6324 - val_loss: 0.7068\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6317 - val_loss: 0.7073\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6302 - val_loss: 0.7130\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6285 - val_loss: 0.7099\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6294 - val_loss: 0.7082\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6263 - val_loss: 0.7246\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6259 - val_loss: 0.7115\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6261 - val_loss: 0.7095\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6281 - val_loss: 0.7062\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6284 - val_loss: 0.7073\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6263 - val_loss: 0.7076\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6288 - val_loss: 0.7060\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6253 - val_loss: 0.7193\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6255 - val_loss: 0.7546\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6269 - val_loss: 0.7049\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6292 - val_loss: 0.7559\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6263 - val_loss: 0.7105\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6259 - val_loss: 0.7052\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6267 - val_loss: 0.7082\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6226 - val_loss: 0.7072\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6220 - val_loss: 0.7067\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6226 - val_loss: 0.7533\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6240 - val_loss: 0.6984\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6264 - val_loss: 0.7044\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6207 - val_loss: 0.7168\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6223 - val_loss: 0.7076\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6214 - val_loss: 0.7009\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6215 - val_loss: 0.7120\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6185 - val_loss: 0.7069\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.6215 - val_loss: 0.7058\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 180us/step - loss: 0.6192 - val_loss: 0.7024\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6200 - val_loss: 0.7302\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6186 - val_loss: 0.7236\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6164 - val_loss: 0.7080\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6128 - val_loss: 0.7002\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6179 - val_loss: 0.7068\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6192 - val_loss: 0.7283\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6155 - val_loss: 0.7054\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6149 - val_loss: 0.7521\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6183 - val_loss: 0.7074\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6162 - val_loss: 0.7101\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 286us/step - loss: 20.2972 - val_loss: 9.0442\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 2.2892 - val_loss: 1.0864\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.9449 - val_loss: 0.8621\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.8217 - val_loss: 0.7933\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7777 - val_loss: 0.7776\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7636 - val_loss: 0.7777\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7567 - val_loss: 0.7543\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7511 - val_loss: 0.7828\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7544 - val_loss: 0.7575\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7458 - val_loss: 0.7721\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7461 - val_loss: 0.7510\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7443 - val_loss: 0.7533\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7466 - val_loss: 0.7673\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7432 - val_loss: 0.7542\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7419 - val_loss: 0.7614\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7372 - val_loss: 0.7518\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7369 - val_loss: 0.7534\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7344 - val_loss: 0.7595\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7343 - val_loss: 0.7470\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7306 - val_loss: 0.7435\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7273 - val_loss: 0.7463\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7291 - val_loss: 0.7412\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7283 - val_loss: 0.7385\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7204 - val_loss: 0.7466\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7185 - val_loss: 0.7384\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7184 - val_loss: 0.7243\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7128 - val_loss: 0.7318\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7145 - val_loss: 0.7329\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7101 - val_loss: 0.7239\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7077 - val_loss: 0.7324\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7021 - val_loss: 0.7215\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7031 - val_loss: 0.7348\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7001 - val_loss: 0.7358\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6986 - val_loss: 0.7213\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6926 - val_loss: 0.7323\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6929 - val_loss: 0.7253\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6879 - val_loss: 0.7243\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6913 - val_loss: 0.7198\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6854 - val_loss: 0.7075\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6886 - val_loss: 0.7153\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6852 - val_loss: 0.7203\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6821 - val_loss: 0.7265\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6784 - val_loss: 0.7071\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6769 - val_loss: 0.7183\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6739 - val_loss: 0.7116\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6766 - val_loss: 0.7136\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6739 - val_loss: 0.7097\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6705 - val_loss: 0.7071\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6733 - val_loss: 0.7021\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6707 - val_loss: 0.7132\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6690 - val_loss: 0.7019\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6671 - val_loss: 0.7070\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6643 - val_loss: 0.7027\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6621 - val_loss: 0.7019\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6618 - val_loss: 0.6894\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6592 - val_loss: 0.6968\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6541 - val_loss: 0.7083\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6592 - val_loss: 0.6966\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6577 - val_loss: 0.6980\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6560 - val_loss: 0.6969\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6533 - val_loss: 0.6917\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6528 - val_loss: 0.6882\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6555 - val_loss: 0.6873\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6522 - val_loss: 0.6920\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6522 - val_loss: 0.6844\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6501 - val_loss: 0.6902\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6469 - val_loss: 0.7034\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6468 - val_loss: 0.6887\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6499 - val_loss: 0.6886\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6435 - val_loss: 0.6887\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6423 - val_loss: 0.6882\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6425 - val_loss: 0.6900\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6442 - val_loss: 0.6767\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6401 - val_loss: 0.6894\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6425 - val_loss: 0.6829\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6425 - val_loss: 0.6821\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6437 - val_loss: 0.6846\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6379 - val_loss: 0.6888\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6396 - val_loss: 0.6814\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6360 - val_loss: 0.6765\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6393 - val_loss: 0.6827\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6342 - val_loss: 0.6857\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6308 - val_loss: 0.6807\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6342 - val_loss: 0.6827\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6317 - val_loss: 0.6992\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6383 - val_loss: 0.6794\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6302 - val_loss: 0.7101\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6323 - val_loss: 0.6834\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6302 - val_loss: 0.6825\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6250 - val_loss: 0.6827\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6269 - val_loss: 0.6985\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6270 - val_loss: 0.6847\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6286 - val_loss: 0.6756\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6237 - val_loss: 0.6934\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6256 - val_loss: 0.6788\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6234 - val_loss: 0.6793\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6207 - val_loss: 0.6812\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6231 - val_loss: 0.6937\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6203 - val_loss: 0.6807\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6213 - val_loss: 0.6783\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6214 - val_loss: 0.6737\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6219 - val_loss: 0.6979\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6212 - val_loss: 0.6718\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6184 - val_loss: 0.6801\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6200 - val_loss: 0.6831\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6192 - val_loss: 0.6795\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6152 - val_loss: 0.6793\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6164 - val_loss: 0.6814\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6160 - val_loss: 0.6735\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6125 - val_loss: 0.6780\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6146 - val_loss: 0.6769\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6126 - val_loss: 0.6717\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6104 - val_loss: 0.6944\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6143 - val_loss: 0.6698\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6136 - val_loss: 0.6717\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6101 - val_loss: 0.6691\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6088 - val_loss: 0.6743\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6133 - val_loss: 0.6667\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6108 - val_loss: 0.6754\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6087 - val_loss: 0.6683\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6110 - val_loss: 0.6779\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6092 - val_loss: 0.6784\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6103 - val_loss: 0.6797\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6052 - val_loss: 0.6719\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6082 - val_loss: 0.6715\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6076 - val_loss: 0.6702\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6067 - val_loss: 0.6679\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6057 - val_loss: 0.6683\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6022 - val_loss: 0.6654\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6017 - val_loss: 0.6697\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6017 - val_loss: 0.6780\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.5997 - val_loss: 0.6750\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6001 - val_loss: 0.6668\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6007 - val_loss: 0.6867\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6007 - val_loss: 0.6761\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6008 - val_loss: 0.6752\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.5981 - val_loss: 0.6752\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6022 - val_loss: 0.6798\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.5973 - val_loss: 0.6668\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6005 - val_loss: 0.6682\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.5989 - val_loss: 0.6810\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.5949 - val_loss: 0.6735\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.5957 - val_loss: 0.6687\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.5967 - val_loss: 0.6813\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.5936 - val_loss: 0.6999\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.5948 - val_loss: 0.6744\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.5934 - val_loss: 0.6837\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.5923 - val_loss: 0.6657\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.5941 - val_loss: 0.6727\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.5947 - val_loss: 0.6739\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 200us/step - loss: 6.2146 - val_loss: 0.8090\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7684 - val_loss: 0.7678\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7534 - val_loss: 0.7567\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7487 - val_loss: 0.7516\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7441 - val_loss: 0.7474\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7358 - val_loss: 0.7370\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7290 - val_loss: 0.7320\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7198 - val_loss: 0.7295\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7124 - val_loss: 0.7217\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7052 - val_loss: 0.7257\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7003 - val_loss: 0.7207\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6921 - val_loss: 0.7156\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6902 - val_loss: 0.7010\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6848 - val_loss: 0.6982\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6779 - val_loss: 0.6907\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6753 - val_loss: 0.6904\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6708 - val_loss: 0.7085\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6654 - val_loss: 0.7163\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 179us/step - loss: 0.6649 - val_loss: 0.6824\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6608 - val_loss: 0.6825\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6582 - val_loss: 0.6797\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6574 - val_loss: 0.6780\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6536 - val_loss: 0.6773\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6525 - val_loss: 0.6678\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6485 - val_loss: 0.6738\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 3s 203us/step - loss: 0.6476 - val_loss: 0.6720\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6436 - val_loss: 0.6672\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6424 - val_loss: 0.6968\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6430 - val_loss: 0.6709\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6416 - val_loss: 0.6701\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6393 - val_loss: 0.6598\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6376 - val_loss: 0.6628\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6350 - val_loss: 0.6639\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6361 - val_loss: 0.6676\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6347 - val_loss: 0.6618\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6346 - val_loss: 0.7077\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6338 - val_loss: 0.6663\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6311 - val_loss: 0.6573\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6301 - val_loss: 0.6639\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6278 - val_loss: 0.6720\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6282 - val_loss: 0.6621\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6282 - val_loss: 0.6608\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6267 - val_loss: 0.6553\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6274 - val_loss: 0.6893\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6245 - val_loss: 0.6717\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6256 - val_loss: 0.6690\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6246 - val_loss: 0.6602\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6230 - val_loss: 0.6559\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6212 - val_loss: 0.6608\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6218 - val_loss: 0.6556\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6208 - val_loss: 0.6556\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6206 - val_loss: 0.6522\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6210 - val_loss: 0.6623\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6203 - val_loss: 0.6655\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6189 - val_loss: 0.6568\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6202 - val_loss: 0.6554\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6175 - val_loss: 0.6542\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6196 - val_loss: 0.6703\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6169 - val_loss: 0.6599\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6149 - val_loss: 0.6589\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6162 - val_loss: 0.6693\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6139 - val_loss: 0.6692\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6143 - val_loss: 0.6534\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6159 - val_loss: 0.6765\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6148 - val_loss: 0.6653\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6155 - val_loss: 0.6531\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6141 - val_loss: 0.6568\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6136 - val_loss: 0.6490\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6131 - val_loss: 0.6485\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6147 - val_loss: 0.6567\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6119 - val_loss: 0.6573\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6107 - val_loss: 0.6572\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6128 - val_loss: 0.6509\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6129 - val_loss: 0.6541\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6092 - val_loss: 0.6503\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6104 - val_loss: 0.6528\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6106 - val_loss: 0.6493\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6118 - val_loss: 0.6581\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6106 - val_loss: 0.6472\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6101 - val_loss: 0.6470\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6076 - val_loss: 0.6743\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6103 - val_loss: 0.6687\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6102 - val_loss: 0.6750\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6086 - val_loss: 0.6500\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6083 - val_loss: 0.6517\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6068 - val_loss: 0.6574\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6070 - val_loss: 0.6609\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6089 - val_loss: 0.6623\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6079 - val_loss: 0.6536\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6082 - val_loss: 0.6515\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6067 - val_loss: 0.6675\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6062 - val_loss: 0.6517\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6080 - val_loss: 0.6451\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6069 - val_loss: 0.6456\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6056 - val_loss: 0.6589\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6063 - val_loss: 0.6513\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6075 - val_loss: 0.6454\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6051 - val_loss: 0.6532\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6046 - val_loss: 0.6475\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6062 - val_loss: 0.6588\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6067 - val_loss: 0.6495\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6057 - val_loss: 0.6633\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6043 - val_loss: 0.6482\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6051 - val_loss: 0.6557\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6062 - val_loss: 0.6559\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6051 - val_loss: 0.6611\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6062 - val_loss: 0.6532\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6029 - val_loss: 0.6542\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6026 - val_loss: 0.6515\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6031 - val_loss: 0.6491\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6042 - val_loss: 0.6496\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6030 - val_loss: 0.6534\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6036 - val_loss: 0.6490\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6041 - val_loss: 0.6557\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6039 - val_loss: 0.6464\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6041 - val_loss: 0.6462\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6024 - val_loss: 0.6486\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6025 - val_loss: 0.6486\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6034 - val_loss: 0.6490\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6027 - val_loss: 0.6502\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6026 - val_loss: 0.6489\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6035 - val_loss: 0.6493\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6028 - val_loss: 0.6686\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6039 - val_loss: 0.6512\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6032 - val_loss: 0.6448\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6031 - val_loss: 0.6559\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6029 - val_loss: 0.6545\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6038 - val_loss: 0.6518\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6029 - val_loss: 0.6746\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6013 - val_loss: 0.6515\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6013 - val_loss: 0.6607\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6019 - val_loss: 0.6483\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6027 - val_loss: 0.6523\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6014 - val_loss: 0.6528\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6017 - val_loss: 0.6549\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6021 - val_loss: 0.6555\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6006 - val_loss: 0.6451\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5996 - val_loss: 0.6545\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6013 - val_loss: 0.6530\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6030 - val_loss: 0.6489\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.5992 - val_loss: 0.6510\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6014 - val_loss: 0.6727\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.5994 - val_loss: 0.6606\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6013 - val_loss: 0.6454\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6011 - val_loss: 0.6507\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.5992 - val_loss: 0.6440\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6010 - val_loss: 0.6566\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.5999 - val_loss: 0.6539\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.5988 - val_loss: 0.6675\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6002 - val_loss: 0.6531\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 195us/step - loss: 5.9945 - val_loss: 0.7992\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 164us/step - loss: 0.7744 - val_loss: 0.7685\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 163us/step - loss: 0.7645 - val_loss: 0.7532\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 163us/step - loss: 0.7587 - val_loss: 0.7472\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 164us/step - loss: 0.7542 - val_loss: 0.7460\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 165us/step - loss: 0.7469 - val_loss: 0.7368\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.7387 - val_loss: 0.7353\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.7351 - val_loss: 0.7229\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.7264 - val_loss: 0.7216\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.7204 - val_loss: 0.7054\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.7136 - val_loss: 0.7029\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.7102 - val_loss: 0.7017\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.7038 - val_loss: 0.6974\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6993 - val_loss: 0.6977\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6943 - val_loss: 0.6928\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6912 - val_loss: 0.6847\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6865 - val_loss: 0.6841\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6830 - val_loss: 0.6816\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6802 - val_loss: 0.6914\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6759 - val_loss: 0.6846\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6730 - val_loss: 0.6736\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6728 - val_loss: 0.6757\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6681 - val_loss: 0.6794\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6663 - val_loss: 0.6738\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6643 - val_loss: 0.6709\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6644 - val_loss: 0.6815\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6585 - val_loss: 0.6625\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6578 - val_loss: 0.6891\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6566 - val_loss: 0.6573\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6543 - val_loss: 0.6603\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6542 - val_loss: 0.6769\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6507 - val_loss: 0.6546\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6515 - val_loss: 0.6580\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6491 - val_loss: 0.6586\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6492 - val_loss: 0.6612\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6456 - val_loss: 0.6554\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6450 - val_loss: 0.6607\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6428 - val_loss: 0.6576\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6429 - val_loss: 0.6733\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6443 - val_loss: 0.7105\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6412 - val_loss: 0.6526\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6378 - val_loss: 0.6529\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6377 - val_loss: 0.6506\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6373 - val_loss: 0.6564\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6376 - val_loss: 0.6525\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6332 - val_loss: 0.6552\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6338 - val_loss: 0.6544\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6306 - val_loss: 0.6517\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6304 - val_loss: 0.6529\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6304 - val_loss: 0.6474\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6295 - val_loss: 0.6603\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6301 - val_loss: 0.6435\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6288 - val_loss: 0.6471\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6281 - val_loss: 0.6565\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6260 - val_loss: 0.6500\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6243 - val_loss: 0.6488\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6258 - val_loss: 0.6442\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6236 - val_loss: 0.6609\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6224 - val_loss: 0.6442\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6218 - val_loss: 0.6433\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6213 - val_loss: 0.6486\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6202 - val_loss: 0.6546\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6175 - val_loss: 0.6698\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6192 - val_loss: 0.6402\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6167 - val_loss: 0.6461\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6169 - val_loss: 0.6478\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6171 - val_loss: 0.6462\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6145 - val_loss: 0.6362\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6135 - val_loss: 0.6421\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6144 - val_loss: 0.6436\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6129 - val_loss: 0.6394\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6138 - val_loss: 0.6555\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6141 - val_loss: 0.6347\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6116 - val_loss: 0.6535\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6108 - val_loss: 0.6358\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6111 - val_loss: 0.6411\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6110 - val_loss: 0.6528\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6082 - val_loss: 0.6333\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6108 - val_loss: 0.6354\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6075 - val_loss: 0.6343\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6075 - val_loss: 0.6401\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6086 - val_loss: 0.6419\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6079 - val_loss: 0.6483\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6070 - val_loss: 0.6395\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6078 - val_loss: 0.6414\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6048 - val_loss: 0.6354\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6047 - val_loss: 0.6381\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6064 - val_loss: 0.6308\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6030 - val_loss: 0.6378\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6043 - val_loss: 0.6343\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6031 - val_loss: 0.6543\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6024 - val_loss: 0.6334\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6018 - val_loss: 0.6372\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6027 - val_loss: 0.6489\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6036 - val_loss: 0.6314\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6028 - val_loss: 0.6383\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6020 - val_loss: 0.6356\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6009 - val_loss: 0.6359\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6011 - val_loss: 0.6347\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6014 - val_loss: 0.6307\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6026 - val_loss: 0.6317\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6004 - val_loss: 0.6301\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6013 - val_loss: 0.6687\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5997 - val_loss: 0.6349\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6001 - val_loss: 0.6382\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5983 - val_loss: 0.6342\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5990 - val_loss: 0.6348\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5984 - val_loss: 0.6539\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.5999 - val_loss: 0.6288\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5999 - val_loss: 0.6391\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5960 - val_loss: 0.6376\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5964 - val_loss: 0.6322\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5969 - val_loss: 0.6362\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5973 - val_loss: 0.6330\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5989 - val_loss: 0.6357\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5958 - val_loss: 0.6270\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5978 - val_loss: 0.6315\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5969 - val_loss: 0.6293\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5957 - val_loss: 0.6412\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5946 - val_loss: 0.6503\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5945 - val_loss: 0.6346\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5959 - val_loss: 0.6293\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5930 - val_loss: 0.6259\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5950 - val_loss: 0.6321\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5960 - val_loss: 0.6312\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5918 - val_loss: 0.6251\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5938 - val_loss: 0.6616\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5924 - val_loss: 0.6328\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5925 - val_loss: 0.6338\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5921 - val_loss: 0.6500\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5932 - val_loss: 0.6440\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5899 - val_loss: 0.6323\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5918 - val_loss: 0.6274\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5903 - val_loss: 0.6421\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5901 - val_loss: 0.6274\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5912 - val_loss: 0.6335\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5893 - val_loss: 0.6257\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5920 - val_loss: 0.6395\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5920 - val_loss: 0.6241\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5900 - val_loss: 0.6296\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5907 - val_loss: 0.6282\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5885 - val_loss: 0.6323\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5887 - val_loss: 0.6276\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5890 - val_loss: 0.6430\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5885 - val_loss: 0.6396\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5893 - val_loss: 0.6286\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5897 - val_loss: 0.6284\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5871 - val_loss: 0.6418\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5891 - val_loss: 0.6219\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5874 - val_loss: 0.6289\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 197us/step - loss: 5.9139 - val_loss: 0.7773\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7770 - val_loss: 0.7453\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7653 - val_loss: 0.7350\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7577 - val_loss: 0.7355\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7518 - val_loss: 0.7354\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7448 - val_loss: 0.7286\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7400 - val_loss: 0.7246\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7330 - val_loss: 0.7240\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7275 - val_loss: 0.7257\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7206 - val_loss: 0.7116\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7155 - val_loss: 0.7206\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7129 - val_loss: 0.7031\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7058 - val_loss: 0.7202\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7040 - val_loss: 0.7169\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6992 - val_loss: 0.7004\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6960 - val_loss: 0.6989\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6936 - val_loss: 0.6990\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6882 - val_loss: 0.6897\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6856 - val_loss: 0.6844\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6842 - val_loss: 0.6819\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6814 - val_loss: 0.6815\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6776 - val_loss: 0.6829\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6770 - val_loss: 0.6856\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6736 - val_loss: 0.6863\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6714 - val_loss: 0.6753\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6661 - val_loss: 0.6801\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6664 - val_loss: 0.6760\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6631 - val_loss: 0.6724\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6595 - val_loss: 0.6677\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6584 - val_loss: 0.6831\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6563 - val_loss: 0.6723\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6552 - val_loss: 0.6821\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6546 - val_loss: 0.6766\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6524 - val_loss: 0.6649\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6494 - val_loss: 0.6721\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6509 - val_loss: 0.6906\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6502 - val_loss: 0.6654\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6487 - val_loss: 0.6590\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6442 - val_loss: 0.6611\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6443 - val_loss: 0.6618\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6439 - val_loss: 0.6625\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6429 - val_loss: 0.6564\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6432 - val_loss: 0.6524\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6407 - val_loss: 0.6623\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6394 - val_loss: 0.6535\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6372 - val_loss: 0.6669\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6404 - val_loss: 0.6674\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6368 - val_loss: 0.6950\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6361 - val_loss: 0.6542\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6374 - val_loss: 0.6503\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6347 - val_loss: 0.6576\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6318 - val_loss: 0.6542\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6323 - val_loss: 0.6786\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6321 - val_loss: 0.6565\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6311 - val_loss: 0.6585\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6308 - val_loss: 0.6592\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6316 - val_loss: 0.6713\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6298 - val_loss: 0.6554\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6315 - val_loss: 0.6672\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6278 - val_loss: 0.6513\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6322 - val_loss: 0.6601\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6284 - val_loss: 0.6520\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6276 - val_loss: 0.6570\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6280 - val_loss: 0.6531\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6246 - val_loss: 0.6478\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6236 - val_loss: 0.6556\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6261 - val_loss: 0.6495\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6233 - val_loss: 0.6497\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6241 - val_loss: 0.6485\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6243 - val_loss: 0.6572\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6231 - val_loss: 0.6519\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6234 - val_loss: 0.6490\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6240 - val_loss: 0.6497\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6225 - val_loss: 0.6529\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6227 - val_loss: 0.6466\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6241 - val_loss: 0.6592\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6204 - val_loss: 0.6519\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6225 - val_loss: 0.6492\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6219 - val_loss: 0.6529\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6196 - val_loss: 0.6533\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6195 - val_loss: 0.6465\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6197 - val_loss: 0.7047\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6184 - val_loss: 0.6616\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6185 - val_loss: 0.6470\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6188 - val_loss: 0.6676\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6191 - val_loss: 0.6655\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6183 - val_loss: 0.6516\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6161 - val_loss: 0.6609\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6179 - val_loss: 0.6615\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6180 - val_loss: 0.6478\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6165 - val_loss: 0.6491\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6185 - val_loss: 0.6494\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6165 - val_loss: 0.6555\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6150 - val_loss: 0.6484\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6162 - val_loss: 0.6604\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6165 - val_loss: 0.6533\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6168 - val_loss: 0.6676\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6167 - val_loss: 0.6529\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6157 - val_loss: 0.6555\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6131 - val_loss: 0.6435\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6143 - val_loss: 0.6472\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6159 - val_loss: 0.6464\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6129 - val_loss: 0.6502\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6134 - val_loss: 0.6508\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6163 - val_loss: 0.6451\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6145 - val_loss: 0.6782\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6113 - val_loss: 0.6607\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6149 - val_loss: 0.6463\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6133 - val_loss: 0.6465\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6118 - val_loss: 0.6800\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6117 - val_loss: 0.6513\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6110 - val_loss: 0.6572\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6121 - val_loss: 0.6794\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6111 - val_loss: 0.6481\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6122 - val_loss: 0.6565\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6111 - val_loss: 0.6630\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 178us/step - loss: 0.6110 - val_loss: 0.6487\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6111 - val_loss: 0.6566\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6110 - val_loss: 0.6451\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6131 - val_loss: 0.6605\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6101 - val_loss: 0.6491\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6100 - val_loss: 0.6452\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6093 - val_loss: 0.6433\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6106 - val_loss: 0.6425\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6080 - val_loss: 0.6529\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6101 - val_loss: 0.6687\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6093 - val_loss: 0.6452\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6101 - val_loss: 0.6502\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6084 - val_loss: 0.6586\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6096 - val_loss: 0.6656\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6106 - val_loss: 0.6475\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6086 - val_loss: 0.6465\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6074 - val_loss: 0.6431\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6104 - val_loss: 0.6537\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6088 - val_loss: 0.6449\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6068 - val_loss: 0.6459\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6094 - val_loss: 0.6533\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6081 - val_loss: 0.6497\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6084 - val_loss: 0.6466\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6074 - val_loss: 0.6507\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6083 - val_loss: 0.6515\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6073 - val_loss: 0.6605\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6079 - val_loss: 0.6455\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6087 - val_loss: 0.6472\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6074 - val_loss: 0.6438\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6077 - val_loss: 0.6577\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6062 - val_loss: 0.6474\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6071 - val_loss: 0.6482\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6061 - val_loss: 0.6633\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6054 - val_loss: 0.6473\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 201us/step - loss: 6.0223 - val_loss: 0.8004\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.7722 - val_loss: 0.7658\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.7585 - val_loss: 0.7533\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.7551 - val_loss: 0.7471\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.7499 - val_loss: 0.7413\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.7438 - val_loss: 0.7418\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7383 - val_loss: 0.7755\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7333 - val_loss: 0.7373\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7234 - val_loss: 0.7172\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7188 - val_loss: 0.7195\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7104 - val_loss: 0.7208\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.7062 - val_loss: 0.7015\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6992 - val_loss: 0.7199\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6932 - val_loss: 0.7104\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6868 - val_loss: 0.6911\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6839 - val_loss: 0.7256\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6798 - val_loss: 0.6802\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6771 - val_loss: 0.6926\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6705 - val_loss: 0.6764\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6666 - val_loss: 0.6758\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6649 - val_loss: 0.6785\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6582 - val_loss: 0.7072\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6579 - val_loss: 0.6632\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6548 - val_loss: 0.6674\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6527 - val_loss: 0.6661\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6502 - val_loss: 0.6645\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6487 - val_loss: 0.6956\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6456 - val_loss: 0.6649\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6446 - val_loss: 0.6749\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6439 - val_loss: 0.6631\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6422 - val_loss: 0.6617\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6392 - val_loss: 0.6634\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6398 - val_loss: 0.6568\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6400 - val_loss: 0.6690\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6369 - val_loss: 0.6571\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6347 - val_loss: 0.6566\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6329 - val_loss: 0.6570\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6341 - val_loss: 0.6747\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6340 - val_loss: 0.6675\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6319 - val_loss: 0.6620\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6312 - val_loss: 0.6558\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6282 - val_loss: 0.6652\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6248 - val_loss: 0.6556\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6285 - val_loss: 0.6534\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6289 - val_loss: 0.6565\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6261 - val_loss: 0.6660\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6263 - val_loss: 0.6491\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6244 - val_loss: 0.6512\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6235 - val_loss: 0.6537\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6233 - val_loss: 0.6531\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6231 - val_loss: 0.6506\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6244 - val_loss: 0.6504\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6210 - val_loss: 0.6597\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6219 - val_loss: 0.6581\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6207 - val_loss: 0.6537\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6201 - val_loss: 0.6553\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6189 - val_loss: 0.6492\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6184 - val_loss: 0.6497\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6171 - val_loss: 0.6600\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6186 - val_loss: 0.6666\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6163 - val_loss: 0.6466\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6177 - val_loss: 0.6508\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6179 - val_loss: 0.6512\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6172 - val_loss: 0.6493\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6169 - val_loss: 0.6739\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6156 - val_loss: 0.6602\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6167 - val_loss: 0.6510\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6157 - val_loss: 0.6474\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6152 - val_loss: 0.6509\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6159 - val_loss: 0.6487\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6145 - val_loss: 0.6583\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6129 - val_loss: 0.6723\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6142 - val_loss: 0.6675\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6116 - val_loss: 0.6502\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6123 - val_loss: 0.6471\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6107 - val_loss: 0.6442\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6128 - val_loss: 0.6440\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6137 - val_loss: 0.6451\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6095 - val_loss: 0.6526\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6109 - val_loss: 0.6474\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6106 - val_loss: 0.6453\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6117 - val_loss: 0.6494\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6097 - val_loss: 0.6495\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6105 - val_loss: 0.6502\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6108 - val_loss: 0.6501\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6107 - val_loss: 0.6466\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6085 - val_loss: 0.6507\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6083 - val_loss: 0.6543\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6109 - val_loss: 0.6470\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6098 - val_loss: 0.6429\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6085 - val_loss: 0.6430\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6102 - val_loss: 0.6436\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6087 - val_loss: 0.6530\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6099 - val_loss: 0.6678\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6101 - val_loss: 0.6450\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6058 - val_loss: 0.6428\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6085 - val_loss: 0.6509\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6079 - val_loss: 0.6545\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6070 - val_loss: 0.6559\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6081 - val_loss: 0.6834\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6062 - val_loss: 0.6541\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6063 - val_loss: 0.6519\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6056 - val_loss: 0.6472\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6068 - val_loss: 0.6451\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6055 - val_loss: 0.6434\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6038 - val_loss: 0.6561\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6038 - val_loss: 0.6617\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6060 - val_loss: 0.6520\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6052 - val_loss: 0.6423\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6040 - val_loss: 0.6490\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6046 - val_loss: 0.6421\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6077 - val_loss: 0.6498\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6058 - val_loss: 0.6659\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6059 - val_loss: 0.6440\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6035 - val_loss: 0.6580\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6050 - val_loss: 0.6502\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6043 - val_loss: 0.6467\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6037 - val_loss: 0.6470\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6036 - val_loss: 0.6476\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6033 - val_loss: 0.6509\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 178us/step - loss: 0.6054 - val_loss: 0.6455\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6029 - val_loss: 0.6509\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6020 - val_loss: 0.6467\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6029 - val_loss: 0.6647\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6023 - val_loss: 0.6416\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6027 - val_loss: 0.6545\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6025 - val_loss: 0.6393\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6022 - val_loss: 0.6487\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6022 - val_loss: 0.6489\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6023 - val_loss: 0.6414\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6014 - val_loss: 0.6448\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6018 - val_loss: 0.6836\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6020 - val_loss: 0.6595\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6025 - val_loss: 0.6543\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6020 - val_loss: 0.6600\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6024 - val_loss: 0.6496\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6000 - val_loss: 0.6419\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6014 - val_loss: 0.6423\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6014 - val_loss: 0.6468\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.5992 - val_loss: 0.6459\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.5999 - val_loss: 0.6416\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6006 - val_loss: 0.6475\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6024 - val_loss: 0.6401\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6031 - val_loss: 0.6438\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6017 - val_loss: 0.6467\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6006 - val_loss: 0.6395\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6011 - val_loss: 0.6540\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6003 - val_loss: 0.6469\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6013 - val_loss: 0.6449\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6017 - val_loss: 0.6392\n"
     ]
    }
   ],
   "source": [
    "# To save time: always apply PCA for now\n",
    "apply_pca = True\n",
    "apply_iqr = [False, True]\n",
    "n_samples = [5000, 20000] #[5000, 20000]\n",
    "n_iter = 4 #5\n",
    "\n",
    "noiqr_5000 = []\n",
    "noiqr_20000 = []\n",
    "iqr_5000 = []\n",
    "iqr_20000 = []\n",
    "\n",
    "for b in apply_iqr:\n",
    "    for n in n_samples:\n",
    "        for i in range(n_iter):\n",
    "            # load each time a different set (kind of cross-val)\n",
    "            X_train, X_test, y_train, y_test = load_data_train_test(n, X_tot, y_tot, iqr=b)\n",
    "            \n",
    "            # PCA\n",
    "            if apply_pca:\n",
    "                n_pcs = 80\n",
    "                X_train, X_test = do_PCA(X_train, X_test, n_pcs)\n",
    "                #nb_input_neurons = n\n",
    "            \n",
    "            # NN\n",
    "            k_ann = k_models.model_6(X_train, 'mean_absolute_error') \n",
    "            k_ann.fit(X_train, y_train, epochs=150, batch_size=10, validation_split=0.2)\n",
    "            \n",
    "            y_hat = k_ann.predict(X_test)\n",
    "            mse_nn, mae_nn = compute_score(y_test, y_hat)\n",
    "            \n",
    "            # Store result\n",
    "            # Case: iqr, 5000 samples\n",
    "            if (b and (n == n_samples[0])):\n",
    "                iqr_5000.append(mse_nn)\n",
    "            elif ((not b) and (n == n_samples[0])):\n",
    "                noiqr_5000.append(mse_nn)\n",
    "            elif (b and (n == n_samples[1])):\n",
    "                iqr_20000.append(mse_nn)\n",
    "            else:\n",
    "                noiqr_20000.append(mse_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stats_iqr_vs_n_2.pkl', 'wb') as f:\n",
    "    pickle.dump([noiqr_5000, noiqr_20000, iqr_5000, iqr_20000], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Figures\n",
    "<a id='figures'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Distributions\n",
    "<a id='fig_dist'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-5, 65), Text(0.5, 0, 'Shielding')]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "sns.set(font_scale=1) \n",
    "plt.subplot(2,2,1)\n",
    "plt.title('Shielding distribution \\n (w/o outlier removal)', fontsize=15)\n",
    "sns.distplot(y_tot).set(xlim=(-5,65),ylim=(0,0.14))\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('Shielding distribution \\n (after applying IQR)', fontsize=15)\n",
    "sns.distplot(y_filtered).set(xlim=(-5,65),ylim=(0,0.14))\n",
    "plt.subplot(2,2,3)\n",
    "b1 = sns.boxplot(x=y_tot, linewidth=1.5).set(xlabel='Shielding', xlim=(-5,65))\n",
    "#b1.set_xlabel(\"Shielding\",fontsize=10)\n",
    "plt.subplot(2,2,4)\n",
    "sns.boxplot(x=y_filtered, linewidth=1.5).set(xlabel='Shielding', xlim=(-5,65))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"iqr.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 IQR\n",
    "<a id='fig_iqr'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noiqr_5000</th>\n",
       "      <th>noiqr_20000</th>\n",
       "      <th>iqr_5000</th>\n",
       "      <th>iqr_20000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.966410</td>\n",
       "      <td>1.164088</td>\n",
       "      <td>0.891626</td>\n",
       "      <td>0.832680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.286497</td>\n",
       "      <td>1.257255</td>\n",
       "      <td>0.887988</td>\n",
       "      <td>0.744232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.757331</td>\n",
       "      <td>0.852787</td>\n",
       "      <td>1.002124</td>\n",
       "      <td>0.833200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.238798</td>\n",
       "      <td>1.663109</td>\n",
       "      <td>1.009964</td>\n",
       "      <td>0.722377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   noiqr_5000  noiqr_20000  iqr_5000  iqr_20000\n",
       "0    1.966410     1.164088  0.891626   0.832680\n",
       "1    1.286497     1.257255  0.887988   0.744232\n",
       "2    1.757331     0.852787  1.002124   0.833200\n",
       "3    1.238798     1.663109  1.009964   0.722377"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iqr = pd.DataFrame({\n",
    "    'noiqr_5000': noiqr_5000,\n",
    "    'noiqr_20000': noiqr_20000,\n",
    "    'iqr_5000': iqr_5000,\n",
    "    'iqr_20000': iqr_20000\n",
    "})\n",
    "df_iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>iqr</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.966410</td>\n",
       "      <td>no</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.286497</td>\n",
       "      <td>no</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.757331</td>\n",
       "      <td>no</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.238798</td>\n",
       "      <td>no</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.164088</td>\n",
       "      <td>no</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.257255</td>\n",
       "      <td>no</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.852787</td>\n",
       "      <td>no</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.663109</td>\n",
       "      <td>no</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.891626</td>\n",
       "      <td>yes</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.887988</td>\n",
       "      <td>yes</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.002124</td>\n",
       "      <td>yes</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.009964</td>\n",
       "      <td>yes</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.832680</td>\n",
       "      <td>yes</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.744232</td>\n",
       "      <td>yes</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.833200</td>\n",
       "      <td>yes</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.722377</td>\n",
       "      <td>yes</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mse  iqr      n\n",
       "0   1.966410   no   5000\n",
       "1   1.286497   no   5000\n",
       "2   1.757331   no   5000\n",
       "3   1.238798   no   5000\n",
       "4   1.164088   no  20000\n",
       "5   1.257255   no  20000\n",
       "6   0.852787   no  20000\n",
       "7   1.663109   no  20000\n",
       "8   0.891626  yes   5000\n",
       "9   0.887988  yes   5000\n",
       "10  1.002124  yes   5000\n",
       "11  1.009964  yes   5000\n",
       "12  0.832680  yes  20000\n",
       "13  0.744232  yes  20000\n",
       "14  0.833200  yes  20000\n",
       "15  0.722377  yes  20000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_iqr_n = noiqr_5000 + noiqr_20000 + iqr_5000 + iqr_20000\n",
    "\n",
    "iqrs_labels = ['no','no','no','no','no','no','no','no','yes','yes','yes','yes','yes','yes','yes','yes']\n",
    "n_labels = [5000,5000,5000,5000,20000,20000,20000,20000,5000,5000,5000,5000,20000,20000,20000,20000]\n",
    "\n",
    "my_df = pd.DataFrame({\n",
    "    'mse': tot_iqr_n,\n",
    "    'iqr': iqrs_labels,\n",
    "    'n': n_labels\n",
    "})\n",
    "my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iqr_vs_n, _ = plt.subplots(figsize=(10,8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "ax = sns.boxplot(data=my_df, x='iqr', y='mse', hue='n', linewidth=1.)\n",
    "ax.set_xticklabels(['No IQR', 'IQR'], fontsize=20)\n",
    "ax.set_xlabel('', fontsize=20)\n",
    "ax.set_ylabel('MSE', fontsize=20)\n",
    "\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='20')\n",
    "plt.setp(ax.get_legend().get_title(), fontsize='28')\n",
    "\n",
    "for patch in ax.artists:\n",
    "    r, g, b, a = patch.get_facecolor()\n",
    "    patch.set_facecolor((r, g, b, .75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_vs_n.savefig(\"iqr_vs_n_2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Feature distributions\n",
    "<a id='fig_feature_dist'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number = [12711, 1457, 4502]\n",
    "\n",
    "feature_0 = X_tot[:, feature_number[0]]\n",
    "feature_1 = X_tot[:, feature_number[1]]\n",
    "feature_2 = X_tot[:, feature_number[2]]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(14,5))\n",
    "\n",
    "# Divide the figure into a 2x1 grid, and give me the first section\n",
    "ax_1 = fig.add_subplot(131)\n",
    "\n",
    "# Divide the figure into a 2x1 grid, and give me the second section\n",
    "ax_2 = fig.add_subplot(132)\n",
    "ax_3 = fig.add_subplot(133)\n",
    "\n",
    "\n",
    "sns.distplot(feature_0, ax=ax_1, kde=False, hist_kws=dict(edgecolor=\"w\", linewidth=1))\n",
    "\n",
    "ax_1.axes.set_title(\"Feature {}\".format(str(feature_number[0]), fontsize=20))\n",
    "ax_1.set_xlabel(\"\", fontsize=17)\n",
    "ax_1.set_ylabel(\"Count\",fontsize=17)\n",
    "\n",
    "sns.distplot(feature_1, ax=ax_2, kde=False, hist_kws=dict(edgecolor=\"w\", linewidth=1))\n",
    "\n",
    "ax_2.axes.set_title(\"Feature {}\".format(str(feature_number[1]), fontsize=20))\n",
    "ax_2.set_xlabel(\"Feature value\", fontsize=17)\n",
    "ax_2.set_ylabel(\"\", fontsize=17)\n",
    "                    \n",
    "sns.distplot(feature_2, ax=ax_3, kde=False, hist_kws=dict(edgecolor=\"w\", linewidth=1))\n",
    "\n",
    "ax_3.axes.set_title(\"Feature {}\".format(str(feature_number[2]), fontsize=20))\n",
    "ax_3.set_xlabel(\"\", fontsize=17)\n",
    "ax_3.set_ylabel(\"\", fontsize=17)\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note NN\n",
    "- Simple model is the best (1 hidden of 100 neurons)\n",
    "- For now dropout does not help\n",
    "- Increasing the number of hidden neurons to 200 does not help\n",
    "- Decreasing the number of hidden neurons to 50 shows problem of cenvergence\n",
    "- Increasing the number of samples show little or no improvements (filter with **IQR** and **minmaxscaler** before and/or **PCA**)\n",
    "- Test Dropout in PyTorch (specify when training and when NOT training)!\n",
    "- Again even with batch norm, the simplest model with one hidden seems the best\n",
    "- Keras: Adam better for model 3 with 100 neurons in hidden layer (sinon does not converge with SGD). Can also try with opt='adam' (...)\n",
    "- **OK definitely best with 100 hidden neurons and adam as optimizer**\n",
    "- Now check with PCA, min_max scaling\n",
    "- Increasing the number of epochs seems a good idea\n",
    "- IQR helps (ouf)!\n",
    "- Increasing the number of samples helps also (ouf)!\n",
    "- Not sure if PCA useful with Keras models\n",
    "- Model_6 still the best at this point. Try with DEEP net (lots of 'small' layers)?\n",
    "\n",
    "> About PCA\n",
    "- Shall we normalize before?\n",
    "- How does the PCA method from sklearn work? Normalize before?\n",
    "- When trying without PCA > apply a SCALER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
