{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning project CS-433: NMR spectroscopy supervised learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedules:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Week 10 (18-24 November): \n",
    " * Tests of various linear models/simple NN on a 10% subset of data\n",
    "* Week 11 (25-1 December):\n",
    " * Feature selection: being able to come with a good set of features\n",
    "* Week 12 (2-8 December):\n",
    " * Start of big scale analysis with Spark, implementation of the models which perform well at small scale\n",
    "* Week 13 (9-15 December):\n",
    " * Wrapping up\n",
    "* Week 14 (16-22 December): \n",
    " * 19th December: Deadline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import scipy.stats\n",
    "import sklearn.metrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm\n",
    "from itertools import combinations\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write here the log of the different techniques/improvements we add to the program: **cf log/models_log.txt** for the different models already tried and their results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline graph coming soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_data_X = np.load('data/CSD-10k_H_fps_1k_MD_n_12_l_9_rc_3.0_gw_0.3_rsr_1.0_rss_2.5_rse_5.npy',mmap_mode='r')\n",
    "tot_data_y = np.load('data/CSD-10k_H_chemical_shieldings.npy',mmap_mode='r')\n",
    "DATA_LEN = tot_data_X.shape[0]\n",
    "DATA_COLS = tot_data_X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(n_samples,tot_data_x = tot_data_X,tot_data_y = tot_data_y):\n",
    "    #np.random.seed(14)\n",
    "    mask_data = np.random.permutation(DATA_LEN)[:n_samples]\n",
    "\n",
    "    data_X = tot_data_X[mask_data]\n",
    "    data_y = tot_data_y[mask_data]\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_train_test(n_samples,tot_data_x = tot_data_X,tot_data_y = tot_data_y):\n",
    "    data_X, data_y = load_data(n_samples,tot_data_x,tot_data_y)\n",
    "    X_train,X_test,y_train,y_test = train_test_split(data_X,data_y,test_size = 0.2)\n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X,data_y = load_data(3000)\n",
    "data_X_df = pd.DataFrame(data_X)\n",
    "data_y_df = pd.DataFrame(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.permutation(DATA_COLS)[:9]\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3)\n",
    "fig.set_size_inches(11,11)\n",
    "for ind,i in enumerate(mask):\n",
    "    index = np.unravel_index(ind,(3,3))\n",
    "    axes[index].ticklabel_format(style='sci',scilimits=(-3,4),axis='both')\n",
    "    data_X_df.iloc[:,i].hist(ax = axes[index],bins = 50)\n",
    "    #data_X_df.iloc[:,i].plot.box(ax = axes[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=3)\n",
    "fig.set_size_inches(11,11)\n",
    "for ind,i in enumerate(mask):\n",
    "    index = np.unravel_index(ind,(3,3))\n",
    "    axes[index].ticklabel_format(style='sci',scilimits=(-3,4),axis='both')\n",
    "    #data_X_df.iloc[:,i].hist(ax = axes[index],bins = 50)\n",
    "    data_X_df.iloc[:,i].plot.box(ax = axes[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the different features are scaled pretty differently, we might want to scale them beforehand. Since they don' look like following a gaussian, we'll apply min/max scaling: but in order to do so, we first need to get rid of the outliers thanks to one of the following methods\n",
    "* Zscore: not adapted as our data might not be gaussian\n",
    "* DBScan:\n",
    "* Isolation Forest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_outliers(meth,X_train,y_train):\n",
    "    \"\"\"\n",
    "    Drops the outliers values from the dataset\n",
    "    masks: [[int]]: each array's indexes correspond to the samples that the corresponding feature considers as outliers.\n",
    "    \"\"\"\n",
    "    masks = np.array([meth(feat) for feat in X_train.T])\n",
    "    masks = np.hstack(masks)\n",
    "   # set_trace()\n",
    "    X_train = np.delete(X_train,masks,axis = 0)\n",
    "    y_train = np.delete(y_train,masks,axis = 0)\n",
    "    return X_train,y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: computationally too demanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering = DBSCAN(eps=0.3, min_samples=2).fit(X_train)\n",
    "#with np.printoptions(threshold=np.inf):\n",
    "#    print(clustering.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interquartile range method (IQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consists in considering as outliers all data points that lie in >1.5 interquartile range from the quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IQR(ys):\n",
    "    \"\"\"\n",
    "    returns the array of indexes of the samples considered as outliers according to IQR\"\"\"\n",
    "    q1, q3 = np.percentile(ys, [5, 95])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - (iqr * 1.5)\n",
    "    upper_bound = q3 + (iqr * 1.5)\n",
    "    return np.where((ys > upper_bound) | (ys < lower_bound))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min/max Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_scaler(scaler,X_train,X_test):\n",
    "    X_train = minmx_scaler.fit_transform(X_train)\n",
    "    X_test = minmx_scaler.transform(X_test)\n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PCA(n_comp,X_train):\n",
    "    \"\"\"\n",
    "    displays the 'elbow' of the PCA, ie the screeplot\"\"\"\n",
    "    pca = PCA(n_components = n_comp)\n",
    "    pca.fit(X_train)\n",
    "    plt.figure(1)\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('Cumulative explained variance')\n",
    "    plt.show()\n",
    "    \n",
    "X_train,_,_,_ = load_data_train_test(3000)\n",
    "plot_PCA(70,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_PCA(X_train,X_test,n):\n",
    "    pca = PCA(n_components = n)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lin_model(reg, X_train,y_train,X_test,y_test):\n",
    "    \"\"\"\n",
    "    Test a model reg which is expected to be already instantiated\n",
    "    return score_train,score_test\"\"\"\n",
    "    lin.fit(X_train,y_train)\n",
    "    train_R2 = reg.score(X_train,y_train)\n",
    "    test_R2 = reg.score(X_test,y_test)\n",
    "    y_hat = reg.predict(X_test)\n",
    "    mse = mean_squared_error(y_test,y_hat)\n",
    "    mae = mean_absolute_error(y_test,y_hat)\n",
    "    print(\"Obtained score on train set %2.2f \" % train_R2)\n",
    "    print(\"Obtained score on test set %2.2f \" % test_R2)\n",
    "    print(\"Obtained MSE on test set %2.2f \" % mse)\n",
    "    print(\"Obtained MAE on test set %2.2f \" % mae)\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each cell here is meant to do a whole pipeline, from loading a certain number of samples, preprocessing etc. We keep using the R2 score as our metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_save,X_test_save,y_train_save,y_test_save = load_data_train_test(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "X_train = np.copy(X_train_save)\n",
    "X_test = np.copy(X_test_save)\n",
    "y_train = np.copy(y_train)\n",
    "y_test = np.copy(y_test_save)\n",
    "X_train,y_train = filter_outliers(IQR,X_train,y_train)\n",
    "X_train,X_test = do_PCA(X_train,X_test,40)\n",
    "minmx_scaler = MinMaxScaler()\n",
    "X_train, X_test = apply_scaler(minmx_scaler,X_train,X_test)\n",
    "lin = LinearRegression(fit_intercept = True).fit(X_train,y_train)\n",
    "train_score,test_score = test_lin_model(lin,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge regression\n",
    "X_train = np.copy(X_train_save)\n",
    "X_test = np.copy(X_test_save)\n",
    "y_train = np.copy(y_train_save)\n",
    "y_test = np.copy(y_test_save)\n",
    "do_PCA(X_train,X_test,40)\n",
    "minmx_scaler = MinMaxScaler()\n",
    "X_train, X_test = apply_scaler(minmx_scaler,X_train,X_test)\n",
    "rid = Ridge().fit(X_train,y_train)\n",
    "train_score,test_score = test_lin_model(rid,X_train,y_train,X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
