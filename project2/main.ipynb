{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning project CS-433: NMR spectroscopy supervised learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedules:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Week 10 (18-24 November): \n",
    " * Tests of various linear models/simple NN on a 10% subset of data\n",
    "* Week 11 (25-1 December):\n",
    " * Feature selection: being able to come with a good set of features\n",
    "* Week 12 (2-8 December):\n",
    " * Start of big scale analysis with Spark, implementation of the models which perform well at small scale\n",
    "* Week 13 (9-15 December):\n",
    " * Wrapping up\n",
    "* Week 14 (16-22 December): \n",
    " * 19th December: Deadline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import scipy.stats\n",
    "import sklearn.metrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write here the log of the different techniques/improvements we add to the program: **cf log/models_log.txt** for the different models already tried and their results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline graph coming soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_data_X = np.load('data/CSD-10k_H_fps_1k_MD_n_12_l_9_rc_3.0_gw_0.3_rsr_1.0_rss_2.5_rse_5.npy',mmap_mode='r')\n",
    "tot_data_y = np.load('data/CSD-10k_H_chemical_shieldings.npy',mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(n_samples,tot_data_x = tot_data_X,tot_data_y = tot_data_y):\n",
    "    \n",
    "    N_SAMPLES = 3000\n",
    "    DATA_LEN = tot_data_X.shape[0]\n",
    "    DATA_COLS = tot_data_X.shape[1]\n",
    "\n",
    "    #np.random.seed(14)\n",
    "    mask_data = np.random.permutation(DATA_LEN)[:N_SAMPLES]\n",
    "\n",
    "    data_X = tot_data_X[mask_data]\n",
    "    data_y = tot_data_y[mask_data]\n",
    "    X_train,X_test,y_train,y_test = train_test_split(data_X,data_y,test_size = 0.2)\n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X_df = pd.DataFrame(data_X)\n",
    "data_y_df = pd.DataFrame(data_y)\n",
    "mask = np.random.permutation(DATA_COLS)[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=3)\n",
    "fig.set_size_inches(11,11)\n",
    "for ind,i in enumerate(mask):\n",
    "    index = np.unravel_index(ind,(3,3))\n",
    "    axes[index].ticklabel_format(style='sci',scilimits=(-3,4),axis='both')\n",
    "    data_X_df.iloc[:,i].hist(ax = axes[index],bins = 50)\n",
    "    #data_X_df.iloc[:,i].plot.box(ax = axes[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the different features are scaled pretty differently, we might want to scale them beforehand. Since they don' look like following a gaussian, we'll apply min/max scaling: but in order to do so, we first need to get rid of the outliers thanks to one of the following methods\n",
    "* Zscore: not adapted as our data might not be gaussian\n",
    "* DBScan:\n",
    "* Isolation Forest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = DBSCAN(eps=0.3, min_samples=2).fit(X_train)\n",
    "\n",
    "with np.printoptions(threshold=np.inf):\n",
    "    print(clustering.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min/max Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_scaler(scaler,X_train,X_test):\n",
    "    X_train = minmx_scaler.fit_transform(X_train)\n",
    "    X_test = minmx_scaler.transform(X_test)\n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PCA(n_comp,X_train):\n",
    "    pca = PCA(n_components = n_comp)\n",
    "    pca.fit(X_train)\n",
    "    plt.figure(1)\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('Cumulative explained variance')\n",
    "    plt.show()\n",
    "    \n",
    "plot_PCA(70,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_PCA(X_train,X_test,n):\n",
    "    pca = PCA(n_components = n)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lin_model(reg, X_train,y_train,X_test,y_test):\n",
    "    \"\"\"\n",
    "    Test a model reg which is expected to be already instantiated\n",
    "    return score_train,score_test\"\"\"\n",
    "    lin.fit(X_train,y_train)\n",
    "    train_score = reg.score(X_train,y_train)\n",
    "    test_score = reg.score(X_test,y_test)\n",
    "    print(\"Obtained score on train set %2.2f \" % train_score)\n",
    "    print(\"Obtained score on test set %2.2f \" % test_score)\n",
    "    return train_score, test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each cell here is meant to do a whole pipeline, from loading a certain number of samples, preprocessing etc. We keep using the R2 score as our metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = load_data(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "#do_PCA(X_train,X_test,40)\n",
    "#minmx_scaler = MinMaxScaler()\n",
    "#X_train, X_test = apply_scaler(minmx_scaler,X_train,X_test)\n",
    "lin = LinearRegression(fit_intercept = True).fit(X_train,y_train)\n",
    "train_score,test_score = test_lin_model(lin,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge regression\n",
    "#do_PCA(X_train,X_test,40)\n",
    "#minmx_scaler = MinMaxScaler()\n",
    "#X_train, X_test = apply_scaler(minmx_scaler,X_train,X_test)\n",
    "rid = Ridge().fit(X_train,y_train)\n",
    "train_score,test_score = test_lin_model(rid,X_train,y_train,X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
