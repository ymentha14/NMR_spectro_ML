{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Machine Learning project CS-433: NMR spectroscopy supervised learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedules:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Week 10 (18-24 November): \n",
    " * Tests of various linear models/simple NN on a 10% subset of data\n",
    "* Week 11 (25-1 December):\n",
    " * Feature selection: being able to come with a good set of features\n",
    "* Week 12 (2-8 December):\n",
    " * Start of big scale analysis with Spark, implementation of the models which perform well at small scale\n",
    "* Week 13 (9-15 December):\n",
    " * Wrapping up\n",
    "* Week 14 (16-22 December): \n",
    " * 19th December: Deadline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Log Book](#log)\n",
    "2. [Pipeline](#pipeline)\n",
    "3. [Data Processing](#data_proc) <br>\n",
    "&emsp;3.1. [Data Vizualisation](#data_viz) <br>\n",
    "&emsp;3.2 [Outliers detection](#outliers) <br>\n",
    "  &emsp;&emsp;3.2.1 [DBSCAN](#dbscan) <br>\n",
    "  &emsp;&emsp;3.2.2 [Inter quantile range method](#iqr) <br>\n",
    "&emsp;3.3 [Scaling](#scaling) <br>\n",
    "&emsp;&emsp;3.3.1 [Min max scaling](#minmax) <br>\n",
    "&emsp;3.4 [Dimensionality reduction](#dim_red) <br>\n",
    "  &emsp;&emsp;3.4.1 [PCA](#pca) <br>\n",
    "&emsp;3.5 [Feature Selection](#feat_sel) <br>\n",
    "  &emsp;&emsp;3.5.1 [Relative importance from linear regression](#rel_imp_lin) <br>\n",
    "  &emsp;&emsp;3.5.2 [Random forest](#rand_for) <br>\n",
    "  &emsp;&emsp;3.5.3 [Univariate feature selection](#un_feat_sel) <br>\n",
    "  &emsp;&emsp;3.5.4 [Recursive feature selection](#rec_feat_sel) <br>\n",
    "  &emsp;&emsp;3.5.5 [Lasso Regression](#lasso) <br>\n",
    "  &emsp;&emsp;3.5.6 [Boruta](#boruta) <br>\n",
    "&emsp;3.6 [Models](#models) <br>\n",
    "  &emsp;&emsp;3.6.1 [Linear Models](#lin_mods) <br>\n",
    "  &emsp;&emsp;3.6.2 [Neural Networks](#NN) <br>\n",
    "4. [Main](#main) <br>\n",
    "   4.1 [ANN implementation](#ann_imp) <br>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scipy.stats\n",
    "import sklearn.metrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest,f_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso,ElasticNet\n",
    "\n",
    "# For neural net part\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(sys.path[0],'src'))\n",
    "import helpers as hl\n",
    "import data_viz as dv\n",
    "import outliers as out\n",
    "import lin_mods as lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import data\n",
    "<a id='import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './data'\n",
    "\n",
    "files = os.listdir(data_folder)  \n",
    "X_files = [filename for filename in files if (filename.endswith('.npy') and ('rsr' in filename))]\n",
    "y_files = [filename for filename in files if (filename.endswith('.npy') and ('chemical_shielding' in filename))]\n",
    "\n",
    "\n",
    "tot_data_X = np.load(data_folder + '/' + X_files[1], mmap_mode='r')\n",
    "tot_data_Y = np.load(data_folder + '/' + y_files[0], mmap_mode='r')\n",
    "data_X,data_y = hl.load_data(100,tot_data_X,tot_data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38514, 14400)\n",
      "(38514,)\n"
     ]
    }
   ],
   "source": [
    "print(X_tot.shape)\n",
    "print(y_tot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Vizualisation\n",
    "<a id='data_viz'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x792 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dv.plot_distribs(pd.DataFrame(data_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the different features are scaled pretty differently, we might want to scale them beforehand. Since they don' look like following a gaussian, we'll apply min/max scaling: but in order to do so, we first need to get rid of the outliers thanks to one of the following methods\n",
    "* Zscore: not adapted as our data might not be gaussian\n",
    "* DBScan:\n",
    "* Isolation Forest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing\n",
    "<a id='preprocess'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### 2.1 Outliers detection\n",
    "<a id='outliers'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 DBSCAN\n",
    "<a id = 'dbscan'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: computationally too demanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Interquartile range method (IQR)\n",
    "<a id = 'iqr'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consists in considering as outliers all data points that lie in >1.5 interquartile range from the quartiles:\n",
    "\n",
    "* Method 1: removing the samples which are considered as outliers in their label\n",
    "* Method 2: removing the samples which are considered as outliers for any of the x features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X,data_y = hl.load_data(100,tot_data_X,tot_data_Y)\n",
    "lin = LinearRegression()\n",
    "SCORING = ['neg_mean_squared_error', 'neg_mean_absolute_error','r2']\n",
    "\n",
    "#TODO:: make an accurate KFold\n",
    "\n",
    "#method 1\n",
    "out_remover = out.IQR_outlier()\n",
    "out_remover.fit(data_X)\n",
    "data_X1,data_y1 = out_remover.transform(data_X,data_y)\n",
    "scores1 = cross_validate(lin, data_X1, data_y1, cv=10,scoring = SCORING)\n",
    "\n",
    "#method 2\n",
    "data_X2,data_y2 = out.IQR_y_outliers(data_X,data_y)\n",
    "scores2 = cross_validate(lin,data_X,data_y,cv = 10,scoring = SCORING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the 2 methods on a classic linear regression task in order to assess which one performs better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEMCAYAAADu7jDJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAeAUlEQVR4nO3df1TV9eHH8RcXEKZyEuY0UPuhhkeihsNfm0aKkoKA4I6N5nY6TumHObF1mqGub6VHsK3DcNkPqtmWTlwaMUlp4A/ohJayNcWoRI/V4Ye2wEK0y6/7/cPByUDk8oH7+QDPx1/e+/l83vd16XZf933v576vm8PhcAgAgC6ymR0AANC7USQAAEMoEgCAIRQJAMAQigQAYIiH2QFcqbm5WXV1dfL09JSbm5vZcQCgV3A4HGpoaNCgQYNks7Wdf/SrIqmrq9Mnn3xidgwA6JUCAwPl4+PT5vp+VSSenp6SLv8xBgwYYHIaAOgd6uvr9cknn7Q+h35XvyqSlrezBgwYIC8vL5PTAEDvcrWPBPiwHQBgCEUCADCEIgEAGEKRoMuqq6v12GOPqaamxuwoAExEkaDLMjMz9eGHHyozM9PsKABMRJGgS6qrq7Vv3z45HA7l5+czKwH6MYoEXZKZmanm5mZJl1cMYFYC9F8UCbrk4MGDamxslCQ1NjbqwIEDJicCYBaKBF0yY8YMeXhc/j6rh4eHZs6caXIiAGaxRJFcunRJK1euVEREhObOndvhq9vS0lItWrRIUVFRioqKUkFBgQuTokVCQkLr4m02m00JCQkmJwJgFksskfLKK69o0KBBysvL05kzZ7Ro0SL985//1KBBg67Y7+LFi1q+fLmeeeYZhYSEqLGxUbW1tSal7t/8/Pw0a9Ys5ebmavbs2fL19TU7EgCTWGJGsnfv3tZXtDfddJOCg4NVWFjYZr+cnByFhoYqJCRE0uW3VHgCM09CQoKCgoKYjQD9nCVmJBUVFRoxYkTrZX9/f1VVVbXZr6ysTB4eHkpMTNS5c+d06623atWqVbruuutcGRf/4+fnp9TUVLNjADCZS4okPj5eFRUV7W4rKirq9DhNTU06fPiwMjMzNXToUKWkpCg1NVUpKSlO5SkpKXFqfwDA1bmkSLKysjrcHhAQoPLycvn5+UmSKisrNWXKlHb3mzJlioYNGyZJiomJ0erVq53OExwczDLyANBJdru9wxfglviMZO7cudqxY4ck6cyZMzp+/LjuuOOONvtFRkbq+PHjunDhgiSpsLBQ48aNc2lWAMCVLFEkS5Ys0ddff62IiAjdf//9euqppzR48GBJUnp6urZv3y7p8oxk6dKlSkhIUExMjE6cOKHk5GQzo/drLNoIQJLcHA6Hw+wQrtIyPeOtre7x3HPPKTc3V5GRkXrwwQfNjgOgh1zrudMSMxL0PizaCKAFRYIuYdFGAC0oEnQJizYCaEGRoEtYtBFAC4oEXcKijQBaUCTokpZFG93c3Fi0EejnLLHWFnqnhIQEffbZZ8xGgH6OIkGXsWgjAIm3tgAABjEjAdCt9u/fr7y8PFMznD9/XpI0ZMgQU3NIUkREhMLDw82O0aMoEgB9TnV1tSRrFEl/QJEA6Fbh4eGmvwJvWczV2d8qQtfwGQkAwBCKBABgCEUCADCEIgEAGEKRAAAMoUgAAIZY4vTfS5cuKTk5WSdOnJC7u7tWrVrV7rLkzc3N2rBhgw4dOiSbzaZhw4Zpw4YNGj58uAmpAQCSRWYkr7zyigYNGqS8vDy98MILWrt2rerq6trst3//fh07dkzZ2dnavXu3xo4dq+eff96ExACAFpYokr1797auIHvTTTcpODhYhYWF7e5bX18vu92u5uZm1dXV6frrr3dlVADAd1jira2KigqNGDGi9bK/v7+qqqra7BceHq73339f06dPl7e3t0aPHq3HH3/clVEBAN/hkiKJj49XRUVFu9uKioo6Pc6JEyd06tQpFRYWauDAgdqwYYNSU1OdLpOSkhKn9gfQu9TW1kqSiouLTU7SP7ikSLKysjrcHhAQoPLycvn5+UmSKisrNWXKlHbHmTp1qnx8fCRJsbGxWr16tdN5goOD5eXl5fRxAHqHnTt3SpJCQ0NNTtI32O32Dl+AW+Izkrlz52rHjh2SpDNnzuj48eO644472uw3cuRIHT58WA0NDZKkgoIC3XLLLS7NCgC4kiU+I1myZIkee+wxRUREyGaz6amnntLgwYMlSenp6Ro2bJjuueceLVq0SCdPnlRsbKw8PDzk7++vdevWmZweAPo3SxTJwIEDtWnTpna3JSUltf7by8uLZaEBwGIs8dYWAKD3okgAAIZQJAAAQygSAIAhFAkAwBCKBABgCEUCADCEIgEAGEKRAAAMoUgAAIZQJAAAQygSAIAhFAkAwBCKBABgCEUCADCEIgEAGEKRAAAMoUgAAIZYokiys7MVExOjoKAgbd26tcN9//73vysiIkKzZ8/WU089pebmZhelBAC0xxK/2T5+/HilpaUpIyOjw/0+//xzPfvss3rzzTc1ZMgQJSYm6h//+Ifi4uJclBSwrpdeekmnT582O4YltPwdkpOTTU5iDaNHj1ZiYmKPjW+JIgkMDJQk2WwdT5DefvttzZ49W35+fpKkhQsX6o033qBIAF1+8jxZekLXD7bE/9am+p7j8jsVtZ9/bHIS81VdaOzx2+hVj7jKykoFBAS0Xg4ICFBlZaWJiQBruX6whxbf7md2DFjIlmPVPX4bLimS+Ph4VVRUtLutqKhI7u7urojRqqSkxKW3B7hCbW2t2RFgUbW1tSouLu6x8V1SJFlZWd0yjr+//xWFVFFRIX9/f6fHCQ4OlpeXV7dkAqxi586dqj1vdgpYkY+Pj0JDQ7t8vN1u7/AFuCXO2uqsOXPmKD8/X9XV1Wpubtbrr7+uyMhIs2MBQL9miSLJyclRWFiYcnNzlZ6errCwMJWVlUmS0tPTtX37dknSqFGjtGzZMt1999266667NHLkSMXGxpoZHQD6PUt82B4dHa3o6Oh2tyUlJV1xOSEhQQkJCa6IBQDoBEvMSAAAvRdFAgAwhCIBABhCkQAADKFIAACGUCQAAEMoEgCAIRQJAMAQigQAYAhFAgAwhCIBABhCkQAADKFIAACGUCQAAEMoEgCAIZb4PRIAxtXU1Oi/Fxq15Vi12VFgIVUXGtVYU9Ojt8GMBABgCDMSoI/w9fWVx4VzWny7n9lRYCFbjlXLx9e3R2/DEjOS7OxsxcTEKCgoSFu3br3qfvn5+VqwYIGio6M1b948/fnPf3ZhSgBAe5yakTQ0NOg///mPzp07p6ioKF28eFGSNHDgQEMhxo8fr7S0NGVkZHS43w9+8AM9//zzGj58uGpra7VgwQLdfvvtmjhxoqHbBwB0XaeL5OOPP9aDDz6oAQMG6OzZs4qKitKRI0eUlZWlP/7xj4ZCBAYGSpJsto4nSD/84Q9b/+3j46MxY8aovLycIgEAE3X6ra0nnnhCK1asUG5urjw8LvfPpEmTVFxc3GPhOnLq1Cl98MEHmjp1qim3DwC4rNMzkrKyMs2fP1+S5ObmJunyW1p2u/2ax8bHx6uioqLdbUVFRXJ3d+9sDEnSuXPntGzZMj3++OMaPny4U8dKUklJidPHAFZXW1trdgRYVG1tbY++6O90kYwYMUIlJSW67bbbWq87duyYbrjhhmsem5WV1bV07fjyyy+1ePFiLV26VFFRUV0aIzg4WF5eXt2WCbCCnTt3qva82SlgRT4+PgoNDe3y8Xa7vcMX4J0ukqSkJN1///1KSEhQQ0ODXnzxRWVmZmrdunVdDuesmpoaLV68WIsWLdLChQtddrsAgKvr9GckM2fO1EsvvaTq6mpNmjRJ5eXl+tOf/qTp06cbDpGTk6OwsDDl5uYqPT1dYWFhKisrkySlp6dr+/btkqSMjAydOXNGO3bs0Pz58zV//nzt2rXL8O0DALrOzeFwOMwO4Sot0zPe2kJflJycrJOlJ3T9YL5nfKG+WZI0eIAlvipnqqoLjbpl/K1KSUnp8hjXeu7s9CMuPT39qtuSkpK6lg5Atxk9erTZESzji9OnJUn+o/ib+KjnHxudLpKqqqorLn/xxRc6cuSIZs+e3e2hADgvMTHR7AiWkZycLEmGXoWj8zpdJO39ByksLNRbb73VrYEAAL2LoTdTp0+frocffri7sgDoA/bv36+8vDxTM5z+31tbLTMTM0VERCg8PNzsGD2q00Xy+eefX3H50qVLysnJkb+/f7eHAgAj/PxYAdmVOl0kERERcnNzU8tJXt/73vc0fvx4paam9lg4AL1PeHh4n38Fjit1ukg++uijnswBAOilOMkaAGBIhzOSO++8s3WBxo4cPHiwu/IAAHqZDovk97//vatyAAB6qQ6LZPLkya7KASdZ4RTL8+cvLzU7ZMgQU3NI/eMUS8CqnPoeSWlpqY4ePaqamhp9e4kulkjpn6qrqyVZo0gAmKfTRbJjxw6lpKRo2rRpKiwsVFhYmN59913NmjWrJ/NZzksvvdT6ZSdYR15enukztNGjR7NMCfqlThfJyy+/rJdfflkTJ07UpEmTtHnzZhUUFGjPnj09mc9yTp8+rZIPP5a7N6/Cmxsv/7Jl6emzJicxX9M3/KIU+q9OF8mXX36piRMnSpJsNpuam5t155136tFHH+2xcFbl7j1EA2/sXzMxdOzip/vMjgCYptNFcv311+vzzz/XqFGjdNNNN2nfvn3y9fWVp6dnT+YDAFhcp4tk6dKlOn36tEaNGqVly5YpKSlJDQ0NWrNmTU/mAwBYXKeLpLS0VDExMZIuf1Hx/fffV0NDgwYNGtRj4QAA1ufUEinLli3TXXfdpU2bNqm8vLzbSiQ7O1sxMTEKCgrS1q1br7m/3W5XVFSUFixY0C23DwDouk4XyZo1a1RYWKj/+7//U2Vlpe6++24tWLBAW7ZsMRxi/PjxSktLU3R0dKf2T0tLU0hIiOHbBQAY59SMxGazadq0aUpJSVFOTo6GDBmip59+2nCIwMBAjR07VjbbteMcPXpUZ86c0fz58w3fLgDAOKeKpK6uTtnZ2brvvvs0Z84cubu7u/T3SC5evKgNGzboySefdNltAgA61ukP21esWKF33nlHQUFBmjdvnlJTUzv9K2Tx8fGqqKhod1tRUZHc3d07Nc7TTz+tn//85xo+fLjOnDnT2ehtlJSUdPnY2traLh+Lvq22tlbFxcVmxwBcrtNFEhwcrMcee0wBAQFO30hWVpbTx7SnuLhYhYWFeu6552S32/XVV18pJiZGu3fvdmqc4OBgeXl5dSnDzp07pS8udulY9G0+Pj4KDQ01OwbQ7ex2e4cvwDtdJPfdd1+3BDLi24Xx3nvvaePGjXrjjTdMTAQAsMQvJObk5CgsLEy5ublKT09XWFiYysrKJEnp6enavn27yQkBAFfj1DLyPSU6Ovqqp/5ebYn6KVOmMBsBAAuwxIwEANB7USQAAEMoEgCAIRQJAMAQigQAYAhFAgAwhCIBABhCkQAADKFIAACGUCQAAEMoEgCAIZZYa6s3qampUdM353Xx031mR4GFNH1zXjU1A8yOAZiCGQkAwBBmJE7y9fVVVU29Bt44y+wosJCLn+6Tr6+v2TEAUzAjAQAYQpEAAAyhSAAAhlAkAABDKBIAgCGWKJLs7GzFxMQoKChIW7du7XDf0tJSLVq0SFFRUYqKilJBQYGLUgIA2mOJ03/Hjx+vtLQ0ZWRkdLjfxYsXtXz5cj3zzDMKCQlRY2OjamtrXZQSANAeSxRJYGCgJMlm63iClJOTo9DQUIWEhEiSPDw8OHcfAExmiSLprLKyMnl4eCgxMVHnzp3TrbfeqlWrVum6665zapySkpIuZ2AGhKupra1VcXGx2TEAl3NJkcTHx6uioqLdbUVFRXJ3d+/UOE1NTTp8+LAyMzM1dOhQpaSkKDU1VSkpKU7lCQ4OlpeXl1PHtNi5c6f0xcUuHYu+zcfHR6GhoWbHALqd3W7v8AW4S4okKyurW8YJCAjQlClTNGzYMElSTEyMVq9e3S1jAwC6xhJnbXVWZGSkjh8/rgsXLkiSCgsLNW7cOJNTAUD/ZokiycnJUVhYmHJzc5Wenq6wsDCVlZVJktLT07V9+3ZJl2ckS5cuVUJCgmJiYnTixAklJyebGR0A+j1LfNgeHR2t6OjodrclJSVdcTkuLk5xcXGuiAWgl6qurtbTTz+tVatWcWanC1hiRgIA3SkzM1MffvihMjMzzY7SL1AkAPqU6upq7du3Tw6HQ/n5+aqpqTE7Up9HkQDoUzIzM9Xc3CxJam5uZlbiAhQJgD7l4MGDamxslCQ1NjbqwIEDJifq+ygSAH3KjBkz5OFx+TwiDw8PzZw50+REfR9FAqBPSUhIaF23z2azKSEhweREfR9FAqBP8fPz06xZs+Tm5qbZs2dz+q8LWOJ7JADQnRISEvTZZ58xG3ERiqQLmr45r4uf7jM7humaG7+RJNk8vE1OYr6mb85LGm52DPyPn5+fUlNTzY7Rb1AkTho9erTZESzj9OnTkqTRo3kClYbz2EC/RZE4KTEx0ewIltGyzpmzy/gD6Fv4sB0AYAhFAgAwhCIBABhCkQAADKFIAACGUCQAAEMoEgCAIZYokuzsbMXExCgoKEhbt2696n7Nzc1av3695s2bp5iYGC1ZskRnz551YVIAwHdZokjGjx+vtLS0q/5ue4v9+/fr2LFjys7O1u7duzV27Fg9//zzLkoJAGiPJb7ZHhgYKEmtSz93pL6+Xna7XTabTXV1dRo5cmRPxwMAdMASRdJZ4eHhev/99zV9+nR5e3tr9OjRevzxx50ep6SkpAfS9T+1tbWSpOLiYpOTADCTS4okPj5eFRUV7W4rKiqSu7t7p8Y5ceKETp06pcLCQg0cOFAbNmxQamqq02USHBwsLy8vp45BWzt37pQkhYaGmpwEQE+y2+0dvgB3SZFkZWV12zhTp06Vj4+PJCk2NlarV6/ulrEBAF1jiQ/bO2vkyJE6fPiwGhoaJEkFBQW65ZZbTE4FAP2bJYokJydHYWFhys3NVXp6usLCwlRWViZJSk9P1/bt2yVJixYt0rBhwxQbG6uYmBiVlJS0LmUOADCHJT5sj46Ovuqpv0lJSa3/9vLy4rcvAMBiLDEjAQD0XhQJAMAQigQAYAhFAgAwhCIBABhCkQAADKFIAACGUCQAAEMoEgCAIRQJAMAQigQAYAhFAgAwhCIBABhCkQAADKFIAACGUCQAAEMoEgCAIRQJAMAQS/zU7pNPPqlDhw5pwIABGjhwoNasWaPbbrut3X03b96srKwsSVJ8fLweeughV0YFAHyHJYokLCxMq1evlqenpw4cOKCHH35Y+fn5bfY7cuSIcnNzlZOTI0lauHChJk+erEmTJrk6MgDgfyzx1tbMmTPl6ekpSQoJCVFVVZWam5vb7Ldnzx7FxcXJ29tb3t7eiouL0549e1wdFwDwLZYokm/btm2bZsyYIZutbbTKykoFBAS0Xvb391dlZaUr4wEAvsMlb23Fx8eroqKi3W1FRUVyd3eXJL311lvavXu3tm3b1qN5SkpKenR8V/jggw/073//29QMVVVVkqTly5ebmkOSJkyYoJCQELNjAP2SS4qk5cPxjuTl5SktLU2vvvqqhg4d2u4+/v7+VxRSZWWl/P39nc4THBwsLy8vp4+zkq+++kplZWWmZmhqapIk+fj4mJpDkm6++WaFhoaaHQPok+x2e4cvwC3xYfuBAweUkpKiLVu2aOTIkVfdb+7cuVq/fr0WLVokSXrzzTf1u9/9zlUxLSU8PFzh4eFmxwAAaxRJcnKyPD09tWLFitbrXn31Vfn6+mrNmjUKDw/XrFmzNGXKFN11112Kjo6Ww+FQXFycJk+ebGJyAICbw+FwmB3CVVqmZ33hrS0AcJVrPXda7qwtAEDvQpEAAAyhSAAAhlAkAABDKBIAgCGWOP3XVVpOUKuvrzc5CQD0Hi3PmVc7ybdfFUlDQ4Mk6ZNPPjE5CQD0Pg0NDfL29m5zfb/6Hklzc7Pq6urk6ekpNzc3s+MAQK/gcDjU0NCgQYMGtbugbr8qEgBA9+PDdgCAIRQJAMAQigQAYAhFAgAwhCIBABhCkQAADKFIAACGUCQwpLS0VHv27LniunHjxqmurq7d/Tdu3Kjw8HCNGzeOFQbQo5x5bNbU1CgxMVFz5sxRTEyMli9frurqaldF7fUoEhhSWlqq3NzcTu8/a9Ysbdu2TSNGjOjBVIBzj003NzctXbpUb7/9tnbv3q1Ro0bpD3/4Qw8n7Dv61VpbuNK4ceO0cuVK5efn6/z581q/fr2Kior0zjvvqLGxUenp6RozZowkKSsrS3/729/U1NSkwYMH64knnpCvr682bdqkCxcuaP78+Zo0aZLWrl0rSXrttdeUl5en8+fP67e//a3mzJkjSZo4caJp9xe9h6sfm0OGDNGUKVNabz8kJETbt2835b73Sg70W4GBgY6tW7c6HA6HY8+ePY6QkBDHgQMHHA6Hw5GRkeF45JFHHA6Hw3HkyBFHYmKiw263OxwOh+PgwYOOn/3sZw6Hw+HYtWuX49e//nWbcV977TWHw+FwHD161DF9+vQ2tz1z5kzHxx9/3CP3C72fmY/NpqYmx7333uv4y1/+0iP3rS9iRtLPRUZGSpJuvfVWSdKMGTMkScHBwcrLy5Mk7d+/Xx999JEWLlwo6fICbl9//XWH40ZFRUm6/Mru3Llzstvt8vLy6om7gD7KrMfmunXrNHDgQP3iF7/o1vvTl1Ek/VzL/0A2m00DBgxovd5ms6mxsVHS5f85f/rTnyopKcnpcd3d3SVJjY2NFAmcYsZjc+PGjfr000/1wgsvtLvKLdrHXwrXFB4eruzsbFVVVUmSmpqaVFJSIkkaPHiwamtrzYyHfqw7H5tpaWkqKSnR5s2bryguXBtFgmuaNGmSVq5cqQcffFCxsbGKjo7Wvn37JEk//vGPdenSJcXGxmr9+vXXHGv9+vUKCwtTVVWVFi9erHnz5vV0fPRh3fXYPHnypF544QWdO3dOCQkJmj9/vh566CFX3IU+gd8jAQAYwowEAGAIRQIAMIQiAQAYQpEAAAyhSAAAhlAkgEmOHj3augbZtbzxxhu65557rrr9l7/8pV5//fXuigY4hSIBTDJx4kS9/fbbZscADKNIABO0LPEB9AUUCeCEjIwMrVix4orr1q9fr/Xr12vXrl2KjIzUhAkTNGvWLGVmZrbu89577yksLEwZGRmaNm2akpOTW6/79tizZ8/WhAkTFBUV1bowYQuHw6F169YpNDRUc+fO1aFDh66ac+fOnYqMjNSkSZO0ZMkSlZeXd9NfAGiLIgGcMG/ePBUUFOjChQuSLq/tlJubq+joaH3/+9/Xiy++qH/9619KSUlRSkqKTpw40Xrsf//7X3311Vc6cOCA1q1b12bsUaNGadu2bSouLtby5cv16KOP6ty5c63bjx07plGjRunw4cNasWKFli9frvPnz7cZJz8/Xy+++KKeffZZHTp0SKGhoXrkkUd64K8BXEaRAE4YMWKEgoKClJ+fL0k6fPiwvL29FRISohkzZuiGG26Qm5ubJk+erGnTpuno0aOtx9psNq1YsUIDBgyQt7d3m7EjIyM1fPhw2Ww2RUVF6cYbb9SxY8dat/v5+enee++Vp6enoqKidPPNN+vgwYNtxsnMzNR9992nMWPGyMPDQw888IBKS0uZlaDHsIw84KTo6Gjl5OQoLi5OOTk5io6OliQVFBRo8+bNOnPmjJqbm/XNN98oMDCw9ThfX98Ol9J/8803tWXLltYn/IsXL6qmpqZ1+/Dhw+Xm5tZ6OSAg4IoZS4uKigpt2LBBGzdubL3O4XDo7Nmz/MQxegRFAjgpMjJSGzduVFVVlfLy8rRjxw7V19drxYoV2rhxo2bNmiVPT08tW7ZM314T9dsl8F3l5eVau3atXn31VU2YMEHu7u6aP3/+FfucPXtWDoejdZzKykqFh4e3Gcvf318PPPCAYmNju+keAx3jrS3ASX5+fpo8ebKSk5M1cuRIjRkzRvX19aqvr5efn588PDxUUFCgd999t9NjXrp0SW5ubvLz85Mk7dq1SydPnrxin+rqav31r39VQ0OD9u7dq1OnTunOO+9sM1ZCQoIyMjJaj6+trdXevXsN3GOgY8xIgC6Ijo7WqlWr9Oijj0q6/CNKa9eu1cqVK1VfX6+ZM2e2O1u4mrFjx+pXv/qVEhIS5Obmpri4OP3oRz+6Yp/bb79dn376qaZOnaqhQ4dq06ZN8vX1bTNWRESE6urq9Jvf/Ebl5eXy8fHRT37yk9afrgW6G79HAgAwhLe2AACGUCQAAEMoEgCAIRQJAMAQigQAYAhFAgAwhCIBABhCkQAADKFIAACG/D/U3QiJ1cufrgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_box_plot(scores1,scores2,metric):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    scores_df = pd.DataFrame({'meth1':scores1[metric],'meth2':scores2[metric]})\n",
    "    sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(scores_df))\n",
    "plot_box_plot(scores1,scores2,'test_neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*c.f.* `data_preprocessing.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Min/max Scaling\n",
    "<a id='minmax'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minmax scaling seems as an appropriate way to scale our data only if an outlier removing method is applied beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot of min/max scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Data Normalization\n",
    "<a id='minmax'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot of normalized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Standardization\n",
    "<a id='std'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our data definitely does not follow a normal distribution, standardization has not been taken into account for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 PCA\n",
    "<a id='pca'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_X,data_y = hl.load_data(100,tot_data_X,tot_data_Y)\n",
    "hl.plot_PCA(100,data_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 2.4 Feature Selection\n",
    "<a id='feat_sel'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.3 Univariate feature selection\n",
    "<a id='un_feat_sel'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running univariate feature selection on the laptops turned out to be extremely costly: this is why we performed it after running a PCA on the trained subset of data. This was before having access to more computational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,axes = plt.subplots(2,2,figsize = (20,10))\n",
    "for i in range(4):\n",
    "    data_X,data_y = hl.load_data(100,tot_data_X,tot_data_Y)\n",
    "    pca = PCA(n_components = 100)\n",
    "    pca.fit(data_X)\n",
    "    data_X_pca = pca.transform(data_X)\n",
    "    hl.univ_feat_sel(data_X_pca,data_y,ax = axes[np.unravel_index(i,(2,2))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the importance of the various PCs is not regular across various random samples for data_X: this is probably due to the fact that the PCs are not the same for a different subset of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.4 Recursive feature elimination\n",
    "<a id='rec_feat_sel'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS turns out to be extensively demanding in computational power: it crashed many times on our laptops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 3\n",
      "Selected Features: [False False False ... False False False]\n",
      "Feature Ranking: [   94 13683 13650 ...  7192  6586  5788]\n"
     ]
    }
   ],
   "source": [
    "#almost impossible without a pca ==> pca first\n",
    "data_X,data_y = hl.load_data(60,tot_data_X,tot_data_Y)\n",
    "pca = PCA(n_components = 3)\n",
    "pca.fit(data_X)\n",
    "data_X_pca = pca.transform(data_X)\n",
    "ridge = Ridge(alpha = 0.1)\n",
    "rfe = RFE(ridge,0.1)\n",
    "fit = rfe.fit(data_X,data_y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 2.5 Linear Models Optimization\n",
    "<a id='models'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X,data_y = hl.load_data(100,tot_data_X,tot_data_Y)\n",
    "\n",
    "iqr = out.IQR_outlier()\n",
    "min_max = MinMaxScaler()\n",
    "pca = PCA(n_components=3)\n",
    "data_X,data_y = iqr.fit_transform(data_X,data_y)\n",
    "pipeline = Pipeline([('min_max', min_max), ('pca', pca)])\n",
    "data_X = pipeline.fit_transform(data_X)\n",
    "\n",
    "interv = np.logspace(np.log10(0.001),np.log10(10000),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398.1071705534969\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alph_opt = lm.test_alphas_meth(Ridge,interv,data_X,data_y,k = 2)\n",
    "print(alph_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X,data_y = hl.load_data(100,tot_data_X,tot_data_Y)\n",
    "\n",
    "iqr = out.IQR_outlier()\n",
    "min_max = MinMaxScaler()\n",
    "pca = PCA(n_components=3)\n",
    "data_X,data_y = iqr.fit_transform(data_X,data_y)\n",
    "pipeline = Pipeline([('min_max', min_max), ('pca', pca)])\n",
    "data_X = pipeline.fit_transform(data_X)\n",
    "\n",
    "interv = np.logspace(np.log10(0.001),np.log10(10000),6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Elastic Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch NN\n",
    "<a id='pytorch'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_c.f._ `neural_nets.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras NN\n",
    "<a id='keras'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_c.f._ `k_models.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main\n",
    "<a id='main'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main consists of the different grid search for linear models and trainings for NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data prior feeding it to the mode\n",
    "def load_process_data(n, iqr=False, pca=False, scale=False):\n",
    "    # load all datas\n",
    "    # filter (iqr)\n",
    "    if iqr:\n",
    "        # remove outliers\n",
    "    # select n samples and split in train-test sets\n",
    "    # perform pca\n",
    "    if pca:\n",
    "        # do pca\n",
    "    # normalize \n",
    "    if scale:\n",
    "        # do normalization\n",
    "    #return X_train, X_test, y_train, y_test\n",
    "    raise NotImplementedError "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How man PCA components do we need?\n",
    "plot_PCA(100, X_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">40 components to explain 95% of the total variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 PyTorch pipeline\n",
    "<a id='main_pytorch'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_process_data(n, iqr=False, pca=False, scale=False):\n",
    "    if iqr:\n",
    "        X_train, X_test, y_train, y_test = load_data_train_test(n, X_tot, y_tot, iqr=apply_iqr)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = load_data_train_test(n, X_tot, y_tot)\n",
    "        \n",
    "    if scale:\n",
    "        min_max = MinMaxScaler()\n",
    "        X_train, X_test = apply_scaler(min_max, X_train, X_test)\n",
    "        \n",
    "    if pca:\n",
    "        n = 80\n",
    "        X_train, X_test = do_PCA(X_train, X_test, n)\n",
    "        nb_input_neurons = n\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37338, 14400)\n",
      "(37338,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "mse_storage = []\n",
    "mae_storage = []\n",
    "r2_storage = []\n",
    "\n",
    "mini_batch_size = 10\n",
    "nb_input_neurons = 14400\n",
    "\n",
    "# Load data, remove outliers but do not split yet into train and test\n",
    "X, y = load_data(38514, tot_data_x=X_tot, tot_data_y=y_tot, iqr=True)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "[epoch 1] loss: 195956.91\n",
      "[epoch 2] loss: 5717.32\n",
      "[epoch 3] loss: 3410.36\n",
      "[epoch 4] loss: 2653.15\n",
      "[epoch 5] loss: 2306.10\n",
      "[epoch 6] loss: 2093.14\n",
      "[epoch 7] loss: 1942.76\n",
      "[epoch 8] loss: 1829.11\n",
      "[epoch 9] loss: 1739.60\n",
      "[epoch 10] loss: 1666.88\n",
      "[epoch 11] loss: 1606.23\n",
      "[epoch 12] loss: 1554.56\n",
      "[epoch 13] loss: 1509.86\n",
      "[epoch 14] loss: 1470.77\n",
      "[epoch 15] loss: 1436.31\n",
      "[epoch 16] loss: 1405.71\n",
      "[epoch 17] loss: 1378.39\n",
      "[epoch 18] loss: 1353.86\n",
      "[epoch 19] loss: 1331.73\n",
      "[epoch 20] loss: 1311.67\n",
      "[epoch 21] loss: 1293.40\n",
      "[epoch 22] loss: 1276.70\n",
      "[epoch 23] loss: 1261.37\n",
      "[epoch 24] loss: 1247.26\n",
      "[epoch 25] loss: 1234.22\n",
      "[epoch 26] loss: 1222.14\n",
      "[epoch 27] loss: 1210.92\n",
      "[epoch 28] loss: 1200.46\n",
      "[epoch 29] loss: 1190.69\n",
      "[epoch 30] loss: 1181.55\n",
      "[epoch 31] loss: 1172.98\n",
      "[epoch 32] loss: 1164.92\n",
      "[epoch 33] loss: 1157.34\n",
      "[epoch 34] loss: 1150.19\n",
      "[epoch 35] loss: 1143.43\n",
      "[epoch 36] loss: 1137.03\n",
      "[epoch 37] loss: 1130.97\n",
      "[epoch 38] loss: 1125.22\n",
      "[epoch 39] loss: 1119.75\n",
      "[epoch 40] loss: 1114.55\n",
      "[epoch 41] loss: 1109.59\n",
      "[epoch 42] loss: 1104.86\n",
      "[epoch 43] loss: 1100.34\n",
      "[epoch 44] loss: 1096.02\n",
      "[epoch 45] loss: 1091.88\n",
      "[epoch 46] loss: 1087.91\n",
      "[epoch 47] loss: 1084.10\n",
      "[epoch 48] loss: 1080.44\n",
      "[epoch 49] loss: 1076.92\n",
      "[epoch 50] loss: 1073.53\n",
      "[epoch 51] loss: 1070.26\n",
      "[epoch 52] loss: 1067.11\n",
      "[epoch 53] loss: 1064.07\n",
      "[epoch 54] loss: 1061.13\n",
      "[epoch 55] loss: 1058.28\n",
      "[epoch 56] loss: 1055.53\n",
      "[epoch 57] loss: 1052.86\n",
      "[epoch 58] loss: 1050.28\n",
      "[epoch 59] loss: 1047.77\n",
      "[epoch 60] loss: 1045.33\n",
      "[epoch 61] loss: 1042.97\n",
      "[epoch 62] loss: 1040.67\n",
      "[epoch 63] loss: 1038.43\n",
      "[epoch 64] loss: 1036.25\n",
      "[epoch 65] loss: 1034.12\n",
      "[epoch 66] loss: 1032.06\n",
      "[epoch 67] loss: 1030.04\n",
      "[epoch 68] loss: 1028.07\n",
      "[epoch 69] loss: 1026.15\n",
      "[epoch 70] loss: 1024.27\n",
      "[epoch 71] loss: 1022.44\n",
      "[epoch 72] loss: 1020.64\n",
      "[epoch 73] loss: 1018.89\n",
      "[epoch 74] loss: 1017.17\n",
      "[epoch 75] loss: 1015.49\n",
      "[epoch 76] loss: 1013.85\n",
      "[epoch 77] loss: 1012.24\n",
      "[epoch 78] loss: 1010.66\n",
      "[epoch 79] loss: 1009.11\n",
      "[epoch 80] loss: 1007.59\n",
      "[epoch 81] loss: 1006.10\n",
      "[epoch 82] loss: 1004.64\n",
      "[epoch 83] loss: 1003.20\n",
      "[epoch 84] loss: 1001.79\n",
      "[epoch 85] loss: 1000.41\n",
      "[epoch 86] loss: 999.05\n",
      "[epoch 87] loss: 997.71\n",
      "[epoch 88] loss: 996.40\n",
      "[epoch 89] loss: 995.11\n",
      "[epoch 90] loss: 993.84\n",
      "[epoch 91] loss: 992.59\n",
      "[epoch 92] loss: 991.36\n",
      "[epoch 93] loss: 990.15\n",
      "[epoch 94] loss: 988.96\n",
      "[epoch 95] loss: 987.78\n",
      "[epoch 96] loss: 986.63\n",
      "[epoch 97] loss: 985.49\n",
      "[epoch 98] loss: 984.37\n",
      "[epoch 99] loss: 983.27\n",
      "[epoch 100] loss: 982.18\n",
      "[epoch 101] loss: 981.11\n",
      "[epoch 102] loss: 980.05\n",
      "[epoch 103] loss: 979.01\n",
      "[epoch 104] loss: 977.98\n",
      "[epoch 105] loss: 976.96\n",
      "[epoch 106] loss: 975.96\n",
      "[epoch 107] loss: 974.98\n",
      "[epoch 108] loss: 974.00\n",
      "[epoch 109] loss: 973.04\n",
      "[epoch 110] loss: 972.09\n",
      "[epoch 111] loss: 971.16\n",
      "[epoch 112] loss: 970.23\n",
      "[epoch 113] loss: 969.32\n",
      "[epoch 114] loss: 968.42\n",
      "[epoch 115] loss: 967.53\n",
      "[epoch 116] loss: 966.65\n",
      "[epoch 117] loss: 965.79\n",
      "[epoch 118] loss: 964.93\n",
      "[epoch 119] loss: 964.08\n",
      "[epoch 120] loss: 963.24\n",
      "[epoch 121] loss: 962.42\n",
      "[epoch 122] loss: 961.60\n",
      "[epoch 123] loss: 960.79\n",
      "[epoch 124] loss: 959.99\n",
      "[epoch 125] loss: 959.20\n",
      "[epoch 126] loss: 958.42\n",
      "[epoch 127] loss: 957.65\n",
      "[epoch 128] loss: 956.88\n",
      "[epoch 129] loss: 956.13\n",
      "[epoch 130] loss: 955.38\n",
      "[epoch 131] loss: 954.64\n",
      "[epoch 132] loss: 953.91\n",
      "[epoch 133] loss: 953.19\n",
      "[epoch 134] loss: 952.47\n",
      "[epoch 135] loss: 951.76\n",
      "[epoch 136] loss: 951.06\n",
      "[epoch 137] loss: 950.37\n",
      "[epoch 138] loss: 949.68\n",
      "[epoch 139] loss: 949.00\n",
      "[epoch 140] loss: 948.33\n",
      "[epoch 141] loss: 947.66\n",
      "[epoch 142] loss: 947.01\n",
      "[epoch 143] loss: 946.35\n",
      "[epoch 144] loss: 945.71\n",
      "[epoch 145] loss: 945.07\n",
      "[epoch 146] loss: 944.43\n",
      "[epoch 147] loss: 943.80\n",
      "[epoch 148] loss: 943.18\n",
      "[epoch 149] loss: 942.57\n",
      "[epoch 150] loss: 941.96\n",
      "FOLD 2\n",
      "[epoch 1] loss: 206755.53\n",
      "[epoch 2] loss: 5089.68\n",
      "[epoch 3] loss: 2948.42\n",
      "[epoch 4] loss: 2376.50\n",
      "[epoch 5] loss: 2092.57\n",
      "[epoch 6] loss: 1911.44\n",
      "[epoch 7] loss: 1783.12\n",
      "[epoch 8] loss: 1685.23\n",
      "[epoch 9] loss: 1606.86\n",
      "[epoch 10] loss: 1542.26\n",
      "[epoch 11] loss: 1487.61\n",
      "[epoch 12] loss: 1440.78\n",
      "[epoch 13] loss: 1400.17\n",
      "[epoch 14] loss: 1364.95\n",
      "[epoch 15] loss: 1333.94\n",
      "[epoch 16] loss: 1306.22\n",
      "[epoch 17] loss: 1281.45\n",
      "[epoch 18] loss: 1259.13\n",
      "[epoch 19] loss: 1238.88\n",
      "[epoch 20] loss: 1220.54\n",
      "[epoch 21] loss: 1202.21\n",
      "[epoch 22] loss: 1182.42\n",
      "[epoch 23] loss: 1165.37\n",
      "[epoch 24] loss: 1149.02\n",
      "[epoch 25] loss: 1133.33\n",
      "[epoch 26] loss: 1118.36\n",
      "[epoch 27] loss: 1103.90\n",
      "[epoch 28] loss: 1089.83\n",
      "[epoch 29] loss: 1076.33\n",
      "[epoch 30] loss: 1063.30\n",
      "[epoch 31] loss: 1050.63\n",
      "[epoch 32] loss: 1038.49\n",
      "[epoch 33] loss: 1026.78\n",
      "[epoch 34] loss: 1015.59\n",
      "[epoch 35] loss: 1004.75\n",
      "[epoch 36] loss: 994.35\n",
      "[epoch 37] loss: 984.43\n",
      "[epoch 38] loss: 974.80\n",
      "[epoch 39] loss: 965.45\n",
      "[epoch 40] loss: 956.44\n",
      "[epoch 41] loss: 947.95\n",
      "[epoch 42] loss: 939.68\n",
      "[epoch 43] loss: 931.79\n",
      "[epoch 44] loss: 924.18\n",
      "[epoch 45] loss: 916.78\n",
      "[epoch 46] loss: 909.48\n",
      "[epoch 47] loss: 902.35\n",
      "[epoch 48] loss: 895.72\n",
      "[epoch 49] loss: 889.18\n",
      "[epoch 50] loss: 882.67\n",
      "[epoch 51] loss: 876.28\n",
      "[epoch 52] loss: 870.33\n",
      "[epoch 53] loss: 864.39\n",
      "[epoch 54] loss: 858.72\n",
      "[epoch 55] loss: 853.40\n",
      "[epoch 56] loss: 847.89\n",
      "[epoch 57] loss: 842.66\n",
      "[epoch 58] loss: 837.56\n",
      "[epoch 59] loss: 832.67\n",
      "[epoch 60] loss: 827.67\n",
      "[epoch 61] loss: 822.92\n",
      "[epoch 62] loss: 818.12\n",
      "[epoch 63] loss: 813.55\n",
      "[epoch 64] loss: 809.23\n",
      "[epoch 65] loss: 804.82\n",
      "[epoch 66] loss: 800.54\n",
      "[epoch 67] loss: 796.17\n",
      "[epoch 68] loss: 792.10\n",
      "[epoch 69] loss: 788.10\n",
      "[epoch 70] loss: 784.03\n",
      "[epoch 71] loss: 779.94\n",
      "[epoch 72] loss: 776.40\n",
      "[epoch 73] loss: 772.46\n",
      "[epoch 74] loss: 768.67\n",
      "[epoch 75] loss: 765.00\n",
      "[epoch 76] loss: 761.10\n",
      "[epoch 77] loss: 757.13\n",
      "[epoch 78] loss: 753.31\n",
      "[epoch 79] loss: 748.91\n",
      "[epoch 80] loss: 745.04\n",
      "[epoch 81] loss: 740.90\n",
      "[epoch 82] loss: 737.17\n",
      "[epoch 83] loss: 733.24\n",
      "[epoch 84] loss: 729.25\n",
      "[epoch 85] loss: 725.55\n",
      "[epoch 86] loss: 722.09\n",
      "[epoch 87] loss: 718.50\n",
      "[epoch 88] loss: 714.74\n",
      "[epoch 89] loss: 711.17\n",
      "[epoch 90] loss: 707.72\n",
      "[epoch 91] loss: 704.12\n",
      "[epoch 92] loss: 700.78\n",
      "[epoch 93] loss: 697.49\n",
      "[epoch 94] loss: 694.08\n",
      "[epoch 95] loss: 690.78\n",
      "[epoch 96] loss: 687.65\n",
      "[epoch 97] loss: 684.76\n",
      "[epoch 98] loss: 681.39\n",
      "[epoch 99] loss: 678.35\n",
      "[epoch 100] loss: 675.28\n",
      "[epoch 101] loss: 672.62\n",
      "[epoch 102] loss: 669.50\n",
      "[epoch 103] loss: 666.75\n",
      "[epoch 104] loss: 663.73\n",
      "[epoch 105] loss: 661.04\n",
      "[epoch 106] loss: 658.24\n",
      "[epoch 107] loss: 655.83\n",
      "[epoch 108] loss: 653.00\n",
      "[epoch 109] loss: 650.60\n",
      "[epoch 110] loss: 648.32\n",
      "[epoch 111] loss: 645.68\n",
      "[epoch 112] loss: 643.23\n",
      "[epoch 113] loss: 640.86\n",
      "[epoch 114] loss: 638.46\n",
      "[epoch 115] loss: 636.25\n",
      "[epoch 116] loss: 634.05\n",
      "[epoch 117] loss: 631.80\n",
      "[epoch 118] loss: 629.59\n",
      "[epoch 119] loss: 627.28\n",
      "[epoch 120] loss: 625.15\n",
      "[epoch 121] loss: 622.89\n",
      "[epoch 122] loss: 621.13\n",
      "[epoch 123] loss: 618.82\n",
      "[epoch 124] loss: 616.61\n",
      "[epoch 125] loss: 614.71\n",
      "[epoch 126] loss: 612.18\n",
      "[epoch 127] loss: 610.88\n",
      "[epoch 128] loss: 608.41\n",
      "[epoch 129] loss: 606.73\n",
      "[epoch 130] loss: 604.99\n",
      "[epoch 131] loss: 603.00\n",
      "[epoch 132] loss: 601.28\n",
      "[epoch 133] loss: 600.00\n",
      "[epoch 134] loss: 597.98\n",
      "[epoch 135] loss: 596.07\n",
      "[epoch 136] loss: 594.31\n",
      "[epoch 137] loss: 592.43\n",
      "[epoch 138] loss: 591.28\n",
      "[epoch 139] loss: 589.43\n",
      "[epoch 140] loss: 587.77\n",
      "[epoch 141] loss: 586.24\n",
      "[epoch 142] loss: 584.70\n",
      "[epoch 143] loss: 583.03\n",
      "[epoch 144] loss: 581.17\n",
      "[epoch 145] loss: 579.89\n",
      "[epoch 146] loss: 578.32\n",
      "[epoch 147] loss: 576.88\n",
      "[epoch 148] loss: 575.20\n",
      "[epoch 149] loss: 573.49\n",
      "[epoch 150] loss: 571.87\n",
      "FOLD 3\n",
      "[epoch 1] loss: 201723.32\n",
      "[epoch 2] loss: 5839.57\n",
      "[epoch 3] loss: 3380.44\n",
      "[epoch 4] loss: 2616.68\n",
      "[epoch 5] loss: 2272.29\n",
      "[epoch 6] loss: 2063.28\n",
      "[epoch 7] loss: 1918.59\n",
      "[epoch 8] loss: 1811.02\n",
      "[epoch 9] loss: 1726.68\n",
      "[epoch 10] loss: 1657.81\n",
      "[epoch 11] loss: 1600.05\n",
      "[epoch 12] loss: 1550.71\n",
      "[epoch 13] loss: 1508.01\n",
      "[epoch 14] loss: 1470.66\n",
      "[epoch 15] loss: 1437.71\n",
      "[epoch 16] loss: 1408.42\n",
      "[epoch 17] loss: 1382.20\n",
      "[epoch 18] loss: 1358.60\n",
      "[epoch 19] loss: 1337.23\n",
      "[epoch 20] loss: 1317.81\n",
      "[epoch 21] loss: 1300.06\n",
      "[epoch 22] loss: 1283.79\n",
      "[epoch 23] loss: 1268.82\n",
      "[epoch 24] loss: 1255.00\n",
      "[epoch 25] loss: 1242.20\n",
      "[epoch 26] loss: 1230.33\n",
      "[epoch 27] loss: 1219.28\n",
      "[epoch 28] loss: 1208.98\n",
      "[epoch 29] loss: 1199.35\n",
      "[epoch 30] loss: 1190.34\n",
      "[epoch 31] loss: 1181.88\n",
      "[epoch 32] loss: 1173.94\n",
      "[epoch 33] loss: 1166.46\n",
      "[epoch 34] loss: 1159.42\n",
      "[epoch 35] loss: 1152.76\n",
      "[epoch 36] loss: 1146.46\n",
      "[epoch 37] loss: 1140.50\n",
      "[epoch 38] loss: 1134.83\n",
      "[epoch 39] loss: 1129.46\n",
      "[epoch 40] loss: 1124.34\n",
      "[epoch 41] loss: 1119.47\n",
      "[epoch 42] loss: 1114.82\n",
      "[epoch 43] loss: 1110.38\n",
      "[epoch 44] loss: 1106.13\n",
      "[epoch 45] loss: 1102.06\n",
      "[epoch 46] loss: 1098.16\n",
      "[epoch 47] loss: 1094.41\n",
      "[epoch 48] loss: 1090.81\n",
      "[epoch 49] loss: 1087.35\n",
      "[epoch 50] loss: 1084.01\n",
      "[epoch 51] loss: 1080.80\n",
      "[epoch 52] loss: 1077.70\n",
      "[epoch 53] loss: 1074.70\n",
      "[epoch 54] loss: 1071.80\n",
      "[epoch 55] loss: 1069.00\n",
      "[epoch 56] loss: 1066.28\n",
      "[epoch 57] loss: 1063.65\n",
      "[epoch 58] loss: 1061.10\n",
      "[epoch 59] loss: 1058.63\n",
      "[epoch 60] loss: 1056.22\n",
      "[epoch 61] loss: 1053.88\n",
      "[epoch 62] loss: 1051.61\n",
      "[epoch 63] loss: 1049.39\n",
      "[epoch 64] loss: 1047.23\n",
      "[epoch 65] loss: 1045.13\n",
      "[epoch 66] loss: 1043.08\n",
      "[epoch 67] loss: 1041.08\n",
      "[epoch 68] loss: 1039.13\n",
      "[epoch 69] loss: 1037.22\n",
      "[epoch 70] loss: 1035.36\n",
      "[epoch 71] loss: 1033.54\n",
      "[epoch 72] loss: 1031.76\n",
      "[epoch 73] loss: 1030.01\n",
      "[epoch 74] loss: 1028.31\n",
      "[epoch 75] loss: 1026.64\n",
      "[epoch 76] loss: 1025.00\n",
      "[epoch 77] loss: 1023.39\n",
      "[epoch 78] loss: 1021.82\n",
      "[epoch 79] loss: 1020.28\n",
      "[epoch 80] loss: 1018.77\n",
      "[epoch 81] loss: 1017.28\n",
      "[epoch 82] loss: 1015.82\n",
      "[epoch 83] loss: 1014.39\n",
      "[epoch 84] loss: 1012.98\n",
      "[epoch 85] loss: 1011.60\n",
      "[epoch 86] loss: 1010.24\n",
      "[epoch 87] loss: 1008.91\n",
      "[epoch 88] loss: 1007.59\n",
      "[epoch 89] loss: 1006.30\n",
      "[epoch 90] loss: 1005.03\n",
      "[epoch 91] loss: 1003.78\n",
      "[epoch 92] loss: 1002.55\n",
      "[epoch 93] loss: 1001.34\n",
      "[epoch 94] loss: 1000.15\n",
      "[epoch 95] loss: 998.97\n",
      "[epoch 96] loss: 997.81\n",
      "[epoch 97] loss: 996.67\n",
      "[epoch 98] loss: 995.55\n",
      "[epoch 99] loss: 994.44\n",
      "[epoch 100] loss: 993.35\n",
      "[epoch 101] loss: 992.27\n",
      "[epoch 102] loss: 991.21\n",
      "[epoch 103] loss: 990.17\n",
      "[epoch 104] loss: 989.13\n",
      "[epoch 105] loss: 988.11\n",
      "[epoch 106] loss: 987.11\n",
      "[epoch 107] loss: 986.12\n",
      "[epoch 108] loss: 985.14\n",
      "[epoch 109] loss: 984.17\n",
      "[epoch 110] loss: 983.22\n",
      "[epoch 111] loss: 982.28\n",
      "[epoch 112] loss: 981.35\n",
      "[epoch 113] loss: 980.43\n",
      "[epoch 114] loss: 979.52\n",
      "[epoch 115] loss: 978.63\n",
      "[epoch 116] loss: 977.74\n",
      "[epoch 117] loss: 976.87\n",
      "[epoch 118] loss: 976.00\n",
      "[epoch 119] loss: 975.15\n",
      "[epoch 120] loss: 974.31\n",
      "[epoch 121] loss: 973.47\n",
      "[epoch 122] loss: 972.65\n",
      "[epoch 123] loss: 971.84\n",
      "[epoch 124] loss: 971.03\n",
      "[epoch 125] loss: 970.22\n",
      "[epoch 126] loss: 969.44\n",
      "[epoch 127] loss: 968.65\n",
      "[epoch 128] loss: 967.88\n",
      "[epoch 129] loss: 967.09\n",
      "[epoch 130] loss: 966.33\n",
      "[epoch 131] loss: 965.49\n",
      "[epoch 132] loss: 964.67\n",
      "[epoch 133] loss: 963.84\n",
      "[epoch 134] loss: 963.02\n",
      "[epoch 135] loss: 962.19\n",
      "[epoch 136] loss: 961.36\n",
      "[epoch 137] loss: 960.50\n",
      "[epoch 138] loss: 959.63\n",
      "[epoch 139] loss: 958.83\n",
      "[epoch 140] loss: 957.88\n",
      "[epoch 141] loss: 956.91\n",
      "[epoch 142] loss: 956.06\n",
      "[epoch 143] loss: 955.08\n",
      "[epoch 144] loss: 954.18\n",
      "[epoch 145] loss: 953.02\n",
      "[epoch 146] loss: 951.93\n",
      "[epoch 147] loss: 950.80\n",
      "[epoch 148] loss: 949.59\n",
      "[epoch 149] loss: 948.40\n",
      "[epoch 150] loss: 947.25\n",
      "FOLD 4\n",
      "[epoch 1] loss: 209691.42\n",
      "[epoch 2] loss: 6241.92\n",
      "[epoch 3] loss: 3626.96\n",
      "[epoch 4] loss: 2755.96\n",
      "[epoch 5] loss: 2368.62\n",
      "[epoch 6] loss: 2137.14\n",
      "[epoch 7] loss: 1977.63\n",
      "[epoch 8] loss: 1859.38\n",
      "[epoch 9] loss: 1767.21\n",
      "[epoch 10] loss: 1692.37\n",
      "[epoch 11] loss: 1629.77\n",
      "[epoch 12] loss: 1576.32\n",
      "[epoch 13] loss: 1530.05\n",
      "[epoch 14] loss: 1489.56\n",
      "[epoch 15] loss: 1453.84\n",
      "[epoch 16] loss: 1422.11\n",
      "[epoch 17] loss: 1393.74\n",
      "[epoch 18] loss: 1368.26\n",
      "[epoch 19] loss: 1345.24\n",
      "[epoch 20] loss: 1324.36\n",
      "[epoch 21] loss: 1305.34\n",
      "[epoch 22] loss: 1287.94\n",
      "[epoch 23] loss: 1271.97\n",
      "[epoch 24] loss: 1257.26\n",
      "[epoch 25] loss: 1243.67\n",
      "[epoch 26] loss: 1231.07\n",
      "[epoch 27] loss: 1219.37\n",
      "[epoch 28] loss: 1208.48\n",
      "[epoch 29] loss: 1198.31\n",
      "[epoch 30] loss: 1188.79\n",
      "[epoch 31] loss: 1179.88\n",
      "[epoch 32] loss: 1171.50\n",
      "[epoch 33] loss: 1163.62\n",
      "[epoch 34] loss: 1156.20\n",
      "[epoch 35] loss: 1149.19\n",
      "[epoch 36] loss: 1142.56\n",
      "[epoch 37] loss: 1136.28\n",
      "[epoch 38] loss: 1130.33\n",
      "[epoch 39] loss: 1124.68\n",
      "[epoch 40] loss: 1119.30\n",
      "[epoch 41] loss: 1114.18\n",
      "[epoch 42] loss: 1109.30\n",
      "[epoch 43] loss: 1104.64\n",
      "[epoch 44] loss: 1100.19\n",
      "[epoch 45] loss: 1095.93\n",
      "[epoch 46] loss: 1091.84\n",
      "[epoch 47] loss: 1087.92\n",
      "[epoch 48] loss: 1084.16\n",
      "[epoch 49] loss: 1080.54\n",
      "[epoch 50] loss: 1077.06\n",
      "[epoch 51] loss: 1073.71\n",
      "[epoch 52] loss: 1070.48\n",
      "[epoch 53] loss: 1067.36\n",
      "[epoch 54] loss: 1064.35\n",
      "[epoch 55] loss: 1061.43\n",
      "[epoch 56] loss: 1058.62\n",
      "[epoch 57] loss: 1055.89\n",
      "[epoch 58] loss: 1053.24\n",
      "[epoch 59] loss: 1050.68\n",
      "[epoch 60] loss: 1048.19\n",
      "[epoch 61] loss: 1045.77\n",
      "[epoch 62] loss: 1043.42\n",
      "[epoch 63] loss: 1041.13\n",
      "[epoch 64] loss: 1038.91\n",
      "[epoch 65] loss: 1036.75\n",
      "[epoch 66] loss: 1034.64\n",
      "[epoch 67] loss: 1032.58\n",
      "[epoch 68] loss: 1030.57\n",
      "[epoch 69] loss: 1028.61\n",
      "[epoch 70] loss: 1026.70\n",
      "[epoch 71] loss: 1024.84\n",
      "[epoch 72] loss: 1023.01\n",
      "[epoch 73] loss: 1021.23\n",
      "[epoch 74] loss: 1019.48\n",
      "[epoch 75] loss: 1017.77\n",
      "[epoch 76] loss: 1016.10\n",
      "[epoch 77] loss: 1014.46\n",
      "[epoch 78] loss: 1012.86\n",
      "[epoch 79] loss: 1011.29\n",
      "[epoch 80] loss: 1009.75\n",
      "[epoch 81] loss: 1008.24\n",
      "[epoch 82] loss: 1006.75\n",
      "[epoch 83] loss: 1005.30\n",
      "[epoch 84] loss: 1003.87\n",
      "[epoch 85] loss: 1002.47\n",
      "[epoch 86] loss: 1001.09\n",
      "[epoch 87] loss: 999.74\n",
      "[epoch 88] loss: 998.40\n",
      "[epoch 89] loss: 997.10\n",
      "[epoch 90] loss: 995.81\n",
      "[epoch 91] loss: 994.55\n",
      "[epoch 92] loss: 993.30\n",
      "[epoch 93] loss: 992.08\n",
      "[epoch 94] loss: 990.88\n",
      "[epoch 95] loss: 989.69\n",
      "[epoch 96] loss: 988.53\n",
      "[epoch 97] loss: 987.38\n",
      "[epoch 98] loss: 986.25\n",
      "[epoch 99] loss: 985.13\n",
      "[epoch 100] loss: 984.03\n",
      "[epoch 101] loss: 982.95\n",
      "[epoch 102] loss: 981.89\n",
      "[epoch 103] loss: 980.84\n",
      "[epoch 104] loss: 979.80\n",
      "[epoch 105] loss: 978.78\n",
      "[epoch 106] loss: 977.77\n",
      "[epoch 107] loss: 976.78\n",
      "[epoch 108] loss: 975.80\n",
      "[epoch 109] loss: 974.83\n",
      "[epoch 110] loss: 973.87\n",
      "[epoch 111] loss: 972.93\n",
      "[epoch 112] loss: 972.00\n",
      "[epoch 113] loss: 971.09\n",
      "[epoch 114] loss: 970.18\n",
      "[epoch 115] loss: 969.29\n",
      "[epoch 116] loss: 968.40\n",
      "[epoch 117] loss: 967.53\n",
      "[epoch 118] loss: 966.67\n",
      "[epoch 119] loss: 965.82\n",
      "[epoch 120] loss: 964.98\n",
      "[epoch 121] loss: 964.15\n",
      "[epoch 122] loss: 963.32\n",
      "[epoch 123] loss: 962.51\n",
      "[epoch 124] loss: 961.71\n",
      "[epoch 125] loss: 960.92\n",
      "[epoch 126] loss: 960.14\n",
      "[epoch 127] loss: 959.36\n",
      "[epoch 128] loss: 958.60\n",
      "[epoch 129] loss: 957.84\n",
      "[epoch 130] loss: 957.09\n",
      "[epoch 131] loss: 956.35\n",
      "[epoch 132] loss: 955.62\n",
      "[epoch 133] loss: 954.89\n",
      "[epoch 134] loss: 954.18\n",
      "[epoch 135] loss: 953.47\n",
      "[epoch 136] loss: 952.77\n",
      "[epoch 137] loss: 952.07\n",
      "[epoch 138] loss: 951.39\n",
      "[epoch 139] loss: 950.71\n",
      "[epoch 140] loss: 950.03\n",
      "[epoch 141] loss: 949.37\n",
      "[epoch 142] loss: 948.71\n",
      "[epoch 143] loss: 948.06\n",
      "[epoch 144] loss: 947.41\n",
      "[epoch 145] loss: 946.77\n",
      "[epoch 146] loss: 946.14\n",
      "[epoch 147] loss: 945.51\n",
      "[epoch 148] loss: 944.89\n",
      "[epoch 149] loss: 944.28\n",
      "[epoch 150] loss: 943.67\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "\n",
    "for idx, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(\"FOLD {}\".format(idx+1))\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    train_input = torch.Tensor(X_train)\n",
    "    test_input = torch.Tensor(X_test)\n",
    "    train_target = torch.Tensor(y_train.reshape(len(y_train), 1))\n",
    "    test_target = torch.Tensor(y_test.reshape(len(y_test), 1))\n",
    "    \n",
    "    model = Net_4(nb_input_neurons) \n",
    "    losses = train_model(model, train_input, train_target, mini_batch_size, monitor_loss=True)\n",
    "\n",
    "    #Make predictions\n",
    "    y_hat = compute_pred(model, test_input)\n",
    "    #Compute score\n",
    "    mse_nn, mae_nn, r2_nn = compute_score(y_test, y_hat.detach().numpy())\n",
    "    \n",
    "    mse_storage.append(mse_nn)\n",
    "    mae_storage.append(mae_nn)\n",
    "    r2_storage.append(r2_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('metrics_all_dataset2_net4_4fold.pkl', 'wb') as f:\n",
    "    pickle.dump([mse_storage, mae_storage, r2_storage], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.42 +/- 0.0220 \n",
      "MAE: 0.48 +/- 0.0126 \n",
      "R2: 0.95 +/- 0.0019\n"
     ]
    }
   ],
   "source": [
    "print('MSE: {:0.2f} +/- {:0.4f} \\nMAE: {:0.2f} +/- {:0.4f} \\nR2: {:0.2f} +/- {:0.4f}'.format(np.mean(mse_storage), np.std(mse_storage), np.mean(mae_storage), np.std(mae_storage), np.mean(r2_storage), np.std(r2_storage)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Keras pipeline\n",
    "<a id='main_keras'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1280 samples, validate on 320 samples\n",
      "Epoch 1/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 20.5533 - mean_absolute_error: 20.5534 - val_loss: 10.6779 - val_mean_absolute_error: 10.6779\n",
      "Epoch 2/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 3.4412 - mean_absolute_error: 3.4412 - val_loss: 2.1116 - val_mean_absolute_error: 2.1116\n",
      "Epoch 3/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.8856 - mean_absolute_error: 1.8856 - val_loss: 1.7832 - val_mean_absolute_error: 1.7832\n",
      "Epoch 4/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.6338 - mean_absolute_error: 1.6338 - val_loss: 1.6364 - val_mean_absolute_error: 1.6364\n",
      "Epoch 5/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.4684 - mean_absolute_error: 1.4684 - val_loss: 1.4507 - val_mean_absolute_error: 1.4507\n",
      "Epoch 6/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.3487 - mean_absolute_error: 1.3487 - val_loss: 1.3620 - val_mean_absolute_error: 1.3620\n",
      "Epoch 7/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.2593 - mean_absolute_error: 1.2593 - val_loss: 1.3047 - val_mean_absolute_error: 1.3047\n",
      "Epoch 8/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.1838 - mean_absolute_error: 1.1838 - val_loss: 1.2502 - val_mean_absolute_error: 1.2502\n",
      "Epoch 9/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.1258 - mean_absolute_error: 1.1258 - val_loss: 1.2066 - val_mean_absolute_error: 1.2066\n",
      "Epoch 10/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.0773 - mean_absolute_error: 1.0773 - val_loss: 1.1537 - val_mean_absolute_error: 1.1537\n",
      "Epoch 11/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.0469 - mean_absolute_error: 1.0469 - val_loss: 1.1248 - val_mean_absolute_error: 1.1248\n",
      "Epoch 12/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 1.0024 - mean_absolute_error: 1.0024 - val_loss: 1.0928 - val_mean_absolute_error: 1.0928\n",
      "Epoch 13/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.9663 - mean_absolute_error: 0.9663 - val_loss: 1.0636 - val_mean_absolute_error: 1.0636\n",
      "Epoch 14/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.9486 - mean_absolute_error: 0.9486 - val_loss: 1.0322 - val_mean_absolute_error: 1.0322\n",
      "Epoch 15/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.9272 - mean_absolute_error: 0.9272 - val_loss: 1.0828 - val_mean_absolute_error: 1.0828\n",
      "Epoch 16/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.9107 - mean_absolute_error: 0.9107 - val_loss: 1.0040 - val_mean_absolute_error: 1.0040\n",
      "Epoch 17/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8827 - mean_absolute_error: 0.8827 - val_loss: 0.9901 - val_mean_absolute_error: 0.9901\n",
      "Epoch 18/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8781 - mean_absolute_error: 0.8781 - val_loss: 0.9749 - val_mean_absolute_error: 0.9749\n",
      "Epoch 19/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8446 - mean_absolute_error: 0.8446 - val_loss: 0.9781 - val_mean_absolute_error: 0.9781\n",
      "Epoch 20/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8415 - mean_absolute_error: 0.8415 - val_loss: 0.9896 - val_mean_absolute_error: 0.9896\n",
      "Epoch 21/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8250 - mean_absolute_error: 0.8250 - val_loss: 0.9458 - val_mean_absolute_error: 0.9458\n",
      "Epoch 22/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8086 - mean_absolute_error: 0.8086 - val_loss: 0.9559 - val_mean_absolute_error: 0.9559\n",
      "Epoch 23/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.8053 - mean_absolute_error: 0.8053 - val_loss: 0.9622 - val_mean_absolute_error: 0.9622\n",
      "Epoch 24/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7821 - mean_absolute_error: 0.7821 - val_loss: 0.9340 - val_mean_absolute_error: 0.9340\n",
      "Epoch 25/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7826 - mean_absolute_error: 0.7826 - val_loss: 0.9283 - val_mean_absolute_error: 0.9283\n",
      "Epoch 26/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7803 - mean_absolute_error: 0.7803 - val_loss: 0.9226 - val_mean_absolute_error: 0.9226\n",
      "Epoch 27/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7768 - mean_absolute_error: 0.7768 - val_loss: 0.9319 - val_mean_absolute_error: 0.9319\n",
      "Epoch 28/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7615 - mean_absolute_error: 0.7615 - val_loss: 0.9995 - val_mean_absolute_error: 0.9995\n",
      "Epoch 29/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7684 - mean_absolute_error: 0.7684 - val_loss: 0.9637 - val_mean_absolute_error: 0.9637\n",
      "Epoch 30/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7622 - mean_absolute_error: 0.7622 - val_loss: 0.9131 - val_mean_absolute_error: 0.9131\n",
      "Epoch 31/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7485 - mean_absolute_error: 0.7485 - val_loss: 0.9188 - val_mean_absolute_error: 0.9188\n",
      "Epoch 32/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7424 - mean_absolute_error: 0.7424 - val_loss: 0.9017 - val_mean_absolute_error: 0.9017\n",
      "Epoch 33/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7434 - mean_absolute_error: 0.7434 - val_loss: 0.9331 - val_mean_absolute_error: 0.9331\n",
      "Epoch 34/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7264 - mean_absolute_error: 0.7264 - val_loss: 0.9186 - val_mean_absolute_error: 0.9186\n",
      "Epoch 35/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7198 - mean_absolute_error: 0.7198 - val_loss: 0.9513 - val_mean_absolute_error: 0.9513\n",
      "Epoch 36/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7331 - mean_absolute_error: 0.7331 - val_loss: 0.9123 - val_mean_absolute_error: 0.9123\n",
      "Epoch 37/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7221 - mean_absolute_error: 0.7221 - val_loss: 0.9004 - val_mean_absolute_error: 0.9004\n",
      "Epoch 38/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7271 - mean_absolute_error: 0.7271 - val_loss: 0.8990 - val_mean_absolute_error: 0.8990\n",
      "Epoch 39/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7113 - mean_absolute_error: 0.7113 - val_loss: 0.9280 - val_mean_absolute_error: 0.9280\n",
      "Epoch 40/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.7045 - mean_absolute_error: 0.7045 - val_loss: 0.9209 - val_mean_absolute_error: 0.9209\n",
      "Epoch 41/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6925 - mean_absolute_error: 0.6925 - val_loss: 0.8963 - val_mean_absolute_error: 0.8963\n",
      "Epoch 42/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6879 - mean_absolute_error: 0.6879 - val_loss: 0.9023 - val_mean_absolute_error: 0.9023\n",
      "Epoch 43/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6798 - mean_absolute_error: 0.6798 - val_loss: 0.9094 - val_mean_absolute_error: 0.9094\n",
      "Epoch 44/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6825 - mean_absolute_error: 0.6825 - val_loss: 0.9322 - val_mean_absolute_error: 0.9322 - loss: 0.6855 - mean_absolut\n",
      "Epoch 45/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6736 - mean_absolute_error: 0.6736 - val_loss: 0.9072 - val_mean_absolute_error: 0.9072\n",
      "Epoch 46/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6767 - mean_absolute_error: 0.6767 - val_loss: 0.9403 - val_mean_absolute_error: 0.9403\n",
      "Epoch 47/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6700 - mean_absolute_error: 0.6700 - val_loss: 0.9227 - val_mean_absolute_error: 0.9227\n",
      "Epoch 48/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6590 - mean_absolute_error: 0.6590 - val_loss: 0.9006 - val_mean_absolute_error: 0.9006\n",
      "Epoch 49/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6623 - mean_absolute_error: 0.6623 - val_loss: 0.9027 - val_mean_absolute_error: 0.9027\n",
      "Epoch 50/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6640 - mean_absolute_error: 0.6640 - val_loss: 0.8982 - val_mean_absolute_error: 0.8982\n",
      "Epoch 51/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6573 - mean_absolute_error: 0.6573 - val_loss: 0.8995 - val_mean_absolute_error: 0.8995\n",
      "Epoch 52/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6508 - mean_absolute_error: 0.6508 - val_loss: 0.8898 - val_mean_absolute_error: 0.8898\n",
      "Epoch 53/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6592 - mean_absolute_error: 0.6592 - val_loss: 0.9011 - val_mean_absolute_error: 0.9011\n",
      "Epoch 54/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6454 - mean_absolute_error: 0.6454 - val_loss: 0.9154 - val_mean_absolute_error: 0.9154\n",
      "Epoch 55/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6478 - mean_absolute_error: 0.6478 - val_loss: 0.8987 - val_mean_absolute_error: 0.8987\n",
      "Epoch 56/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6349 - mean_absolute_error: 0.6349 - val_loss: 0.8987 - val_mean_absolute_error: 0.8987\n",
      "Epoch 57/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6553 - mean_absolute_error: 0.6553 - val_loss: 0.9073 - val_mean_absolute_error: 0.9073\n",
      "Epoch 58/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6306 - mean_absolute_error: 0.6306 - val_loss: 0.9502 - val_mean_absolute_error: 0.9502\n",
      "Epoch 59/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6256 - mean_absolute_error: 0.6256 - val_loss: 0.8995 - val_mean_absolute_error: 0.8995\n",
      "Epoch 60/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6298 - mean_absolute_error: 0.6298 - val_loss: 0.8885 - val_mean_absolute_error: 0.8885\n",
      "Epoch 61/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6302 - mean_absolute_error: 0.6302 - val_loss: 0.8880 - val_mean_absolute_error: 0.8880\n",
      "Epoch 62/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6201 - mean_absolute_error: 0.6201 - val_loss: 0.8977 - val_mean_absolute_error: 0.8977\n",
      "Epoch 63/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6212 - mean_absolute_error: 0.6212 - val_loss: 0.9090 - val_mean_absolute_error: 0.9090\n",
      "Epoch 64/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6221 - mean_absolute_error: 0.6221 - val_loss: 0.9033 - val_mean_absolute_error: 0.9033\n",
      "Epoch 65/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6105 - mean_absolute_error: 0.6105 - val_loss: 0.9020 - val_mean_absolute_error: 0.9020\n",
      "Epoch 66/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6172 - mean_absolute_error: 0.6172 - val_loss: 0.8895 - val_mean_absolute_error: 0.8895\n",
      "Epoch 67/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6110 - mean_absolute_error: 0.6110 - val_loss: 0.8952 - val_mean_absolute_error: 0.8952\n",
      "Epoch 68/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6133 - mean_absolute_error: 0.6133 - val_loss: 0.8986 - val_mean_absolute_error: 0.8986\n",
      "Epoch 69/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6068 - mean_absolute_error: 0.6068 - val_loss: 0.8976 - val_mean_absolute_error: 0.8976\n",
      "Epoch 70/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6164 - mean_absolute_error: 0.6164 - val_loss: 0.9015 - val_mean_absolute_error: 0.9015\n",
      "Epoch 71/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6087 - mean_absolute_error: 0.6087 - val_loss: 0.9093 - val_mean_absolute_error: 0.9093\n",
      "Epoch 72/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6060 - mean_absolute_error: 0.6060 - val_loss: 0.9320 - val_mean_absolute_error: 0.9320\n",
      "Epoch 73/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6069 - mean_absolute_error: 0.6069 - val_loss: 0.8818 - val_mean_absolute_error: 0.8818\n",
      "Epoch 74/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6012 - mean_absolute_error: 0.6012 - val_loss: 0.8895 - val_mean_absolute_error: 0.8895\n",
      "Epoch 75/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5948 - mean_absolute_error: 0.5948 - val_loss: 0.9510 - val_mean_absolute_error: 0.9510\n",
      "Epoch 76/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.6030 - mean_absolute_error: 0.6030 - val_loss: 0.8884 - val_mean_absolute_error: 0.8884\n",
      "Epoch 77/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5905 - mean_absolute_error: 0.5905 - val_loss: 0.9292 - val_mean_absolute_error: 0.9292\n",
      "Epoch 78/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5869 - mean_absolute_error: 0.5869 - val_loss: 0.8999 - val_mean_absolute_error: 0.8999\n",
      "Epoch 79/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5858 - mean_absolute_error: 0.5858 - val_loss: 0.9045 - val_mean_absolute_error: 0.9045\n",
      "Epoch 80/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5877 - mean_absolute_error: 0.5877 - val_loss: 0.9128 - val_mean_absolute_error: 0.9128\n",
      "Epoch 81/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5888 - mean_absolute_error: 0.5888 - val_loss: 0.9007 - val_mean_absolute_error: 0.9007\n",
      "Epoch 82/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5773 - mean_absolute_error: 0.5773 - val_loss: 0.8913 - val_mean_absolute_error: 0.8913\n",
      "Epoch 83/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5893 - mean_absolute_error: 0.5893 - val_loss: 0.9104 - val_mean_absolute_error: 0.9104\n",
      "Epoch 84/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5784 - mean_absolute_error: 0.5784 - val_loss: 0.9112 - val_mean_absolute_error: 0.9112\n",
      "Epoch 85/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5794 - mean_absolute_error: 0.5794 - val_loss: 0.9158 - val_mean_absolute_error: 0.9158\n",
      "Epoch 86/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5762 - mean_absolute_error: 0.5762 - val_loss: 0.9056 - val_mean_absolute_error: 0.9056\n",
      "Epoch 87/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5746 - mean_absolute_error: 0.5746 - val_loss: 0.9235 - val_mean_absolute_error: 0.9235\n",
      "Epoch 88/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5826 - mean_absolute_error: 0.5826 - val_loss: 0.9050 - val_mean_absolute_error: 0.9050\n",
      "Epoch 89/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5721 - mean_absolute_error: 0.5721 - val_loss: 0.9014 - val_mean_absolute_error: 0.9014\n",
      "Epoch 90/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5667 - mean_absolute_error: 0.5667 - val_loss: 0.9011 - val_mean_absolute_error: 0.9011\n",
      "Epoch 91/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5798 - mean_absolute_error: 0.5798 - val_loss: 0.9099 - val_mean_absolute_error: 0.9099\n",
      "Epoch 92/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5689 - mean_absolute_error: 0.5689 - val_loss: 0.9198 - val_mean_absolute_error: 0.9198\n",
      "Epoch 93/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5734 - mean_absolute_error: 0.5734 - val_loss: 0.8973 - val_mean_absolute_error: 0.8973\n",
      "Epoch 94/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5832 - mean_absolute_error: 0.5832 - val_loss: 0.9035 - val_mean_absolute_error: 0.9035\n",
      "Epoch 95/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5598 - mean_absolute_error: 0.5598 - val_loss: 0.8960 - val_mean_absolute_error: 0.8960\n",
      "Epoch 96/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5584 - mean_absolute_error: 0.5584 - val_loss: 0.8992 - val_mean_absolute_error: 0.8992\n",
      "Epoch 97/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5649 - mean_absolute_error: 0.5649 - val_loss: 0.9141 - val_mean_absolute_error: 0.9141\n",
      "Epoch 98/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5523 - mean_absolute_error: 0.5523 - val_loss: 0.9010 - val_mean_absolute_error: 0.9010\n",
      "Epoch 99/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5598 - mean_absolute_error: 0.5598 - val_loss: 0.9065 - val_mean_absolute_error: 0.9065\n",
      "Epoch 100/100\n",
      "1280/1280 [==============================] - 2s 2ms/step - loss: 0.5583 - mean_absolute_error: 0.5583 - val_loss: 0.9037 - val_mean_absolute_error: 0.9037\n",
      "Train on 1280 samples, validate on 320 samples\n",
      "Epoch 1/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 18.8452 - val_loss: 4.8843\n",
      "Epoch 2/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.3295 - val_loss: 1.9703\n",
      "Epoch 3/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7548 - val_loss: 1.6349\n",
      "Epoch 4/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5050 - val_loss: 1.5179\n",
      "Epoch 5/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.3417 - val_loss: 1.3470\n",
      "Epoch 6/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.2435 - val_loss: 1.2814\n",
      "Epoch 7/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.1624 - val_loss: 1.2135\n",
      "Epoch 8/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.0929 - val_loss: 1.1724\n",
      "Epoch 9/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.0303 - val_loss: 1.1081\n",
      "Epoch 10/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.9942 - val_loss: 1.0730\n",
      "Epoch 11/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.9517 - val_loss: 1.0579\n",
      "Epoch 12/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.9239 - val_loss: 1.0167\n",
      "Epoch 13/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.9062 - val_loss: 0.9932\n",
      "Epoch 14/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.8659 - val_loss: 0.9700\n",
      "Epoch 15/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.8520 - val_loss: 0.9597\n",
      "Epoch 16/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.8371 - val_loss: 0.9447\n",
      "Epoch 17/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.8404 - val_loss: 0.9816\n",
      "Epoch 18/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.8127 - val_loss: 0.9249\n",
      "Epoch 19/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7918 - val_loss: 0.9186\n",
      "Epoch 20/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7891 - val_loss: 0.9090\n",
      "Epoch 21/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7738 - val_loss: 0.9247\n",
      "Epoch 22/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7743 - val_loss: 0.9096\n",
      "Epoch 23/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7558 - val_loss: 0.9220\n",
      "Epoch 24/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7526 - val_loss: 0.8966\n",
      "Epoch 25/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7420 - val_loss: 0.9184\n",
      "Epoch 26/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7467 - val_loss: 0.9166\n",
      "Epoch 27/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7442 - val_loss: 0.8978\n",
      "Epoch 28/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7223 - val_loss: 0.9736\n",
      "Epoch 29/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7249 - val_loss: 0.9099\n",
      "Epoch 30/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7342 - val_loss: 0.9018\n",
      "Epoch 31/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7155 - val_loss: 0.9038\n",
      "Epoch 32/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7000 - val_loss: 0.9975\n",
      "Epoch 33/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.7012 - val_loss: 0.9412\n",
      "Epoch 34/100\n",
      "1280/1280 [==============================] - ETA: 0s - loss: 0.694 - 4s 3ms/step - loss: 0.6972 - val_loss: 0.9703\n",
      "Epoch 35/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6858 - val_loss: 0.8939\n",
      "Epoch 36/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6753 - val_loss: 0.9570\n",
      "Epoch 37/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6838 - val_loss: 0.8904\n",
      "Epoch 38/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6713 - val_loss: 0.8973\n",
      "Epoch 39/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6640 - val_loss: 0.9024\n",
      "Epoch 40/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6539 - val_loss: 0.8935\n",
      "Epoch 41/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6496 - val_loss: 0.9160\n",
      "Epoch 42/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6515 - val_loss: 0.8943\n",
      "Epoch 43/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6604 - val_loss: 0.8869\n",
      "Epoch 44/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6407 - val_loss: 0.8865\n",
      "Epoch 45/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6523 - val_loss: 0.9133\n",
      "Epoch 46/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6403 - val_loss: 0.9136\n",
      "Epoch 47/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6299 - val_loss: 0.8866\n",
      "Epoch 48/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6220 - val_loss: 0.9075\n",
      "Epoch 49/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6228 - val_loss: 0.8990\n",
      "Epoch 50/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6243 - val_loss: 0.8925\n",
      "Epoch 51/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6345 - val_loss: 0.8851\n",
      "Epoch 52/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6146 - val_loss: 0.8885\n",
      "Epoch 53/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6098 - val_loss: 0.9135\n",
      "Epoch 54/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6050 - val_loss: 0.9256\n",
      "Epoch 55/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6026 - val_loss: 0.8935\n",
      "Epoch 56/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6097 - val_loss: 0.8855\n",
      "Epoch 57/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6011 - val_loss: 0.9168\n",
      "Epoch 58/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5927 - val_loss: 0.8822\n",
      "Epoch 59/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5980 - val_loss: 0.8858\n",
      "Epoch 60/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5879 - val_loss: 0.8794\n",
      "Epoch 61/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.6019 - val_loss: 0.8807\n",
      "Epoch 62/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5886 - val_loss: 0.8682\n",
      "Epoch 63/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5831 - val_loss: 0.8810\n",
      "Epoch 64/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5827 - val_loss: 0.9095\n",
      "Epoch 65/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5827 - val_loss: 0.8965\n",
      "Epoch 66/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5763 - val_loss: 0.8906\n",
      "Epoch 67/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5685 - val_loss: 0.9132\n",
      "Epoch 68/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5721 - val_loss: 0.8764\n",
      "Epoch 69/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5777 - val_loss: 0.8837\n",
      "Epoch 70/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5801 - val_loss: 0.9155\n",
      "Epoch 71/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5817 - val_loss: 0.8808\n",
      "Epoch 72/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5718 - val_loss: 0.9199\n",
      "Epoch 73/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5659 - val_loss: 0.9320\n",
      "Epoch 74/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5639 - val_loss: 0.8825\n",
      "Epoch 75/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5630 - val_loss: 0.8947\n",
      "Epoch 76/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5505 - val_loss: 0.8850\n",
      "Epoch 77/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5594 - val_loss: 0.9336\n",
      "Epoch 78/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5656 - val_loss: 0.9256\n",
      "Epoch 79/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5480 - val_loss: 0.8779\n",
      "Epoch 80/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5472 - val_loss: 0.8944\n",
      "Epoch 81/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5412 - val_loss: 0.8974\n",
      "Epoch 82/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5406 - val_loss: 0.9095\n",
      "Epoch 83/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5404 - val_loss: 0.9141\n",
      "Epoch 84/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5386 - val_loss: 0.8927\n",
      "Epoch 85/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5339 - val_loss: 0.8939\n",
      "Epoch 86/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5542 - val_loss: 0.8851\n",
      "Epoch 87/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5330 - val_loss: 0.9109\n",
      "Epoch 88/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5290 - val_loss: 0.8923\n",
      "Epoch 89/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5273 - val_loss: 0.9155\n",
      "Epoch 90/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5279 - val_loss: 0.9020\n",
      "Epoch 91/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5239 - val_loss: 0.8970\n",
      "Epoch 92/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5209 - val_loss: 0.8979\n",
      "Epoch 93/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5192 - val_loss: 0.9030\n",
      "Epoch 94/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5181 - val_loss: 0.9405\n",
      "Epoch 95/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5284 - val_loss: 0.9256\n",
      "Epoch 96/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5191 - val_loss: 0.9009\n",
      "Epoch 97/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5153 - val_loss: 0.9028\n",
      "Epoch 98/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5029 - val_loss: 0.8929\n",
      "Epoch 99/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5145 - val_loss: 0.9048\n",
      "Epoch 100/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 0.5070 - val_loss: 0.9202\n",
      "Train on 1280 samples, validate on 320 samples\n",
      "Epoch 1/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 21.9362 - val_loss: 21.0831\n",
      "Epoch 2/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 10.6694 - val_loss: 4.9718\n",
      "Epoch 3/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 4.1598 - val_loss: 3.9340\n",
      "Epoch 4/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 3.2858 - val_loss: 3.5508\n",
      "Epoch 5/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.9754 - val_loss: 2.8293\n",
      "Epoch 6/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.9384 - val_loss: 2.2671\n",
      "Epoch 7/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.6124 - val_loss: 2.2294\n",
      "Epoch 8/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.5956 - val_loss: 1.9236\n",
      "Epoch 9/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.6133 - val_loss: 1.2556\n",
      "Epoch 10/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.4611 - val_loss: 1.4869\n",
      "Epoch 11/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.5565 - val_loss: 1.2181\n",
      "Epoch 12/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.3193 - val_loss: 2.0742\n",
      "Epoch 13/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.4152 - val_loss: 1.5683\n",
      "Epoch 14/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.3199 - val_loss: 1.9536\n",
      "Epoch 15/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.2937 - val_loss: 1.5653\n",
      "Epoch 16/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.3405 - val_loss: 1.2393\n",
      "Epoch 17/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.2545 - val_loss: 1.1195\n",
      "Epoch 18/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.1344 - val_loss: 1.1949\n",
      "Epoch 19/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.2316 - val_loss: 1.1902\n",
      "Epoch 20/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.0960 - val_loss: 1.5547\n",
      "Epoch 21/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.1313 - val_loss: 1.4119\n",
      "Epoch 22/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.0559 - val_loss: 1.0102\n",
      "Epoch 23/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.1752 - val_loss: 1.2884\n",
      "Epoch 24/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.1634 - val_loss: 1.2553\n",
      "Epoch 25/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9911 - val_loss: 1.1318\n",
      "Epoch 26/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.0460 - val_loss: 1.2042\n",
      "Epoch 27/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.0200 - val_loss: 1.0775\n",
      "Epoch 28/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.0328 - val_loss: 1.0665\n",
      "Epoch 29/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9516 - val_loss: 1.0490\n",
      "Epoch 30/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 2.0006 - val_loss: 0.9345\n",
      "Epoch 31/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9600 - val_loss: 1.1783\n",
      "Epoch 32/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9171 - val_loss: 1.1449\n",
      "Epoch 33/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9090 - val_loss: 0.9420\n",
      "Epoch 34/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9830 - val_loss: 0.9518\n",
      "Epoch 35/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8611 - val_loss: 0.9754\n",
      "Epoch 36/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9569 - val_loss: 1.0691\n",
      "Epoch 37/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8941 - val_loss: 0.9009\n",
      "Epoch 38/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.9785 - val_loss: 1.0169\n",
      "Epoch 39/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8517 - val_loss: 1.3498\n",
      "Epoch 40/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8961 - val_loss: 1.1343\n",
      "Epoch 41/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8304 - val_loss: 0.9606\n",
      "Epoch 42/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8578 - val_loss: 1.0208\n",
      "Epoch 43/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8504 - val_loss: 0.9355\n",
      "Epoch 44/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8637 - val_loss: 1.4083\n",
      "Epoch 45/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8517 - val_loss: 1.3353\n",
      "Epoch 46/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8388 - val_loss: 0.9508\n",
      "Epoch 47/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8552 - val_loss: 1.1281\n",
      "Epoch 48/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8456 - val_loss: 0.9294\n",
      "Epoch 49/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8169 - val_loss: 1.0934\n",
      "Epoch 50/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8308 - val_loss: 1.1383\n",
      "Epoch 51/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7614 - val_loss: 0.9391\n",
      "Epoch 52/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7698 - val_loss: 0.8672\n",
      "Epoch 53/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7587 - val_loss: 1.6818\n",
      "Epoch 54/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7704 - val_loss: 1.4286\n",
      "Epoch 55/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7162 - val_loss: 0.9472\n",
      "Epoch 56/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7757 - val_loss: 0.9156\n",
      "Epoch 57/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7820 - val_loss: 1.0793\n",
      "Epoch 58/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7642 - val_loss: 0.9951\n",
      "Epoch 59/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.8157 - val_loss: 0.9489\n",
      "Epoch 60/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7290 - val_loss: 1.1018\n",
      "Epoch 61/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6372 - val_loss: 1.0664\n",
      "Epoch 62/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7434 - val_loss: 0.9636\n",
      "Epoch 63/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6715 - val_loss: 0.9984\n",
      "Epoch 64/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7047 - val_loss: 1.1442\n",
      "Epoch 65/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7028 - val_loss: 0.9360\n",
      "Epoch 66/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6986 - val_loss: 0.8796\n",
      "Epoch 67/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7504 - val_loss: 0.9445\n",
      "Epoch 68/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6265 - val_loss: 1.0664\n",
      "Epoch 69/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6449 - val_loss: 0.8823\n",
      "Epoch 70/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7221 - val_loss: 1.0630\n",
      "Epoch 71/100\n",
      "1280/1280 [==============================] - 3s 3ms/step - loss: 1.7181 - val_loss: 0.9589\n",
      "Epoch 72/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6745 - val_loss: 1.2331\n",
      "Epoch 73/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6898 - val_loss: 1.0802\n",
      "Epoch 74/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5732 - val_loss: 0.9860\n",
      "Epoch 75/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5998 - val_loss: 0.9259\n",
      "Epoch 76/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.7093 - val_loss: 0.9657\n",
      "Epoch 77/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5907 - val_loss: 1.0717\n",
      "Epoch 78/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5309 - val_loss: 0.9039\n",
      "Epoch 79/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6774 - val_loss: 0.9090\n",
      "Epoch 80/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6279 - val_loss: 0.9330\n",
      "Epoch 81/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6408 - val_loss: 0.8968\n",
      "Epoch 82/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5419 - val_loss: 1.0997\n",
      "Epoch 83/100\n",
      "1280/1280 [==============================] - 3s 3ms/step - loss: 1.6346 - val_loss: 0.8592\n",
      "Epoch 84/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5298 - val_loss: 1.0378\n",
      "Epoch 85/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5996 - val_loss: 0.8719\n",
      "Epoch 86/100\n",
      "1280/1280 [==============================] - 3s 3ms/step - loss: 1.5407 - val_loss: 0.9061\n",
      "Epoch 87/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6536 - val_loss: 0.9594\n",
      "Epoch 88/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5870 - val_loss: 0.9579\n",
      "Epoch 89/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6459 - val_loss: 0.8652\n",
      "Epoch 90/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5481 - val_loss: 0.9130\n",
      "Epoch 91/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5439 - val_loss: 0.9201\n",
      "Epoch 92/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6152 - val_loss: 0.9055\n",
      "Epoch 93/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5744 - val_loss: 0.8613\n",
      "Epoch 94/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5480 - val_loss: 0.9054\n",
      "Epoch 95/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5386 - val_loss: 0.8726\n",
      "Epoch 96/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5859 - val_loss: 1.0245\n",
      "Epoch 97/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6000 - val_loss: 1.2286\n",
      "Epoch 98/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.6297 - val_loss: 0.9892\n",
      "Epoch 99/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5165 - val_loss: 0.9072\n",
      "Epoch 100/100\n",
      "1280/1280 [==============================] - 4s 3ms/step - loss: 1.5887 - val_loss: 0.9081\n"
     ]
    }
   ],
   "source": [
    "models = [k_models.model_5, k_models.model_6, k_models.model_7]\n",
    "\n",
    "model_names = [] #[m.__name__ for m in models]\n",
    "epochs_list = []\n",
    "mse_list = []\n",
    "mae_list = []\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_data_train_test(2000, X_tot, y_tot)\n",
    "\n",
    "nb_epochs = [100]\n",
    "for m in models:\n",
    "    for e in nb_epochs:\n",
    "        network = m(X_train, 'mean_absolute_error')\n",
    "        network.fit(X_train, y_train, epochs=e, batch_size=10, validation_split=0.2)\n",
    "\n",
    "        y_hat = network.predict(X_test)\n",
    "        mse_nn, mae_nn = compute_score(y_test, y_hat)\n",
    "\n",
    "        model_names.append(m.__name__)\n",
    "        epochs_list.append(e)\n",
    "        mse_list.append(mse_nn)\n",
    "        mae_list.append(mae_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            model_5     model_6     model_7 \n",
      "\n",
      " Epochs  100         100         100        \n",
      "\n",
      " MSE       1.01402     0.944624    1.2443   \n",
      "\n",
      " MAE       0.768377    0.754768    0.866663 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([model_names, ['Epochs']+epochs_list, ['MSE']+mse_list, ['MAE']+mae_list], headers=\"firstrow\", tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick tests\n",
    "apply_pca = False\n",
    "apply_iqr = True\n",
    "\n",
    "if apply_iqr:\n",
    "    X_train, X_test, y_train, y_test = load_data_train_test(12000, X_tot, y_tot, iqr=apply_iqr)\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = load_data_train_test(12000, X_tot, y_tot)\n",
    "\n",
    "if apply_pca:\n",
    "    n = 100\n",
    "    X_train, X_test = do_PCA(X_train, X_test, n)\n",
    "    nb_input_neurons = n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7680 samples, validate on 1920 samples\n",
      "Epoch 1/150\n",
      "7680/7680 [==============================] - 27s 3ms/step - loss: 4.3517 - val_loss: 1.1326\n",
      "Epoch 2/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 1.0146 - val_loss: 0.8708\n",
      "Epoch 3/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 0.8197 - val_loss: 0.7872\n",
      "Epoch 4/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.7380 - val_loss: 0.7370\n",
      "Epoch 5/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.6953 - val_loss: 0.6850\n",
      "Epoch 6/150\n",
      "7680/7680 [==============================] - 28s 4ms/step - loss: 0.6662 - val_loss: 0.6687\n",
      "Epoch 7/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 0.6464 - val_loss: 0.6513\n",
      "Epoch 8/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.6306 - val_loss: 0.6370\n",
      "Epoch 9/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.6140 - val_loss: 0.6369\n",
      "Epoch 10/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 0.6005 - val_loss: 0.6448\n",
      "Epoch 11/150\n",
      "7680/7680 [==============================] - 24s 3ms/step - loss: 0.5937 - val_loss: 0.6185\n",
      "Epoch 12/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.5808 - val_loss: 0.6438\n",
      "Epoch 13/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.5702 - val_loss: 0.6016\n",
      "Epoch 14/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.5661 - val_loss: 0.6178\n",
      "Epoch 15/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.5595 - val_loss: 0.5951\n",
      "Epoch 16/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 0.5458 - val_loss: 0.5860\n",
      "Epoch 17/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.5400 - val_loss: 0.6213\n",
      "Epoch 18/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.5352 - val_loss: 0.5938\n",
      "Epoch 19/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.5322 - val_loss: 0.5697\n",
      "Epoch 20/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.5291 - val_loss: 0.5731\n",
      "Epoch 21/150\n",
      "7680/7680 [==============================] - 25s 3ms/step - loss: 0.5247 - val_loss: 0.5779\n",
      "Epoch 22/150\n",
      "7680/7680 [==============================] - 26s 3ms/step - loss: 0.5207 - val_loss: 0.5709\n",
      "Epoch 23/150\n",
      "7680/7680 [==============================] - 26s 3ms/step - loss: 0.5111 - val_loss: 0.5703\n",
      "Epoch 24/150\n",
      "7680/7680 [==============================] - 26s 3ms/step - loss: 0.5122 - val_loss: 0.5708\n",
      "Epoch 25/150\n",
      "7680/7680 [==============================] - 24s 3ms/step - loss: 0.5086 - val_loss: 0.5652\n",
      "Epoch 26/150\n",
      "7680/7680 [==============================] - 25s 3ms/step - loss: 0.5010 - val_loss: 0.5613\n",
      "Epoch 27/150\n",
      "7680/7680 [==============================] - 27s 3ms/step - loss: 0.4959 - val_loss: 0.5688\n",
      "Epoch 28/150\n",
      "7680/7680 [==============================] - 25s 3ms/step - loss: 0.5056 - val_loss: 0.5712\n",
      "Epoch 29/150\n",
      "7680/7680 [==============================] - 32s 4ms/step - loss: 0.4928 - val_loss: 0.5826\n",
      "Epoch 30/150\n",
      "7680/7680 [==============================] - 27s 4ms/step - loss: 0.4942 - val_loss: 0.5526\n",
      "Epoch 31/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4893 - val_loss: 0.5643\n",
      "Epoch 32/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4875 - val_loss: 0.5527\n",
      "Epoch 33/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4889 - val_loss: 0.5541\n",
      "Epoch 34/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4843 - val_loss: 0.5538\n",
      "Epoch 35/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4794 - val_loss: 0.5501\n",
      "Epoch 36/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4793 - val_loss: 0.5533\n",
      "Epoch 37/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4794 - val_loss: 0.5496\n",
      "Epoch 38/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4790 - val_loss: 0.5770\n",
      "Epoch 39/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4691 - val_loss: 0.5657\n",
      "Epoch 40/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4670 - val_loss: 0.5523\n",
      "Epoch 41/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4724 - val_loss: 0.5714\n",
      "Epoch 42/150\n",
      "7680/7680 [==============================] - 24s 3ms/step - loss: 0.4634 - val_loss: 0.5437\n",
      "Epoch 43/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4632 - val_loss: 0.5801\n",
      "Epoch 44/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4629 - val_loss: 0.5658\n",
      "Epoch 45/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4655 - val_loss: 0.5407\n",
      "Epoch 46/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4596 - val_loss: 0.5432\n",
      "Epoch 47/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4570 - val_loss: 0.5400\n",
      "Epoch 48/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4590 - val_loss: 0.5716\n",
      "Epoch 49/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4620 - val_loss: 0.6188\n",
      "Epoch 50/150\n",
      "7680/7680 [==============================] - 25s 3ms/step - loss: 0.4519 - val_loss: 0.5896\n",
      "Epoch 51/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4595 - val_loss: 0.5511\n",
      "Epoch 52/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4500 - val_loss: 0.5539\n",
      "Epoch 53/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4443 - val_loss: 0.5684\n",
      "Epoch 54/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4489 - val_loss: 0.5926\n",
      "Epoch 55/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4452 - val_loss: 0.5668\n",
      "Epoch 56/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4410 - val_loss: 0.5497\n",
      "Epoch 57/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4469 - val_loss: 0.5646\n",
      "Epoch 58/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4499 - val_loss: 0.5402\n",
      "Epoch 59/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4437 - val_loss: 0.5362\n",
      "Epoch 60/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4432 - val_loss: 0.5575\n",
      "Epoch 61/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4388 - val_loss: 0.5347\n",
      "Epoch 62/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4412 - val_loss: 0.5401\n",
      "Epoch 63/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4405 - val_loss: 0.5477\n",
      "Epoch 64/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4344 - val_loss: 0.5390\n",
      "Epoch 65/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4376 - val_loss: 0.5747\n",
      "Epoch 66/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4343 - val_loss: 0.5600\n",
      "Epoch 67/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4296 - val_loss: 0.5437\n",
      "Epoch 68/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4306 - val_loss: 0.5831\n",
      "Epoch 69/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4345 - val_loss: 0.6191\n",
      "Epoch 70/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4385 - val_loss: 0.5409\n",
      "Epoch 71/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4301 - val_loss: 0.5396\n",
      "Epoch 72/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4304 - val_loss: 0.5517\n",
      "Epoch 73/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4284 - val_loss: 0.5441\n",
      "Epoch 74/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4322 - val_loss: 0.5373\n",
      "Epoch 75/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4269 - val_loss: 0.5354\n",
      "Epoch 76/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4273 - val_loss: 0.5365\n",
      "Epoch 77/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4269 - val_loss: 0.5409\n",
      "Epoch 78/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4241 - val_loss: 0.5347\n",
      "Epoch 79/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4297 - val_loss: 0.5357\n",
      "Epoch 80/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4233 - val_loss: 0.5425\n",
      "Epoch 81/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4249 - val_loss: 0.5616\n",
      "Epoch 82/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4247 - val_loss: 0.5373\n",
      "Epoch 83/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4194 - val_loss: 0.5848\n",
      "Epoch 84/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4280 - val_loss: 0.5360\n",
      "Epoch 85/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4175 - val_loss: 0.5304\n",
      "Epoch 86/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4266 - val_loss: 0.5533\n",
      "Epoch 87/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4179 - val_loss: 0.5327\n",
      "Epoch 88/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4223 - val_loss: 0.5325\n",
      "Epoch 89/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4133 - val_loss: 0.5910\n",
      "Epoch 90/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4191 - val_loss: 0.5628\n",
      "Epoch 91/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4160 - val_loss: 0.5758\n",
      "Epoch 92/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4141 - val_loss: 0.5253\n",
      "Epoch 93/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4142 - val_loss: 0.5455\n",
      "Epoch 94/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 0.4177 - val_loss: 0.5700\n",
      "Epoch 95/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4138 - val_loss: 0.5318\n",
      "Epoch 96/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4152 - val_loss: 0.5523\n",
      "Epoch 97/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4158 - val_loss: 0.5749\n",
      "Epoch 98/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4095 - val_loss: 0.5516\n",
      "Epoch 99/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4130 - val_loss: 0.5440\n",
      "Epoch 100/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4109 - val_loss: 0.5351\n",
      "Epoch 101/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4167 - val_loss: 0.5329\n",
      "Epoch 102/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4066 - val_loss: 0.5391\n",
      "Epoch 103/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4071 - val_loss: 0.5538\n",
      "Epoch 104/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4097 - val_loss: 0.5388\n",
      "Epoch 105/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4115 - val_loss: 0.5352\n",
      "Epoch 106/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4086 - val_loss: 0.5520\n",
      "Epoch 107/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4156 - val_loss: 0.5346\n",
      "Epoch 108/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4067 - val_loss: 0.5387\n",
      "Epoch 109/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4106 - val_loss: 0.5554\n",
      "Epoch 110/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4053 - val_loss: 0.5339\n",
      "Epoch 111/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4091 - val_loss: 0.5254\n",
      "Epoch 112/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4096 - val_loss: 0.5432\n",
      "Epoch 113/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4046 - val_loss: 0.5603\n",
      "Epoch 114/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4054 - val_loss: 0.5843\n",
      "Epoch 115/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4117 - val_loss: 0.5337\n",
      "Epoch 116/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4076 - val_loss: 0.5339\n",
      "Epoch 117/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4081 - val_loss: 0.5346\n",
      "Epoch 118/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4041 - val_loss: 0.5430\n",
      "Epoch 119/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4039 - val_loss: 0.5296\n",
      "Epoch 120/150\n",
      "7680/7680 [==============================] - 21s 3ms/step - loss: 0.4007 - val_loss: 0.5362\n",
      "Epoch 121/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4099 - val_loss: 0.5773\n",
      "Epoch 122/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4040 - val_loss: 0.6434\n",
      "Epoch 123/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4040 - val_loss: 0.5409\n",
      "Epoch 124/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4082 - val_loss: 0.5318\n",
      "Epoch 125/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4047 - val_loss: 0.5287\n",
      "Epoch 126/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4008 - val_loss: 0.5337\n",
      "Epoch 127/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4019 - val_loss: 0.5473\n",
      "Epoch 128/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.3997 - val_loss: 0.5272\n",
      "Epoch 129/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.3977 - val_loss: 0.5296\n",
      "Epoch 130/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.4032 - val_loss: 0.5574\n",
      "Epoch 131/150\n",
      "7680/7680 [==============================] - 20s 3ms/step - loss: 0.3975 - val_loss: 0.5309\n",
      "Epoch 132/150\n",
      "7680/7680 [==============================] - 25s 3ms/step - loss: 0.3988 - val_loss: 0.5904\n",
      "Epoch 133/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4003 - val_loss: 0.5401\n",
      "Epoch 134/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3983 - val_loss: 0.5326\n",
      "Epoch 135/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3963 - val_loss: 0.5345\n",
      "Epoch 136/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4005 - val_loss: 0.5383\n",
      "Epoch 137/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3957 - val_loss: 0.5797\n",
      "Epoch 138/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.4038 - val_loss: 0.5557\n",
      "Epoch 139/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3976 - val_loss: 0.5414\n",
      "Epoch 140/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3942 - val_loss: 0.5449\n",
      "Epoch 141/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3957 - val_loss: 0.5513\n",
      "Epoch 142/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3944 - val_loss: 0.5498\n",
      "Epoch 143/150\n",
      "7680/7680 [==============================] - 23s 3ms/step - loss: 0.3943 - val_loss: 0.5328\n",
      "Epoch 144/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3936 - val_loss: 0.5315\n",
      "Epoch 145/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3965 - val_loss: 0.5308\n",
      "Epoch 146/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3920 - val_loss: 0.5547\n",
      "Epoch 147/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3971 - val_loss: 0.5512\n",
      "Epoch 148/150\n",
      "7680/7680 [==============================] - 22s 3ms/step - loss: 0.3891 - val_loss: 0.5330\n",
      "Epoch 149/150\n",
      "7680/7680 [==============================] - 24s 3ms/step - loss: 0.4047 - val_loss: 0.5488\n",
      "Epoch 150/150\n",
      "7680/7680 [==============================] - 24s 3ms/step - loss: 0.3939 - val_loss: 0.5453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2bd418eae10>"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_ann = k_models.model_8(X_train, 'mean_absolute_error') \n",
    "k_ann.fit(X_train, y_train, epochs=150, batch_size=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.58 \n",
      "MAE: 0.57\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "y_hat = k_ann.predict(X_test)\n",
    "\n",
    "# compute score\n",
    "mse_nn, mae_nn, _ = compute_score(y_test, y_hat)\n",
    "print('MSE: {:.2f} \\nMAE: {:.2f}'.format(mse_nn, mae_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('y_hat_12000_dataset2_model8.pkl', 'wb') as f:\n",
    "    pickle.dump(y_hat, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results tracking**\n",
    "\n",
    "Model    | MSE  | MAE  | PCA | IQR |    n     | dataset | *n* PCs | Epochs | Scaling | \n",
    "---      | ---  | ---  | --- | --- |   ---    |   ---   |   ---   |   ---  |   ---   |\n",
    "model_6  | 0.93 | 0.70 | Yes | Yes |  20000   |   2     |   40    |   150  |   No    |\n",
    "model_6  | 0.85 | 0.67 | No  | Yes |   4000   |   2     |    -    |   150  |   No    |\n",
    "model_6  | 0.78 | 0.64 | Yes | Yes |  20000   |   2     |   60    |   150  |   No    | \n",
    "model_6  | 0.75 | 0.63 | Yes | Yes |  25000   |   2     |   70    |   200  |   No    |\n",
    "model_6  | 0.67 | 0.60 | No  | Yes |   8000   |   2     |    -    |   150  |   No    |\n",
    "model_6  | 0.75 | 0.62 | Yes | Yes |  25000   |   2     |  100    |   150  |   No    |\n",
    "model_6  | 0.60 | 0.55 | No  | Yes |  12000   |   2     |    -    |   150  |   No    |\n",
    "Net_3    | 1.01 | 0.74 | Yes | Yes |   5000   |   2     |   80    |   100  |   No    |\n",
    "Net_3    | 1.03 | 0.73 | Yes | Yes |   8000   |   2     |   80    |   150  |   No    |\n",
    "Net_3    | 0.65 | 0.60 | No  | Yes |   8000   |   2     |    -    |   150  |   No    |\n",
    "Net_3    | 0.52 | 0.54 | No  | Yes |  12000   |   2     |    -    |   150  |   No    |\n",
    "model_8  | 0.58 | 0.57 | No  | Yes |  12000   |   2     |    -    |   150  |   No    |\n",
    "Net_3    | 0.57 | 0.56 | No  | Yes |  12000   |   3     |    -    |   150  |   No    |\n",
    "Net_3    | 0.65 | 0.62 | No  | Yes |  12000   |   2     |    -    |   150  |   Yes   |\n",
    "Net_3    | 1.00 | 0.76 | Yes | Yes |  12000   |   2     |   80    |   150  |   Yes   |\n",
    "Net_3    | 0.86 | 0.69 | Yes | Yes |  25000   |   2     |   80    |   150  |   Yes   |\n",
    "Net_4    | 0.47 | 0.52 | No  | Yes |  12000   |   2     |   -     |   150  |   Yes   |\n",
    "Net_5    | 0.54 | 0.54 | No  | Yes |  12000   |   2     |   -     |   150  |   Yes   |\n",
    "\n",
    "Note: \n",
    "- Larger increase in performances when using more PCs (say 60).\n",
    "- Obviously computationally less demanding\n",
    "- Will the increase in number of samples without PCA perform better? YES it does.\n",
    "- Loss evolution indicates that 150 epochs are not necessary (try with 120)\n",
    "- Plateau reached using larger number of PCs\n",
    "- $r^2 = 0.95$ for Net_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stats\n",
    "<a id='stats'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 IQR vs. sample number\n",
    "<a id='iqr_vs_n'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I should do a function but j'ai la flemme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 287us/step - loss: 20.0163 - val_loss: 7.9898\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 2.1726 - val_loss: 1.1877\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 1.0646 - val_loss: 0.9910\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.9258 - val_loss: 0.8876\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.8713 - val_loss: 0.8659\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.8536 - val_loss: 0.8499\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.8411 - val_loss: 0.8589\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.8331 - val_loss: 0.8565\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.8326 - val_loss: 0.8517\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.8289 - val_loss: 0.8619\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.8315 - val_loss: 0.8580\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.8284 - val_loss: 0.8839\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.8276 - val_loss: 0.8578\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 185us/step - loss: 0.8262 - val_loss: 0.8565\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 187us/step - loss: 0.8234 - val_loss: 0.8550\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 185us/step - loss: 0.8218 - val_loss: 0.8637\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.8203 - val_loss: 0.8654\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 195us/step - loss: 0.8249 - val_loss: 0.8537\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 181us/step - loss: 0.8196 - val_loss: 0.8522\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.8175 - val_loss: 0.8494\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 181us/step - loss: 0.8189 - val_loss: 0.8510\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.8130 - val_loss: 0.8459\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.8115 - val_loss: 0.8501\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.8087 - val_loss: 0.8495\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.8069 - val_loss: 0.8427\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.8080 - val_loss: 0.8414\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.8061 - val_loss: 0.8522\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.8016 - val_loss: 0.8409\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7983 - val_loss: 0.8705\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7991 - val_loss: 0.8329\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7937 - val_loss: 0.8308\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7925 - val_loss: 0.8290\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7902 - val_loss: 0.8265\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7882 - val_loss: 0.8243\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7854 - val_loss: 0.8280\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7852 - val_loss: 0.8237\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7822 - val_loss: 0.8217\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7798 - val_loss: 0.8380\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7824 - val_loss: 0.8184\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7768 - val_loss: 0.8152\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7756 - val_loss: 0.8149\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7689 - val_loss: 0.8147\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7663 - val_loss: 0.8097\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7664 - val_loss: 0.8070\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7636 - val_loss: 0.8069\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7603 - val_loss: 0.8057\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7607 - val_loss: 0.8039\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 185us/step - loss: 0.7592 - val_loss: 0.7992\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 194us/step - loss: 0.7594 - val_loss: 0.7998\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7536 - val_loss: 0.8180\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7510 - val_loss: 0.7949\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7557 - val_loss: 0.7951\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7491 - val_loss: 0.8020\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 199us/step - loss: 0.7492 - val_loss: 0.7872\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7476 - val_loss: 0.8102\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7464 - val_loss: 0.7925\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7436 - val_loss: 0.7915\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7381 - val_loss: 0.7885\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7400 - val_loss: 0.7872\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7419 - val_loss: 0.7938\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7356 - val_loss: 0.7853\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7365 - val_loss: 0.7846\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 179us/step - loss: 0.7343 - val_loss: 0.7871\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7346 - val_loss: 0.7882\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7333 - val_loss: 0.7803\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7310 - val_loss: 0.7772\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7293 - val_loss: 0.7733\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7275 - val_loss: 0.7920\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 187us/step - loss: 0.7246 - val_loss: 0.7810\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.7224 - val_loss: 0.7719\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7225 - val_loss: 0.7775\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.7231 - val_loss: 0.7733\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 181us/step - loss: 0.7203 - val_loss: 0.8159\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 200us/step - loss: 0.7181 - val_loss: 0.7725\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 193us/step - loss: 0.7240 - val_loss: 0.7762\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 194us/step - loss: 0.7181 - val_loss: 0.7717\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 242us/step - loss: 0.7193 - val_loss: 0.7742\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7169 - val_loss: 0.7762\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7129 - val_loss: 0.7686\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.7127 - val_loss: 0.7704\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.7105 - val_loss: 0.7700\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7133 - val_loss: 0.7780\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 179us/step - loss: 0.7084 - val_loss: 0.7688\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7110 - val_loss: 0.7686\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7097 - val_loss: 0.7681\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7073 - val_loss: 0.7755\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7085 - val_loss: 0.7650\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7098 - val_loss: 0.7698\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7076 - val_loss: 0.7663\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7056 - val_loss: 0.7771\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7043 - val_loss: 0.7711\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7024 - val_loss: 0.7642\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7011 - val_loss: 0.7725\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7073 - val_loss: 0.7868\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6984 - val_loss: 0.7672\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6978 - val_loss: 0.7613\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7003 - val_loss: 0.7802\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 181us/step - loss: 0.6998 - val_loss: 0.7655\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6970 - val_loss: 0.7676\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6975 - val_loss: 0.7742\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6971 - val_loss: 0.7676\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 290us/step - loss: 0.6976 - val_loss: 0.7651\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6936 - val_loss: 0.7629\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 181us/step - loss: 0.6944 - val_loss: 0.7562\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6928 - val_loss: 0.7575\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6892 - val_loss: 0.7613\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6892 - val_loss: 0.7596\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6894 - val_loss: 0.7568\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6911 - val_loss: 0.7557\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6886 - val_loss: 0.7734\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6874 - val_loss: 0.7612\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6899 - val_loss: 0.7583\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6887 - val_loss: 0.7786\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6887 - val_loss: 0.7578\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6875 - val_loss: 0.7518\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6893 - val_loss: 0.7562\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6857 - val_loss: 0.7545\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6835 - val_loss: 0.7568\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6860 - val_loss: 0.7538\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6793 - val_loss: 0.7582\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6794 - val_loss: 0.7717\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6840 - val_loss: 0.7573\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6808 - val_loss: 0.7567\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6827 - val_loss: 0.7575\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6801 - val_loss: 0.7563\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6802 - val_loss: 0.7571\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6777 - val_loss: 0.7591\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6772 - val_loss: 0.7639\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6757 - val_loss: 0.7538\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6734 - val_loss: 0.7735\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6756 - val_loss: 0.7574\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6735 - val_loss: 0.7533\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6786 - val_loss: 0.7531\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6747 - val_loss: 0.7676\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6720 - val_loss: 0.7564\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6738 - val_loss: 0.7556\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6726 - val_loss: 0.7531\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6784 - val_loss: 0.7657\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6740 - val_loss: 0.7525\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6715 - val_loss: 0.7631\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6683 - val_loss: 0.7489\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6728 - val_loss: 0.7548\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6671 - val_loss: 0.7529\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6688 - val_loss: 0.7497\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6678 - val_loss: 0.7816\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6677 - val_loss: 0.7643\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6688 - val_loss: 0.7523\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6664 - val_loss: 0.7489\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6676 - val_loss: 0.7442\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6698 - val_loss: 0.7546\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 283us/step - loss: 19.3701 - val_loss: 6.9150\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 1.9228 - val_loss: 1.1749\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.9935 - val_loss: 0.9364\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.8667 - val_loss: 0.8616\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.8181 - val_loss: 0.8320\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.8010 - val_loss: 0.8137\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7988 - val_loss: 0.8149\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7931 - val_loss: 0.7984\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7862 - val_loss: 0.7959\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7832 - val_loss: 0.7950\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7842 - val_loss: 0.7864\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7836 - val_loss: 0.8022\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7811 - val_loss: 0.7964\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7806 - val_loss: 0.7806\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7789 - val_loss: 0.7863\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7792 - val_loss: 0.7783\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7791 - val_loss: 0.7799\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7734 - val_loss: 0.7794\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7730 - val_loss: 0.7960\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7726 - val_loss: 0.7797\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7689 - val_loss: 0.7761\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7668 - val_loss: 0.7899\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7674 - val_loss: 0.7779\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7643 - val_loss: 0.7777\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7626 - val_loss: 0.7904\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7618 - val_loss: 0.7931\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7587 - val_loss: 0.7688\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7585 - val_loss: 0.7702\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7534 - val_loss: 0.7835\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7512 - val_loss: 0.7709\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7492 - val_loss: 0.7689\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7480 - val_loss: 0.7678\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7433 - val_loss: 0.7657\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7455 - val_loss: 0.7666\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7446 - val_loss: 0.7634\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7410 - val_loss: 0.7612\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7428 - val_loss: 0.7678\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7379 - val_loss: 0.7668\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7348 - val_loss: 0.7871\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7332 - val_loss: 0.7616\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7339 - val_loss: 0.7652\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7306 - val_loss: 0.7578\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7289 - val_loss: 0.7963\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7244 - val_loss: 0.7657\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7252 - val_loss: 0.7548\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7216 - val_loss: 0.7577\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7184 - val_loss: 0.7582\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7198 - val_loss: 0.7561\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7203 - val_loss: 0.7525\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7170 - val_loss: 0.7527\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7104 - val_loss: 0.7541\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7128 - val_loss: 0.7531\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7093 - val_loss: 0.7517\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7079 - val_loss: 0.7461\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7096 - val_loss: 0.7571\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7073 - val_loss: 0.7503\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7075 - val_loss: 0.7561\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7059 - val_loss: 0.7535\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7004 - val_loss: 0.7467\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7027 - val_loss: 0.7602\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6979 - val_loss: 0.7496\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6977 - val_loss: 0.7556\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7021 - val_loss: 0.7689\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6972 - val_loss: 0.7480\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6977 - val_loss: 0.7519\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6946 - val_loss: 0.7658\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6931 - val_loss: 0.7476\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6903 - val_loss: 0.7632\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6906 - val_loss: 0.7381\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6894 - val_loss: 0.7357\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6864 - val_loss: 0.7491\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6889 - val_loss: 0.7496\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6861 - val_loss: 0.7377\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6919 - val_loss: 0.7333\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6831 - val_loss: 0.7348\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6826 - val_loss: 0.7397\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6812 - val_loss: 0.7363\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6822 - val_loss: 0.7403\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6843 - val_loss: 0.7367\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6813 - val_loss: 0.7324\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6813 - val_loss: 0.7370\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6753 - val_loss: 0.7468\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6811 - val_loss: 0.7401\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6757 - val_loss: 0.7393\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6739 - val_loss: 0.7640\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6788 - val_loss: 0.7538\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6697 - val_loss: 0.7385\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6766 - val_loss: 0.7312\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6704 - val_loss: 0.7349\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6713 - val_loss: 0.7531\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6700 - val_loss: 0.7435\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6670 - val_loss: 0.7422\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6674 - val_loss: 0.7719\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6683 - val_loss: 0.7400\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6687 - val_loss: 0.7332\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6666 - val_loss: 0.7624\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6684 - val_loss: 0.7370\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6634 - val_loss: 0.7358\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6681 - val_loss: 0.7452\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6612 - val_loss: 0.7359\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6605 - val_loss: 0.7346\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6655 - val_loss: 0.7470\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6599 - val_loss: 0.7355\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6605 - val_loss: 0.7521\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6580 - val_loss: 0.7387\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6574 - val_loss: 0.7390\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6629 - val_loss: 0.7389\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6606 - val_loss: 0.7344\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6557 - val_loss: 0.7405\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6573 - val_loss: 0.7315\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6563 - val_loss: 0.7335\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6559 - val_loss: 0.7377\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6526 - val_loss: 0.7294\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6535 - val_loss: 0.7329\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6564 - val_loss: 0.7298\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6555 - val_loss: 0.7542\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6526 - val_loss: 0.7366\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6541 - val_loss: 0.7349\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6524 - val_loss: 0.7389\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6499 - val_loss: 0.7287\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6484 - val_loss: 0.7331\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6497 - val_loss: 0.7356\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6508 - val_loss: 0.7487\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6539 - val_loss: 0.7456\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6466 - val_loss: 0.7594\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6475 - val_loss: 0.7361\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6466 - val_loss: 0.7350\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6441 - val_loss: 0.7354\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6461 - val_loss: 0.7383\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6482 - val_loss: 0.7433\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6460 - val_loss: 0.7444\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6480 - val_loss: 0.7378\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6457 - val_loss: 0.7406\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6427 - val_loss: 0.7324\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6451 - val_loss: 0.7337\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6481 - val_loss: 0.7448\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6438 - val_loss: 0.7329\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6444 - val_loss: 0.7388\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6419 - val_loss: 0.7507\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6449 - val_loss: 0.7522\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6423 - val_loss: 0.7406\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6435 - val_loss: 0.7725\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6462 - val_loss: 0.7420\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6400 - val_loss: 0.7397\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6386 - val_loss: 0.7389\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6383 - val_loss: 0.7509\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6427 - val_loss: 0.7373\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6416 - val_loss: 0.7449\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6401 - val_loss: 0.7384\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6386 - val_loss: 0.7363\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 290us/step - loss: 20.2210 - val_loss: 9.1806\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 2.4785 - val_loss: 1.1528\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 1.0547 - val_loss: 0.9443\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.9072 - val_loss: 0.8665\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 180us/step - loss: 0.8487 - val_loss: 0.8495\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.8220 - val_loss: 0.8276\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.8148 - val_loss: 0.8285\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.8117 - val_loss: 0.8282\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.8065 - val_loss: 0.8261\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.8019 - val_loss: 0.8238\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7996 - val_loss: 0.8220\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7975 - val_loss: 0.8246\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7955 - val_loss: 0.8202\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7975 - val_loss: 0.8359\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7929 - val_loss: 0.8301\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7901 - val_loss: 0.8166\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7858 - val_loss: 0.8155\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7877 - val_loss: 0.8204\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7839 - val_loss: 0.8121\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7780 - val_loss: 0.8101\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7748 - val_loss: 0.8232\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7725 - val_loss: 0.8072\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7703 - val_loss: 0.8250\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7667 - val_loss: 0.8027\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7615 - val_loss: 0.8040\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7602 - val_loss: 0.7934\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7594 - val_loss: 0.8174\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7560 - val_loss: 0.7924\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7555 - val_loss: 0.7986\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7558 - val_loss: 0.7949\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7444 - val_loss: 0.8059\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7443 - val_loss: 0.7863\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7411 - val_loss: 0.7848\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7390 - val_loss: 0.7812\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7337 - val_loss: 0.7884\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7325 - val_loss: 0.7770\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7321 - val_loss: 0.7774\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7307 - val_loss: 0.7795\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7267 - val_loss: 0.7668\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7228 - val_loss: 0.7720\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7244 - val_loss: 0.7679\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7210 - val_loss: 0.7720\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7177 - val_loss: 0.7640\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7162 - val_loss: 0.7632\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7114 - val_loss: 0.7588\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7155 - val_loss: 0.7695\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7082 - val_loss: 0.7577\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7086 - val_loss: 0.7664\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7050 - val_loss: 0.7569\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7050 - val_loss: 0.7607\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7030 - val_loss: 0.7590\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6985 - val_loss: 0.7541\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6992 - val_loss: 0.7589\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6965 - val_loss: 0.7535\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6922 - val_loss: 0.7460\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6962 - val_loss: 0.7491\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6928 - val_loss: 0.7526\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6892 - val_loss: 0.7462\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6869 - val_loss: 0.7505\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6853 - val_loss: 0.7482\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6864 - val_loss: 0.7465\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6850 - val_loss: 0.7468\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6823 - val_loss: 0.7595\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6819 - val_loss: 0.7518\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6808 - val_loss: 0.7420\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6806 - val_loss: 0.7501\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6782 - val_loss: 0.7453\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6755 - val_loss: 0.7387\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6761 - val_loss: 0.7399\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6726 - val_loss: 0.7448\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6786 - val_loss: 0.7358\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6762 - val_loss: 0.7419\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6718 - val_loss: 0.7456\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6711 - val_loss: 0.7362\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6684 - val_loss: 0.7365\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6664 - val_loss: 0.7437\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6706 - val_loss: 0.7336\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6663 - val_loss: 0.7382\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6681 - val_loss: 0.7376\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6675 - val_loss: 0.7319\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6649 - val_loss: 0.7348\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6594 - val_loss: 0.7371\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6641 - val_loss: 0.7414\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6584 - val_loss: 0.7391\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 179us/step - loss: 0.6601 - val_loss: 0.7325\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6602 - val_loss: 0.7377\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6635 - val_loss: 0.7399\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6587 - val_loss: 0.7476\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6613 - val_loss: 0.7373\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6582 - val_loss: 0.7438\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6544 - val_loss: 0.7347\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6557 - val_loss: 0.7385\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6565 - val_loss: 0.7328\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6534 - val_loss: 0.7372\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6532 - val_loss: 0.7439\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6505 - val_loss: 0.7422\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6503 - val_loss: 0.7404\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6533 - val_loss: 0.7618\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6498 - val_loss: 0.7329\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6469 - val_loss: 0.7348\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6542 - val_loss: 0.7329\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6481 - val_loss: 0.7501\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6509 - val_loss: 0.7549\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6482 - val_loss: 0.7363\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6473 - val_loss: 0.7325\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6469 - val_loss: 0.7330\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6524 - val_loss: 0.7426\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6499 - val_loss: 0.7315\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6523 - val_loss: 0.7431\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6457 - val_loss: 0.7388\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6418 - val_loss: 0.7424\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6421 - val_loss: 0.7320\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6423 - val_loss: 0.7372\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6420 - val_loss: 0.7280\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6401 - val_loss: 0.7297\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6416 - val_loss: 0.7311\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6430 - val_loss: 0.7275\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6399 - val_loss: 0.7363\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6435 - val_loss: 0.7315\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6400 - val_loss: 0.7333\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6388 - val_loss: 0.7464\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6388 - val_loss: 0.7255\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6404 - val_loss: 0.7520\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6395 - val_loss: 0.7275\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6367 - val_loss: 0.7422\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6378 - val_loss: 0.7290\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6383 - val_loss: 0.7362\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6374 - val_loss: 0.7368\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6375 - val_loss: 0.7298\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6441 - val_loss: 0.7461\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6340 - val_loss: 0.7364\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6344 - val_loss: 0.7414\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6360 - val_loss: 0.7353\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6335 - val_loss: 0.7321\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6334 - val_loss: 0.7292\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6329 - val_loss: 0.7259\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6308 - val_loss: 0.7310\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6338 - val_loss: 0.7237\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6321 - val_loss: 0.7295\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6302 - val_loss: 0.7306\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6309 - val_loss: 0.7300\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6311 - val_loss: 0.7269\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6309 - val_loss: 0.7255\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6285 - val_loss: 0.7309\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6287 - val_loss: 0.7238\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6298 - val_loss: 0.7674\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6303 - val_loss: 0.7312\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6297 - val_loss: 0.7328\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6263 - val_loss: 0.7454\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6312 - val_loss: 0.7286\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 286us/step - loss: 20.3486 - val_loss: 9.8560\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 2.6836 - val_loss: 1.2067\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 1.0500 - val_loss: 0.9389\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.8858 - val_loss: 0.8425\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.8228 - val_loss: 0.8246\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.8013 - val_loss: 0.8010\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7957 - val_loss: 0.7925\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7903 - val_loss: 0.7902\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7872 - val_loss: 0.8064\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7872 - val_loss: 0.8018\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7901 - val_loss: 0.8072\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7833 - val_loss: 0.8179\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7826 - val_loss: 0.8005\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7881 - val_loss: 0.7929\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7847 - val_loss: 0.7950\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7842 - val_loss: 0.7971\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7792 - val_loss: 0.7948\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7788 - val_loss: 0.7907\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7754 - val_loss: 0.8025\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7749 - val_loss: 0.7893\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7737 - val_loss: 0.7919\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7750 - val_loss: 0.7897\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7699 - val_loss: 0.7884\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7720 - val_loss: 0.7876\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7681 - val_loss: 0.7883\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7652 - val_loss: 0.7858\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7613 - val_loss: 0.7867\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7619 - val_loss: 0.7864\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7637 - val_loss: 0.7848\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7577 - val_loss: 0.7934\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7607 - val_loss: 0.7855\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7543 - val_loss: 0.8215\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7529 - val_loss: 0.7806\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7493 - val_loss: 0.7800\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7483 - val_loss: 0.7807\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7463 - val_loss: 0.7878\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7424 - val_loss: 0.8172\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7467 - val_loss: 0.7976\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7428 - val_loss: 0.7845\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.7374 - val_loss: 0.7758\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7341 - val_loss: 0.7862\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7345 - val_loss: 0.7741\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7316 - val_loss: 0.7804\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7301 - val_loss: 0.7744\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7292 - val_loss: 0.7702\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7261 - val_loss: 0.8116\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7232 - val_loss: 0.7782\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7247 - val_loss: 0.7784\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7188 - val_loss: 0.7748\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7166 - val_loss: 0.7764\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7190 - val_loss: 0.7740\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7137 - val_loss: 0.7763\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.7103 - val_loss: 0.7693\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7115 - val_loss: 0.7645\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7099 - val_loss: 0.7782\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7098 - val_loss: 0.7661\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.7062 - val_loss: 0.7619\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7011 - val_loss: 0.7675\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.7047 - val_loss: 0.7675\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6991 - val_loss: 0.7703\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7008 - val_loss: 0.7613\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6967 - val_loss: 0.7649\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6955 - val_loss: 0.7660\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6950 - val_loss: 0.7613\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6928 - val_loss: 0.8051\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6922 - val_loss: 0.7669\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6898 - val_loss: 0.7643\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6888 - val_loss: 0.7585\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6848 - val_loss: 0.7485\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6854 - val_loss: 0.7492\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6854 - val_loss: 0.7515\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6870 - val_loss: 0.7557\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6823 - val_loss: 0.7467\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6792 - val_loss: 0.7526\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6828 - val_loss: 0.7533\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6761 - val_loss: 0.7513\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6822 - val_loss: 0.7530\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6746 - val_loss: 0.7554\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6720 - val_loss: 0.7558\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6710 - val_loss: 0.7500\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6710 - val_loss: 0.7423\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6680 - val_loss: 0.7568\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6709 - val_loss: 0.7465\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6685 - val_loss: 0.7454\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6674 - val_loss: 0.7492\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6626 - val_loss: 0.7409\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6611 - val_loss: 0.7461\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6618 - val_loss: 0.7399\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6639 - val_loss: 0.7523\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6592 - val_loss: 0.7510\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6586 - val_loss: 0.7436\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6584 - val_loss: 0.7364\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6558 - val_loss: 0.7494\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6575 - val_loss: 0.7411\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6591 - val_loss: 0.7397\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6530 - val_loss: 0.7420\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6511 - val_loss: 0.7498\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6517 - val_loss: 0.7383\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6498 - val_loss: 0.7428\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6502 - val_loss: 0.7454\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6501 - val_loss: 0.7500\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6468 - val_loss: 0.7421\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6484 - val_loss: 0.7352\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6438 - val_loss: 0.7512\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6478 - val_loss: 0.7488\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6450 - val_loss: 0.7486\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6449 - val_loss: 0.7486\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6445 - val_loss: 0.7398\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6414 - val_loss: 0.7368\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6422 - val_loss: 0.7372\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6442 - val_loss: 0.7493\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6401 - val_loss: 0.7413\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6380 - val_loss: 0.7479\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6384 - val_loss: 0.7510\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6350 - val_loss: 0.7637\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6363 - val_loss: 0.7369\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6346 - val_loss: 0.7415\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6314 - val_loss: 0.7510\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6346 - val_loss: 0.7412\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6328 - val_loss: 0.7350\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6326 - val_loss: 0.7474\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6308 - val_loss: 0.7497\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6326 - val_loss: 0.7336\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6295 - val_loss: 0.7435\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6289 - val_loss: 0.7551\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6260 - val_loss: 0.7859\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6290 - val_loss: 0.7391\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6285 - val_loss: 0.7375\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6296 - val_loss: 0.7440\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6239 - val_loss: 0.7508\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6288 - val_loss: 0.7415\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6249 - val_loss: 0.7420\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6284 - val_loss: 0.7324\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6246 - val_loss: 0.7499\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6241 - val_loss: 0.7399\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6229 - val_loss: 0.7347\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6196 - val_loss: 0.7424\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6200 - val_loss: 0.7394\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6204 - val_loss: 0.7378\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6183 - val_loss: 0.7461\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6202 - val_loss: 0.7425\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6219 - val_loss: 0.7550\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6201 - val_loss: 0.7394\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6175 - val_loss: 0.7700\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6156 - val_loss: 0.7534\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6134 - val_loss: 0.7562\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6128 - val_loss: 0.7374\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6185 - val_loss: 0.7423\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6161 - val_loss: 0.7542\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6104 - val_loss: 0.7563\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 201us/step - loss: 5.9219 - val_loss: 0.8550\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.8129 - val_loss: 0.8282\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.8026 - val_loss: 0.7981\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7992 - val_loss: 0.7967\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7927 - val_loss: 0.7876\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7854 - val_loss: 0.7774\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7760 - val_loss: 0.7798\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7674 - val_loss: 0.7789\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7575 - val_loss: 0.7588\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7486 - val_loss: 0.7674\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7421 - val_loss: 0.7458\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7358 - val_loss: 0.7518\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.7298 - val_loss: 0.7325\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.7228 - val_loss: 0.7308\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.7169 - val_loss: 0.7304\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7118 - val_loss: 0.7207\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7065 - val_loss: 0.7333\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7017 - val_loss: 0.7359\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6972 - val_loss: 0.7206\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6934 - val_loss: 0.7126\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6881 - val_loss: 0.7153\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6867 - val_loss: 0.7130\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6817 - val_loss: 0.7070\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6823 - val_loss: 0.7016\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6785 - val_loss: 0.7086\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6770 - val_loss: 0.7058\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6740 - val_loss: 0.7004\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6700 - val_loss: 0.6975\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6673 - val_loss: 0.6950\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6642 - val_loss: 0.7061\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6618 - val_loss: 0.6936\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6608 - val_loss: 0.7000\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6569 - val_loss: 0.7067\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6560 - val_loss: 0.6917\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6522 - val_loss: 0.6954\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6514 - val_loss: 0.6960\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6511 - val_loss: 0.6919\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6483 - val_loss: 0.6892\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6477 - val_loss: 0.6884\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6455 - val_loss: 0.6922\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6426 - val_loss: 0.6910\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6430 - val_loss: 0.7009\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6409 - val_loss: 0.6996\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6390 - val_loss: 0.6855\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6390 - val_loss: 0.6840\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6375 - val_loss: 0.6814\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6352 - val_loss: 0.6813\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6349 - val_loss: 0.6855\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6328 - val_loss: 0.6831\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6351 - val_loss: 0.6832\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6332 - val_loss: 0.6805\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6308 - val_loss: 0.6986\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6302 - val_loss: 0.6823\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6274 - val_loss: 0.6984\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6282 - val_loss: 0.6782\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6272 - val_loss: 0.6863\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6251 - val_loss: 0.6778\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6235 - val_loss: 0.6794\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6235 - val_loss: 0.6762\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6229 - val_loss: 0.6780\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6209 - val_loss: 0.6829\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6221 - val_loss: 0.6735\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6221 - val_loss: 0.6817\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6209 - val_loss: 0.6815\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6189 - val_loss: 0.6737\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6184 - val_loss: 0.6737\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6179 - val_loss: 0.6826\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6201 - val_loss: 0.6791\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6168 - val_loss: 0.6738\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6163 - val_loss: 0.6751\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6144 - val_loss: 0.6948\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6159 - val_loss: 0.6793\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6134 - val_loss: 0.6763\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6139 - val_loss: 0.6733\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6130 - val_loss: 0.6746\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6131 - val_loss: 0.6836\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6113 - val_loss: 0.6766\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6113 - val_loss: 0.6779\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6109 - val_loss: 0.6835\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6090 - val_loss: 0.6691\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6074 - val_loss: 0.6748\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6086 - val_loss: 0.6842\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6075 - val_loss: 0.6709\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6070 - val_loss: 0.6726\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6066 - val_loss: 0.6747\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6066 - val_loss: 0.6785\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6059 - val_loss: 0.6914\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6072 - val_loss: 0.6765\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6053 - val_loss: 0.6703\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6028 - val_loss: 0.6706\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6038 - val_loss: 0.6800\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6035 - val_loss: 0.6777\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6033 - val_loss: 0.6682\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6027 - val_loss: 0.6698\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6019 - val_loss: 0.6694\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6014 - val_loss: 0.6809\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6012 - val_loss: 0.6769\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6006 - val_loss: 0.6767\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6026 - val_loss: 0.6760\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6027 - val_loss: 0.6756\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5989 - val_loss: 0.6703\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6001 - val_loss: 0.6776\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5974 - val_loss: 0.6771\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5986 - val_loss: 0.6859\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5999 - val_loss: 0.6891\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5986 - val_loss: 0.6687\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5979 - val_loss: 0.6771\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5976 - val_loss: 0.6760\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5972 - val_loss: 0.6743\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5974 - val_loss: 0.6737\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5949 - val_loss: 0.6686\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5967 - val_loss: 0.6871\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5957 - val_loss: 0.6811\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5950 - val_loss: 0.6798\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5954 - val_loss: 0.6676\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5946 - val_loss: 0.6742\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5960 - val_loss: 0.6711\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5956 - val_loss: 0.6772\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5937 - val_loss: 0.6741\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 178us/step - loss: 0.5947 - val_loss: 0.6707\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5935 - val_loss: 0.6787\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5939 - val_loss: 0.6877\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5948 - val_loss: 0.6701\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5920 - val_loss: 0.6708\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5940 - val_loss: 0.6755\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5902 - val_loss: 0.6735\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5952 - val_loss: 0.6681\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5926 - val_loss: 0.6762\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5917 - val_loss: 0.6748\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5927 - val_loss: 0.6728\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5899 - val_loss: 0.6697\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5908 - val_loss: 0.6752\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5909 - val_loss: 0.6834\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5906 - val_loss: 0.6915\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5894 - val_loss: 0.6723\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5892 - val_loss: 0.6655\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5909 - val_loss: 0.6737\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5880 - val_loss: 0.6708\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5932 - val_loss: 0.6770\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5902 - val_loss: 0.6756\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5898 - val_loss: 0.6863\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5914 - val_loss: 0.6713\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5887 - val_loss: 0.6848\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5896 - val_loss: 0.6685\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5915 - val_loss: 0.6701\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5893 - val_loss: 0.6741\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5886 - val_loss: 0.6769\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.5881 - val_loss: 0.6663\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5882 - val_loss: 0.6678\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.5860 - val_loss: 0.6743\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 202us/step - loss: 5.9861 - val_loss: 0.8867\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.8206 - val_loss: 0.8384\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.8068 - val_loss: 0.8295\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.8025 - val_loss: 0.8309\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7986 - val_loss: 0.8306\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7914 - val_loss: 0.8223\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7881 - val_loss: 0.8121\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7849 - val_loss: 0.8287\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7807 - val_loss: 0.8014\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7740 - val_loss: 0.7997\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7689 - val_loss: 0.7937\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7626 - val_loss: 0.7955\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7616 - val_loss: 0.7920\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7563 - val_loss: 0.7803\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7533 - val_loss: 0.8222\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7502 - val_loss: 0.7857\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7495 - val_loss: 0.7736\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7443 - val_loss: 0.7834\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7409 - val_loss: 0.7804\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7404 - val_loss: 0.7718\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7341 - val_loss: 0.7647\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7335 - val_loss: 0.7639\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7315 - val_loss: 0.7590\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7289 - val_loss: 0.7623\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7280 - val_loss: 0.7585\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7263 - val_loss: 0.7561\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7230 - val_loss: 0.7811\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7199 - val_loss: 0.7757\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7195 - val_loss: 0.7525\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7193 - val_loss: 0.7553\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7158 - val_loss: 0.7715\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7138 - val_loss: 0.7641\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7114 - val_loss: 0.7571\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7117 - val_loss: 0.7517\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7099 - val_loss: 0.7559\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7075 - val_loss: 0.7495\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7066 - val_loss: 0.7446\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7059 - val_loss: 0.7450\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7043 - val_loss: 0.7424\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7010 - val_loss: 0.7696\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7001 - val_loss: 0.7518\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6968 - val_loss: 0.7395\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6942 - val_loss: 0.7370\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6934 - val_loss: 0.7373\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6913 - val_loss: 0.7330\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6896 - val_loss: 0.7417\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6876 - val_loss: 0.7355\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6871 - val_loss: 0.7416\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6843 - val_loss: 0.7329\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6839 - val_loss: 0.7931\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6827 - val_loss: 0.7298\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6810 - val_loss: 0.7275\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6819 - val_loss: 0.7303\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6789 - val_loss: 0.7385\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6780 - val_loss: 0.7306\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6763 - val_loss: 0.7376\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6759 - val_loss: 0.7366\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6741 - val_loss: 0.7238\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6722 - val_loss: 0.7290\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6726 - val_loss: 0.7187\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6723 - val_loss: 0.7240\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6684 - val_loss: 0.7258\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6708 - val_loss: 0.7253\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6703 - val_loss: 0.7244\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6666 - val_loss: 0.7242\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6679 - val_loss: 0.7248\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6660 - val_loss: 0.7191\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6666 - val_loss: 0.7209\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6673 - val_loss: 0.7221\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6654 - val_loss: 0.7251\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6671 - val_loss: 0.7130\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6648 - val_loss: 0.7188\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6648 - val_loss: 0.7132\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6624 - val_loss: 0.7237\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6642 - val_loss: 0.7246\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6629 - val_loss: 0.7197\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6627 - val_loss: 0.7099\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6615 - val_loss: 0.7102\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6612 - val_loss: 0.7228\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6618 - val_loss: 0.7088\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6604 - val_loss: 0.7185\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6615 - val_loss: 0.7207\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6589 - val_loss: 0.7175\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6593 - val_loss: 0.7143\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6596 - val_loss: 0.7318\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6573 - val_loss: 0.7176\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6586 - val_loss: 0.7105\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6590 - val_loss: 0.7257\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6570 - val_loss: 0.7220\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6573 - val_loss: 0.7118\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6580 - val_loss: 0.7078\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6547 - val_loss: 0.7158\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6559 - val_loss: 0.7123\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6552 - val_loss: 0.7144\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6548 - val_loss: 0.7034\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6545 - val_loss: 0.7192\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6533 - val_loss: 0.7234\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6544 - val_loss: 0.7271\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6546 - val_loss: 0.7114\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6535 - val_loss: 0.7148\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6532 - val_loss: 0.7092\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6535 - val_loss: 0.7172\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6523 - val_loss: 0.7119\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6523 - val_loss: 0.7044\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6531 - val_loss: 0.7065\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6517 - val_loss: 0.7206\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6508 - val_loss: 0.7154\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6522 - val_loss: 0.7056\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6520 - val_loss: 0.7147\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6524 - val_loss: 0.7106\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6516 - val_loss: 0.7020\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6502 - val_loss: 0.7033\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6502 - val_loss: 0.7149\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6497 - val_loss: 0.7249\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6508 - val_loss: 0.7048\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6494 - val_loss: 0.7114\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6505 - val_loss: 0.7033\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 178us/step - loss: 0.6503 - val_loss: 0.7096\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6482 - val_loss: 0.7069\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 180us/step - loss: 0.6489 - val_loss: 0.7111\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6491 - val_loss: 0.7457\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6503 - val_loss: 0.7118\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6485 - val_loss: 0.7067\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6485 - val_loss: 0.7275\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6489 - val_loss: 0.7024\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6491 - val_loss: 0.7074\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6493 - val_loss: 0.7193\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6470 - val_loss: 0.7449\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6474 - val_loss: 0.7099\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6471 - val_loss: 0.7207\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6470 - val_loss: 0.7352\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6467 - val_loss: 0.7132\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6479 - val_loss: 0.7074\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6473 - val_loss: 0.7069\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6459 - val_loss: 0.7070\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6452 - val_loss: 0.7160\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6467 - val_loss: 0.7123\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6480 - val_loss: 0.7032\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6453 - val_loss: 0.7053\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6451 - val_loss: 0.7131\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6480 - val_loss: 0.7044\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6435 - val_loss: 0.7037\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6439 - val_loss: 0.7010\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6462 - val_loss: 0.7146\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6440 - val_loss: 0.7101\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6444 - val_loss: 0.7173\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6451 - val_loss: 0.7064\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6448 - val_loss: 0.7058\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6445 - val_loss: 0.7041\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6424 - val_loss: 0.7156\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 199us/step - loss: 6.3541 - val_loss: 0.8404\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.8040 - val_loss: 0.7933\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.7906 - val_loss: 0.7867\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7859 - val_loss: 0.7834\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7786 - val_loss: 0.7790\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7740 - val_loss: 0.7714\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7650 - val_loss: 0.8286\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7585 - val_loss: 0.7612\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7483 - val_loss: 0.7638\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.7421 - val_loss: 0.7481\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.7350 - val_loss: 0.7491\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7288 - val_loss: 0.7634\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7223 - val_loss: 0.7351\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.7157 - val_loss: 0.7311\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7105 - val_loss: 0.7238\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7058 - val_loss: 0.7404\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.7041 - val_loss: 0.7261\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6972 - val_loss: 0.7165\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6975 - val_loss: 0.7098\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6894 - val_loss: 0.7301\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6878 - val_loss: 0.7088\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6862 - val_loss: 0.7044\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6843 - val_loss: 0.7020\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6795 - val_loss: 0.6981\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6794 - val_loss: 0.7387\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6767 - val_loss: 0.6993\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6741 - val_loss: 0.6992\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6725 - val_loss: 0.6966\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6698 - val_loss: 0.6986\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6683 - val_loss: 0.7105\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6664 - val_loss: 0.6982\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6649 - val_loss: 0.6933\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6626 - val_loss: 0.6952\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6611 - val_loss: 0.7010\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6629 - val_loss: 0.7004\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6597 - val_loss: 0.7101\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6591 - val_loss: 0.7004\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6575 - val_loss: 0.7078\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6557 - val_loss: 0.6912\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6548 - val_loss: 0.6912\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6532 - val_loss: 0.6853\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6521 - val_loss: 0.6892\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6503 - val_loss: 0.6904\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6518 - val_loss: 0.6850\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6491 - val_loss: 0.6879\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6474 - val_loss: 0.6870\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6457 - val_loss: 0.6818\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6438 - val_loss: 0.6795\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6456 - val_loss: 0.6838\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6443 - val_loss: 0.6831\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6416 - val_loss: 0.6939\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6426 - val_loss: 0.7040\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6385 - val_loss: 0.6886\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6387 - val_loss: 0.6894\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6353 - val_loss: 0.6831\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6359 - val_loss: 0.6829\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6347 - val_loss: 0.6831\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6355 - val_loss: 0.6809\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6335 - val_loss: 0.6815\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6323 - val_loss: 0.6967\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6323 - val_loss: 0.6876\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6320 - val_loss: 0.6805\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6316 - val_loss: 0.6794\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6307 - val_loss: 0.6860\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6278 - val_loss: 0.6747\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6287 - val_loss: 0.6774\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6265 - val_loss: 0.6772\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6265 - val_loss: 0.6821\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6248 - val_loss: 0.6785\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6245 - val_loss: 0.6786\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6247 - val_loss: 0.6857\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6223 - val_loss: 0.6840\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6222 - val_loss: 0.6862\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6234 - val_loss: 0.6834\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6218 - val_loss: 0.6752\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6192 - val_loss: 0.6799\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6196 - val_loss: 0.6897\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6164 - val_loss: 0.7007\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6193 - val_loss: 0.6780\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6167 - val_loss: 0.7053\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6175 - val_loss: 0.6789\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6150 - val_loss: 0.6731\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6152 - val_loss: 0.6845\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6154 - val_loss: 0.6942\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6134 - val_loss: 0.6813\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6148 - val_loss: 0.6828\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6162 - val_loss: 0.6752\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6149 - val_loss: 0.6759\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6124 - val_loss: 0.6797\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6121 - val_loss: 0.6801\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6111 - val_loss: 0.6753\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6112 - val_loss: 0.6765\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6114 - val_loss: 0.6766\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6110 - val_loss: 0.6811\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6108 - val_loss: 0.6710\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6091 - val_loss: 0.6706\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6118 - val_loss: 0.6720\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6099 - val_loss: 0.6765\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6089 - val_loss: 0.6840\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6087 - val_loss: 0.6781\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6074 - val_loss: 0.6734\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6080 - val_loss: 0.6704\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6069 - val_loss: 0.6747\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6059 - val_loss: 0.6721\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6059 - val_loss: 0.6713\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6051 - val_loss: 0.6721\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6069 - val_loss: 0.6831\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6067 - val_loss: 0.6810\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6055 - val_loss: 0.6849\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6055 - val_loss: 0.6727\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6034 - val_loss: 0.6822\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6041 - val_loss: 0.6762\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6051 - val_loss: 0.6746\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6025 - val_loss: 0.6739\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6049 - val_loss: 0.6715\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6052 - val_loss: 0.6705\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6011 - val_loss: 0.6716\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6019 - val_loss: 0.6957\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6027 - val_loss: 0.6699\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6038 - val_loss: 0.6765\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6004 - val_loss: 0.6687\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6000 - val_loss: 0.6877\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6002 - val_loss: 0.6821\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6016 - val_loss: 0.6658\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6005 - val_loss: 0.6933\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6003 - val_loss: 0.6667\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5968 - val_loss: 0.6764\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5990 - val_loss: 0.6704\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5982 - val_loss: 0.6705\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5990 - val_loss: 0.6749\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5986 - val_loss: 0.6886\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5996 - val_loss: 0.6778\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5970 - val_loss: 0.6768\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5985 - val_loss: 0.6714\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5957 - val_loss: 0.6729\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5972 - val_loss: 0.6619\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5973 - val_loss: 0.6890\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5983 - val_loss: 0.6680\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.5964 - val_loss: 0.6639\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.5959 - val_loss: 0.6797\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5950 - val_loss: 0.6742\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5961 - val_loss: 0.6708\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5954 - val_loss: 0.6900\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5944 - val_loss: 0.6617\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5946 - val_loss: 0.6726\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5951 - val_loss: 0.6653\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5959 - val_loss: 0.6623\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5941 - val_loss: 0.6747\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5948 - val_loss: 0.6622\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5962 - val_loss: 0.6677\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 206us/step - loss: 6.0403 - val_loss: 0.8998\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.8163 - val_loss: 0.8320\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.7985 - val_loss: 0.8351\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7920 - val_loss: 0.8169\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7887 - val_loss: 0.8147\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7829 - val_loss: 0.8041\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7744 - val_loss: 0.8122\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7697 - val_loss: 0.7940\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7618 - val_loss: 0.7860\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7542 - val_loss: 0.7849\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7493 - val_loss: 0.7812\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.7446 - val_loss: 0.7728\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7407 - val_loss: 0.7684\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7338 - val_loss: 0.7672\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7298 - val_loss: 0.7793\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.7266 - val_loss: 0.7742\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7224 - val_loss: 0.7540\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7194 - val_loss: 0.7663\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7148 - val_loss: 0.7520\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7106 - val_loss: 0.7530\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.7077 - val_loss: 0.7556\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7061 - val_loss: 0.7625\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7030 - val_loss: 0.7528\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7009 - val_loss: 0.7441\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6984 - val_loss: 0.7430\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6967 - val_loss: 0.7490\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6929 - val_loss: 0.7582\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6910 - val_loss: 0.7440\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6912 - val_loss: 0.7439\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6864 - val_loss: 0.7595\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6878 - val_loss: 0.7417\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6861 - val_loss: 0.7428\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6839 - val_loss: 0.7613\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6807 - val_loss: 0.7522\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6822 - val_loss: 0.7363\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6783 - val_loss: 0.7352\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6804 - val_loss: 0.7616\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6788 - val_loss: 0.7339\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6778 - val_loss: 0.7422\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6752 - val_loss: 0.7346\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6755 - val_loss: 0.7360\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6724 - val_loss: 0.7343\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6738 - val_loss: 0.7337\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6713 - val_loss: 0.7316\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6711 - val_loss: 0.7454\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6693 - val_loss: 0.7498\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6685 - val_loss: 0.7321\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6677 - val_loss: 0.7314\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6676 - val_loss: 0.7395\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6691 - val_loss: 0.7310\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6667 - val_loss: 0.7284\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6669 - val_loss: 0.7466\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6651 - val_loss: 0.7385\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6645 - val_loss: 0.7397\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6632 - val_loss: 0.7327\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6638 - val_loss: 0.7346\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6620 - val_loss: 0.7328\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6626 - val_loss: 0.7272\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6624 - val_loss: 0.7296\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6614 - val_loss: 0.7281\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6624 - val_loss: 0.7436\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6612 - val_loss: 0.7944\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6604 - val_loss: 0.7292\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6582 - val_loss: 0.7261\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6603 - val_loss: 0.7291\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6598 - val_loss: 0.7259\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6564 - val_loss: 0.7273\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6580 - val_loss: 0.7377\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6588 - val_loss: 0.7267\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6590 - val_loss: 0.7288\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6574 - val_loss: 0.7272\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6569 - val_loss: 0.7528\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6553 - val_loss: 0.7216\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6550 - val_loss: 0.7260\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6564 - val_loss: 0.7263\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6550 - val_loss: 0.7219\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6556 - val_loss: 0.7277\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6539 - val_loss: 0.7389\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6529 - val_loss: 0.7500\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6524 - val_loss: 0.7182\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6524 - val_loss: 0.7312\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6550 - val_loss: 0.7203\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6529 - val_loss: 0.7227\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6523 - val_loss: 0.7309\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6545 - val_loss: 0.7187\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6516 - val_loss: 0.7330\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6519 - val_loss: 0.7281\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6514 - val_loss: 0.7262\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6506 - val_loss: 0.7243\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6523 - val_loss: 0.7311\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6515 - val_loss: 0.7252\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6526 - val_loss: 0.7205\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6511 - val_loss: 0.7228\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6498 - val_loss: 0.7341\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6529 - val_loss: 0.7554\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6505 - val_loss: 0.7191\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6490 - val_loss: 0.7165\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6486 - val_loss: 0.7211\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6510 - val_loss: 0.7639\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6486 - val_loss: 0.7227\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6512 - val_loss: 0.7189\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6512 - val_loss: 0.7228\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6480 - val_loss: 0.7265\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6486 - val_loss: 0.7231\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6487 - val_loss: 0.7253\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6489 - val_loss: 0.7162\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6473 - val_loss: 0.7344\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6454 - val_loss: 0.7214\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6480 - val_loss: 0.7241\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6465 - val_loss: 0.7270\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6449 - val_loss: 0.7206\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6468 - val_loss: 0.7163\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6459 - val_loss: 0.7172\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6457 - val_loss: 0.7203\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6459 - val_loss: 0.7405\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6461 - val_loss: 0.7177\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6475 - val_loss: 0.7233\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6455 - val_loss: 0.7240\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6467 - val_loss: 0.7212\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6462 - val_loss: 0.7311\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6431 - val_loss: 0.7241\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6454 - val_loss: 0.7230\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6450 - val_loss: 0.7221\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6447 - val_loss: 0.7178\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6445 - val_loss: 0.7188\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6450 - val_loss: 0.7181\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6414 - val_loss: 0.7200\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6458 - val_loss: 0.7163\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6433 - val_loss: 0.7186\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6426 - val_loss: 0.7187\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6433 - val_loss: 0.7627\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6422 - val_loss: 0.7183\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6442 - val_loss: 0.7142\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6422 - val_loss: 0.7179\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6449 - val_loss: 0.7242\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6419 - val_loss: 0.7166\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6429 - val_loss: 0.7169\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6430 - val_loss: 0.7145\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6420 - val_loss: 0.7323\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6421 - val_loss: 0.7249\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6439 - val_loss: 0.7184\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6411 - val_loss: 0.7194\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6446 - val_loss: 0.7154\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6422 - val_loss: 0.7214\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6425 - val_loss: 0.7227\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6436 - val_loss: 0.7203\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6414 - val_loss: 0.7255\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 0.6426 - val_loss: 0.7337\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6410 - val_loss: 0.7257\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6408 - val_loss: 0.7174\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 284us/step - loss: 20.1978 - val_loss: 7.6535\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 1.9330 - val_loss: 1.0316\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.9175 - val_loss: 0.8456\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.8101 - val_loss: 0.7994\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7757 - val_loss: 0.7804\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7606 - val_loss: 0.7688\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7583 - val_loss: 0.7695\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7543 - val_loss: 0.7700\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7549 - val_loss: 0.7682\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7477 - val_loss: 0.7662\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7479 - val_loss: 0.7628\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7486 - val_loss: 0.7630\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7483 - val_loss: 0.7843\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7496 - val_loss: 0.7879\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7460 - val_loss: 0.7664\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7443 - val_loss: 0.7656\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7414 - val_loss: 0.7602\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7429 - val_loss: 0.7669\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7420 - val_loss: 0.7761\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7350 - val_loss: 0.7704\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7360 - val_loss: 0.7776\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7384 - val_loss: 0.7580\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7328 - val_loss: 0.7688\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7313 - val_loss: 0.7604\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.7308 - val_loss: 0.7720\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7286 - val_loss: 0.7564\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7277 - val_loss: 0.7557\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7248 - val_loss: 0.7889\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7295 - val_loss: 0.7533\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7257 - val_loss: 0.7588\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7228 - val_loss: 0.7500\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7198 - val_loss: 0.7595\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7262 - val_loss: 0.7666\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7215 - val_loss: 0.7573\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7129 - val_loss: 0.7498\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7163 - val_loss: 0.7630\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7155 - val_loss: 0.7479\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7099 - val_loss: 0.7518\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7081 - val_loss: 0.7442\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7093 - val_loss: 0.7621\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7064 - val_loss: 0.7510\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7051 - val_loss: 0.7414\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7074 - val_loss: 0.7471\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.7043 - val_loss: 0.7611\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7013 - val_loss: 0.7413\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7010 - val_loss: 0.7511\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6988 - val_loss: 0.7409\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6994 - val_loss: 0.7385\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6979 - val_loss: 0.7470\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6972 - val_loss: 0.7383\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6944 - val_loss: 0.7422\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6926 - val_loss: 0.7415\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6932 - val_loss: 0.7417\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6879 - val_loss: 0.7521\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6915 - val_loss: 0.7560\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6871 - val_loss: 0.7361\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6875 - val_loss: 0.7575\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6853 - val_loss: 0.7618\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6837 - val_loss: 0.7367\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6852 - val_loss: 0.7383\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6870 - val_loss: 0.7390\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6798 - val_loss: 0.7354\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6825 - val_loss: 0.7390\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6797 - val_loss: 0.7358\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6816 - val_loss: 0.7442\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6762 - val_loss: 0.7473\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6784 - val_loss: 0.7470\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6776 - val_loss: 0.7386\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6762 - val_loss: 0.7354\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6749 - val_loss: 0.7311\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6736 - val_loss: 0.7365\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6769 - val_loss: 0.7370\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6715 - val_loss: 0.7330\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6724 - val_loss: 0.7334\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6696 - val_loss: 0.7441\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6714 - val_loss: 0.7325\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6670 - val_loss: 0.7370\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6669 - val_loss: 0.7315\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6711 - val_loss: 0.7544\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6685 - val_loss: 0.7520\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6725 - val_loss: 0.7314\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6685 - val_loss: 0.7406\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6661 - val_loss: 0.7467\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6669 - val_loss: 0.7338\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6641 - val_loss: 0.7316\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6617 - val_loss: 0.7322\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6644 - val_loss: 0.7303\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6633 - val_loss: 0.7275\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6636 - val_loss: 0.7463\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6639 - val_loss: 0.7340\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6602 - val_loss: 0.7338\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6570 - val_loss: 0.7399\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6587 - val_loss: 0.7267\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6602 - val_loss: 0.7390\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6584 - val_loss: 0.7528\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6543 - val_loss: 0.7265\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6602 - val_loss: 0.7376\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6582 - val_loss: 0.7345\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6565 - val_loss: 0.7256\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6555 - val_loss: 0.7224\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6521 - val_loss: 0.7281\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6556 - val_loss: 0.7399\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6553 - val_loss: 0.7186\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6569 - val_loss: 0.7302\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6563 - val_loss: 0.7303\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6528 - val_loss: 0.7282\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6528 - val_loss: 0.7248\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6522 - val_loss: 0.7329\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6512 - val_loss: 0.7318\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6557 - val_loss: 0.7255\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6497 - val_loss: 0.7231\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6500 - val_loss: 0.7244\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6509 - val_loss: 0.7334\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6483 - val_loss: 0.7275\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6476 - val_loss: 0.7408\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6513 - val_loss: 0.7380\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6497 - val_loss: 0.7232\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6450 - val_loss: 0.7208\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6488 - val_loss: 0.7283\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6463 - val_loss: 0.7249\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6443 - val_loss: 0.7182\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6485 - val_loss: 0.7325\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6463 - val_loss: 0.7222\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6475 - val_loss: 0.7211\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6437 - val_loss: 0.7202\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6402 - val_loss: 0.7280\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6434 - val_loss: 0.7440\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6448 - val_loss: 0.7301\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6413 - val_loss: 0.7165\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6425 - val_loss: 0.7253\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6412 - val_loss: 0.7200\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6432 - val_loss: 0.7200\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6426 - val_loss: 0.7203\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6407 - val_loss: 0.7167\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6390 - val_loss: 0.7170\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6408 - val_loss: 0.7212\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6443 - val_loss: 0.7225\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6417 - val_loss: 0.7173\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6403 - val_loss: 0.7132\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6385 - val_loss: 0.7189\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6399 - val_loss: 0.7356\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6419 - val_loss: 0.7173\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6384 - val_loss: 0.7218\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6376 - val_loss: 0.7248\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6390 - val_loss: 0.7145\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6382 - val_loss: 0.7256\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6409 - val_loss: 0.7195\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6391 - val_loss: 0.7153\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6369 - val_loss: 0.7140\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6359 - val_loss: 0.7163\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 288us/step - loss: 20.8391 - val_loss: 10.2801\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 2.6515 - val_loss: 1.0911\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.9787 - val_loss: 0.8604\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.8418 - val_loss: 0.8170\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7892 - val_loss: 0.7620\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7678 - val_loss: 0.7471\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7578 - val_loss: 0.7408\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7552 - val_loss: 0.7383\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7506 - val_loss: 0.7450\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7512 - val_loss: 0.7402\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7526 - val_loss: 0.7516\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7497 - val_loss: 0.7420\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7485 - val_loss: 0.7419\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7518 - val_loss: 0.7415\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7444 - val_loss: 0.7430\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7457 - val_loss: 0.7537\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7447 - val_loss: 0.7434\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7446 - val_loss: 0.7522\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7460 - val_loss: 0.7689\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7475 - val_loss: 0.7356\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7448 - val_loss: 0.7667\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7465 - val_loss: 0.7736\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7427 - val_loss: 0.7354\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7404 - val_loss: 0.7329\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7423 - val_loss: 0.7403\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7424 - val_loss: 0.7341\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7391 - val_loss: 0.7366\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7402 - val_loss: 0.7340\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7381 - val_loss: 0.7392\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7371 - val_loss: 0.7284\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7385 - val_loss: 0.7314\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7352 - val_loss: 0.7324\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7359 - val_loss: 0.7347\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7353 - val_loss: 0.7269\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7319 - val_loss: 0.7335\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7345 - val_loss: 0.7286\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7341 - val_loss: 0.7470\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7323 - val_loss: 0.7243\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7303 - val_loss: 0.7268\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7299 - val_loss: 0.7666\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7303 - val_loss: 0.7295\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7298 - val_loss: 0.7325\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7315 - val_loss: 0.7211\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7324 - val_loss: 0.7180\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7259 - val_loss: 0.7182\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7264 - val_loss: 0.7166\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7253 - val_loss: 0.7163\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7240 - val_loss: 0.7405\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7235 - val_loss: 0.7146\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7270 - val_loss: 0.7161\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7212 - val_loss: 0.7363\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7251 - val_loss: 0.7185\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7214 - val_loss: 0.7181\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7178 - val_loss: 0.7161\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7198 - val_loss: 0.7181\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7186 - val_loss: 0.7073\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7199 - val_loss: 0.7181\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7182 - val_loss: 0.7071\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7188 - val_loss: 0.7387\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7152 - val_loss: 0.7148\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7201 - val_loss: 0.7134\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7188 - val_loss: 0.7153\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7133 - val_loss: 0.7118\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7164 - val_loss: 0.7068\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7160 - val_loss: 0.7175\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7134 - val_loss: 0.7065\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7092 - val_loss: 0.7089\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7103 - val_loss: 0.7108\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7102 - val_loss: 0.7271\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7086 - val_loss: 0.7054\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7079 - val_loss: 0.7060\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7078 - val_loss: 0.6974\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7072 - val_loss: 0.7003\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7058 - val_loss: 0.7282\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7053 - val_loss: 0.7070\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7014 - val_loss: 0.7167\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.7049 - val_loss: 0.6984\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.7004 - val_loss: 0.7017\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6979 - val_loss: 0.7078\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6999 - val_loss: 0.6988\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6974 - val_loss: 0.7022\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6989 - val_loss: 0.6902\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6997 - val_loss: 0.6978\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6960 - val_loss: 0.7014\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6948 - val_loss: 0.6996\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6935 - val_loss: 0.6950\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6933 - val_loss: 0.6895\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6945 - val_loss: 0.6974\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6886 - val_loss: 0.6926\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6885 - val_loss: 0.6929\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6907 - val_loss: 0.6989\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6896 - val_loss: 0.6868\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6877 - val_loss: 0.7061\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6903 - val_loss: 0.7015\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6876 - val_loss: 0.6929\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6877 - val_loss: 0.6886\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6843 - val_loss: 0.7086\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6821 - val_loss: 0.6908\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6862 - val_loss: 0.6903\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6824 - val_loss: 0.6893\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6813 - val_loss: 0.6885\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6813 - val_loss: 0.6915\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6810 - val_loss: 0.7046\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6790 - val_loss: 0.6850\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6820 - val_loss: 0.7101\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6769 - val_loss: 0.6907\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6791 - val_loss: 0.6809\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6765 - val_loss: 0.6804\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6782 - val_loss: 0.6878\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6733 - val_loss: 0.6899\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6761 - val_loss: 0.6983\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6732 - val_loss: 0.7035\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6756 - val_loss: 0.6908\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6763 - val_loss: 0.6822\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6714 - val_loss: 0.6931\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6744 - val_loss: 0.6999\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6729 - val_loss: 0.7108\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6782 - val_loss: 0.6892\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6695 - val_loss: 0.6889\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6719 - val_loss: 0.7074\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6693 - val_loss: 0.6889\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6709 - val_loss: 0.7016\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6725 - val_loss: 0.6852\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6639 - val_loss: 0.6896\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6674 - val_loss: 0.6931\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6660 - val_loss: 0.6968\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6723 - val_loss: 0.6897\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6651 - val_loss: 0.6834\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6677 - val_loss: 0.6890\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6686 - val_loss: 0.6920\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6627 - val_loss: 0.6853\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6643 - val_loss: 0.6879\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6639 - val_loss: 0.6779\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6642 - val_loss: 0.6891\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6624 - val_loss: 0.6849\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6622 - val_loss: 0.6852\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6620 - val_loss: 0.6810\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.6628 - val_loss: 0.6930\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.6645 - val_loss: 0.6811\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6598 - val_loss: 0.7125\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6660 - val_loss: 0.6817\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6630 - val_loss: 0.6832\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6620 - val_loss: 0.6752\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6599 - val_loss: 0.6869\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6588 - val_loss: 0.6914\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6587 - val_loss: 0.6857\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6615 - val_loss: 0.6828\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6555 - val_loss: 0.6857\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6579 - val_loss: 0.6845\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.6571 - val_loss: 0.6832\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 287us/step - loss: 20.3716 - val_loss: 8.4471\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 2.1711 - val_loss: 0.9850\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.9229 - val_loss: 0.8421\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.8185 - val_loss: 0.7982\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7787 - val_loss: 0.7718\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7654 - val_loss: 0.7540\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 180us/step - loss: 0.7542 - val_loss: 0.7724\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.7548 - val_loss: 0.7583\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.7503 - val_loss: 0.7621\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7507 - val_loss: 0.7529\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7489 - val_loss: 0.7577\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7462 - val_loss: 0.7540\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7450 - val_loss: 0.7859\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7444 - val_loss: 0.7695\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7464 - val_loss: 0.7593\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 182us/step - loss: 0.7427 - val_loss: 0.7531\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7409 - val_loss: 0.7748\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.7400 - val_loss: 0.7786\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7346 - val_loss: 0.7758\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7369 - val_loss: 0.7552\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7335 - val_loss: 0.7505\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7325 - val_loss: 0.7607\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7287 - val_loss: 0.7679\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7285 - val_loss: 0.7489\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7248 - val_loss: 0.7462\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7234 - val_loss: 0.7397\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7290 - val_loss: 0.7436\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7211 - val_loss: 0.7444\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7178 - val_loss: 0.7478\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7179 - val_loss: 0.7421\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.7165 - val_loss: 0.7435\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7162 - val_loss: 0.7338\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7091 - val_loss: 0.7515\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 180us/step - loss: 0.7097 - val_loss: 0.7356\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.7074 - val_loss: 0.7310\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7044 - val_loss: 0.7434\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.7105 - val_loss: 0.7279\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.7002 - val_loss: 0.7420\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6979 - val_loss: 0.7326\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6965 - val_loss: 0.7302\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6954 - val_loss: 0.7349\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6933 - val_loss: 0.7315\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6912 - val_loss: 0.7349\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6890 - val_loss: 0.7266\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6895 - val_loss: 0.7420\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6862 - val_loss: 0.7219\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6836 - val_loss: 0.7243\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6870 - val_loss: 0.7449\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6832 - val_loss: 0.7255\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6822 - val_loss: 0.7331\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6773 - val_loss: 0.7206\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6801 - val_loss: 0.7170\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6734 - val_loss: 0.7169\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6734 - val_loss: 0.7198\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6740 - val_loss: 0.7220\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6706 - val_loss: 0.7305\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6723 - val_loss: 0.7128\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.6689 - val_loss: 0.7181\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6684 - val_loss: 0.7190\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6654 - val_loss: 0.7248\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6656 - val_loss: 0.7168\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6673 - val_loss: 0.7154\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6614 - val_loss: 0.7160\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 179us/step - loss: 0.6613 - val_loss: 0.7250\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6604 - val_loss: 0.7230\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6609 - val_loss: 0.7206\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6587 - val_loss: 0.7145\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6537 - val_loss: 0.7346\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6593 - val_loss: 0.7172\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6574 - val_loss: 0.7121\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6580 - val_loss: 0.7161\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6542 - val_loss: 0.7212\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6556 - val_loss: 0.7153\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6548 - val_loss: 0.7128\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6507 - val_loss: 0.7123\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6530 - val_loss: 0.7164\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.6542 - val_loss: 0.7235\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6488 - val_loss: 0.7190\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6558 - val_loss: 0.7284\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6507 - val_loss: 0.7113\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6455 - val_loss: 0.7141\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6453 - val_loss: 0.7310\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6473 - val_loss: 0.7283\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6427 - val_loss: 0.7164\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6414 - val_loss: 0.7194\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6441 - val_loss: 0.7108\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 180us/step - loss: 0.6443 - val_loss: 0.7172\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6426 - val_loss: 0.7163\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6446 - val_loss: 0.7121\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6384 - val_loss: 0.7381\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6408 - val_loss: 0.7158\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6401 - val_loss: 0.7091\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6382 - val_loss: 0.7131\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6416 - val_loss: 0.7099\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6379 - val_loss: 0.7132\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6363 - val_loss: 0.7111\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6375 - val_loss: 0.7431\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.6357 - val_loss: 0.7413\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6373 - val_loss: 0.7155\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6348 - val_loss: 0.7145\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6361 - val_loss: 0.7121\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6322 - val_loss: 0.7107\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6335 - val_loss: 0.7088\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6360 - val_loss: 0.7136\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.6309 - val_loss: 0.7085\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6360 - val_loss: 0.7049\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6332 - val_loss: 0.7100\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6314 - val_loss: 0.7117\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6332 - val_loss: 0.7139\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6324 - val_loss: 0.7068\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6317 - val_loss: 0.7073\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6302 - val_loss: 0.7130\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6285 - val_loss: 0.7099\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6294 - val_loss: 0.7082\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6263 - val_loss: 0.7246\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6259 - val_loss: 0.7115\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6261 - val_loss: 0.7095\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6281 - val_loss: 0.7062\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6284 - val_loss: 0.7073\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6263 - val_loss: 0.7076\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6288 - val_loss: 0.7060\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6253 - val_loss: 0.7193\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6255 - val_loss: 0.7546\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6269 - val_loss: 0.7049\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6292 - val_loss: 0.7559\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6263 - val_loss: 0.7105\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6259 - val_loss: 0.7052\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6267 - val_loss: 0.7082\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6226 - val_loss: 0.7072\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6220 - val_loss: 0.7067\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6226 - val_loss: 0.7533\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6240 - val_loss: 0.6984\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6264 - val_loss: 0.7044\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6207 - val_loss: 0.7168\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6223 - val_loss: 0.7076\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6214 - val_loss: 0.7009\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6215 - val_loss: 0.7120\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6185 - val_loss: 0.7069\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.6215 - val_loss: 0.7058\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 180us/step - loss: 0.6192 - val_loss: 0.7024\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6200 - val_loss: 0.7302\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6186 - val_loss: 0.7236\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.6164 - val_loss: 0.7080\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6128 - val_loss: 0.7002\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6179 - val_loss: 0.7068\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6192 - val_loss: 0.7283\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6155 - val_loss: 0.7054\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6149 - val_loss: 0.7521\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6183 - val_loss: 0.7074\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.6162 - val_loss: 0.7101\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/150\n",
      "3200/3200 [==============================] - 1s 286us/step - loss: 20.2972 - val_loss: 9.0442\n",
      "Epoch 2/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 2.2892 - val_loss: 1.0864\n",
      "Epoch 3/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.9449 - val_loss: 0.8621\n",
      "Epoch 4/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.8217 - val_loss: 0.7933\n",
      "Epoch 5/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7777 - val_loss: 0.7776\n",
      "Epoch 6/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7636 - val_loss: 0.7777\n",
      "Epoch 7/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7567 - val_loss: 0.7543\n",
      "Epoch 8/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7511 - val_loss: 0.7828\n",
      "Epoch 9/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7544 - val_loss: 0.7575\n",
      "Epoch 10/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7458 - val_loss: 0.7721\n",
      "Epoch 11/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7461 - val_loss: 0.7510\n",
      "Epoch 12/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7443 - val_loss: 0.7533\n",
      "Epoch 13/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7466 - val_loss: 0.7673\n",
      "Epoch 14/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7432 - val_loss: 0.7542\n",
      "Epoch 15/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7419 - val_loss: 0.7614\n",
      "Epoch 16/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7372 - val_loss: 0.7518\n",
      "Epoch 17/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7369 - val_loss: 0.7534\n",
      "Epoch 18/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7344 - val_loss: 0.7595\n",
      "Epoch 19/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7343 - val_loss: 0.7470\n",
      "Epoch 20/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7306 - val_loss: 0.7435\n",
      "Epoch 21/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7273 - val_loss: 0.7463\n",
      "Epoch 22/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7291 - val_loss: 0.7412\n",
      "Epoch 23/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7283 - val_loss: 0.7385\n",
      "Epoch 24/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7204 - val_loss: 0.7466\n",
      "Epoch 25/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7185 - val_loss: 0.7384\n",
      "Epoch 26/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7184 - val_loss: 0.7243\n",
      "Epoch 27/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7128 - val_loss: 0.7318\n",
      "Epoch 28/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.7145 - val_loss: 0.7329\n",
      "Epoch 29/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.7101 - val_loss: 0.7239\n",
      "Epoch 30/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7077 - val_loss: 0.7324\n",
      "Epoch 31/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7021 - val_loss: 0.7215\n",
      "Epoch 32/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.7031 - val_loss: 0.7348\n",
      "Epoch 33/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.7001 - val_loss: 0.7358\n",
      "Epoch 34/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6986 - val_loss: 0.7213\n",
      "Epoch 35/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6926 - val_loss: 0.7323\n",
      "Epoch 36/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6929 - val_loss: 0.7253\n",
      "Epoch 37/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6879 - val_loss: 0.7243\n",
      "Epoch 38/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6913 - val_loss: 0.7198\n",
      "Epoch 39/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6854 - val_loss: 0.7075\n",
      "Epoch 40/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6886 - val_loss: 0.7153\n",
      "Epoch 41/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6852 - val_loss: 0.7203\n",
      "Epoch 42/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6821 - val_loss: 0.7265\n",
      "Epoch 43/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6784 - val_loss: 0.7071\n",
      "Epoch 44/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6769 - val_loss: 0.7183\n",
      "Epoch 45/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6739 - val_loss: 0.7116\n",
      "Epoch 46/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6766 - val_loss: 0.7136\n",
      "Epoch 47/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6739 - val_loss: 0.7097\n",
      "Epoch 48/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6705 - val_loss: 0.7071\n",
      "Epoch 49/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6733 - val_loss: 0.7021\n",
      "Epoch 50/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6707 - val_loss: 0.7132\n",
      "Epoch 51/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6690 - val_loss: 0.7019\n",
      "Epoch 52/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6671 - val_loss: 0.7070\n",
      "Epoch 53/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6643 - val_loss: 0.7027\n",
      "Epoch 54/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6621 - val_loss: 0.7019\n",
      "Epoch 55/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6618 - val_loss: 0.6894\n",
      "Epoch 56/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6592 - val_loss: 0.6968\n",
      "Epoch 57/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6541 - val_loss: 0.7083\n",
      "Epoch 58/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6592 - val_loss: 0.6966\n",
      "Epoch 59/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6577 - val_loss: 0.6980\n",
      "Epoch 60/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6560 - val_loss: 0.6969\n",
      "Epoch 61/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6533 - val_loss: 0.6917\n",
      "Epoch 62/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6528 - val_loss: 0.6882\n",
      "Epoch 63/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6555 - val_loss: 0.6873\n",
      "Epoch 64/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6522 - val_loss: 0.6920\n",
      "Epoch 65/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6522 - val_loss: 0.6844\n",
      "Epoch 66/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6501 - val_loss: 0.6902\n",
      "Epoch 67/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6469 - val_loss: 0.7034\n",
      "Epoch 68/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6468 - val_loss: 0.6887\n",
      "Epoch 69/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6499 - val_loss: 0.6886\n",
      "Epoch 70/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6435 - val_loss: 0.6887\n",
      "Epoch 71/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6423 - val_loss: 0.6882\n",
      "Epoch 72/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6425 - val_loss: 0.6900\n",
      "Epoch 73/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.6442 - val_loss: 0.6767\n",
      "Epoch 74/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6401 - val_loss: 0.6894\n",
      "Epoch 75/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6425 - val_loss: 0.6829\n",
      "Epoch 76/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6425 - val_loss: 0.6821\n",
      "Epoch 77/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6437 - val_loss: 0.6846\n",
      "Epoch 78/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6379 - val_loss: 0.6888\n",
      "Epoch 79/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6396 - val_loss: 0.6814\n",
      "Epoch 80/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6360 - val_loss: 0.6765\n",
      "Epoch 81/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6393 - val_loss: 0.6827\n",
      "Epoch 82/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6342 - val_loss: 0.6857\n",
      "Epoch 83/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6308 - val_loss: 0.6807\n",
      "Epoch 84/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6342 - val_loss: 0.6827\n",
      "Epoch 85/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6317 - val_loss: 0.6992\n",
      "Epoch 86/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6383 - val_loss: 0.6794\n",
      "Epoch 87/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6302 - val_loss: 0.7101\n",
      "Epoch 88/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6323 - val_loss: 0.6834\n",
      "Epoch 89/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6302 - val_loss: 0.6825\n",
      "Epoch 90/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6250 - val_loss: 0.6827\n",
      "Epoch 91/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6269 - val_loss: 0.6985\n",
      "Epoch 92/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6270 - val_loss: 0.6847\n",
      "Epoch 93/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6286 - val_loss: 0.6756\n",
      "Epoch 94/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6237 - val_loss: 0.6934\n",
      "Epoch 95/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6256 - val_loss: 0.6788\n",
      "Epoch 96/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6234 - val_loss: 0.6793\n",
      "Epoch 97/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6207 - val_loss: 0.6812\n",
      "Epoch 98/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6231 - val_loss: 0.6937\n",
      "Epoch 99/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6203 - val_loss: 0.6807\n",
      "Epoch 100/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6213 - val_loss: 0.6783\n",
      "Epoch 101/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6214 - val_loss: 0.6737\n",
      "Epoch 102/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6219 - val_loss: 0.6979\n",
      "Epoch 103/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6212 - val_loss: 0.6718\n",
      "Epoch 104/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6184 - val_loss: 0.6801\n",
      "Epoch 105/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6200 - val_loss: 0.6831\n",
      "Epoch 106/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6192 - val_loss: 0.6795\n",
      "Epoch 107/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6152 - val_loss: 0.6793\n",
      "Epoch 108/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6164 - val_loss: 0.6814\n",
      "Epoch 109/150\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.6160 - val_loss: 0.6735\n",
      "Epoch 110/150\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.6125 - val_loss: 0.6780\n",
      "Epoch 111/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6146 - val_loss: 0.6769\n",
      "Epoch 112/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6126 - val_loss: 0.6717\n",
      "Epoch 113/150\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.6104 - val_loss: 0.6944\n",
      "Epoch 114/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6143 - val_loss: 0.6698\n",
      "Epoch 115/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6136 - val_loss: 0.6717\n",
      "Epoch 116/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6101 - val_loss: 0.6691\n",
      "Epoch 117/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6088 - val_loss: 0.6743\n",
      "Epoch 118/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.6133 - val_loss: 0.6667\n",
      "Epoch 119/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6108 - val_loss: 0.6754\n",
      "Epoch 120/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6087 - val_loss: 0.6683\n",
      "Epoch 121/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6110 - val_loss: 0.6779\n",
      "Epoch 122/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6092 - val_loss: 0.6784\n",
      "Epoch 123/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6103 - val_loss: 0.6797\n",
      "Epoch 124/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6052 - val_loss: 0.6719\n",
      "Epoch 125/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6082 - val_loss: 0.6715\n",
      "Epoch 126/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6076 - val_loss: 0.6702\n",
      "Epoch 127/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6067 - val_loss: 0.6679\n",
      "Epoch 128/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6057 - val_loss: 0.6683\n",
      "Epoch 129/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6022 - val_loss: 0.6654\n",
      "Epoch 130/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6017 - val_loss: 0.6697\n",
      "Epoch 131/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6017 - val_loss: 0.6780\n",
      "Epoch 132/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.5997 - val_loss: 0.6750\n",
      "Epoch 133/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6001 - val_loss: 0.6668\n",
      "Epoch 134/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6007 - val_loss: 0.6867\n",
      "Epoch 135/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6007 - val_loss: 0.6761\n",
      "Epoch 136/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.6008 - val_loss: 0.6752\n",
      "Epoch 137/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.5981 - val_loss: 0.6752\n",
      "Epoch 138/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6022 - val_loss: 0.6798\n",
      "Epoch 139/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.5973 - val_loss: 0.6668\n",
      "Epoch 140/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.6005 - val_loss: 0.6682\n",
      "Epoch 141/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.5989 - val_loss: 0.6810\n",
      "Epoch 142/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.5949 - val_loss: 0.6735\n",
      "Epoch 143/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.5957 - val_loss: 0.6687\n",
      "Epoch 144/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.5967 - val_loss: 0.6813\n",
      "Epoch 145/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.5936 - val_loss: 0.6999\n",
      "Epoch 146/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.5948 - val_loss: 0.6744\n",
      "Epoch 147/150\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.5934 - val_loss: 0.6837\n",
      "Epoch 148/150\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.5923 - val_loss: 0.6657\n",
      "Epoch 149/150\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.5941 - val_loss: 0.6727\n",
      "Epoch 150/150\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.5947 - val_loss: 0.6739\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 200us/step - loss: 6.2146 - val_loss: 0.8090\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7684 - val_loss: 0.7678\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7534 - val_loss: 0.7567\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7487 - val_loss: 0.7516\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7441 - val_loss: 0.7474\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7358 - val_loss: 0.7370\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7290 - val_loss: 0.7320\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7198 - val_loss: 0.7295\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7124 - val_loss: 0.7217\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7052 - val_loss: 0.7257\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7003 - val_loss: 0.7207\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6921 - val_loss: 0.7156\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6902 - val_loss: 0.7010\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6848 - val_loss: 0.6982\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6779 - val_loss: 0.6907\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6753 - val_loss: 0.6904\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6708 - val_loss: 0.7085\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6654 - val_loss: 0.7163\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 179us/step - loss: 0.6649 - val_loss: 0.6824\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6608 - val_loss: 0.6825\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6582 - val_loss: 0.6797\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6574 - val_loss: 0.6780\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6536 - val_loss: 0.6773\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6525 - val_loss: 0.6678\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6485 - val_loss: 0.6738\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 3s 203us/step - loss: 0.6476 - val_loss: 0.6720\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6436 - val_loss: 0.6672\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6424 - val_loss: 0.6968\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6430 - val_loss: 0.6709\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6416 - val_loss: 0.6701\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6393 - val_loss: 0.6598\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6376 - val_loss: 0.6628\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6350 - val_loss: 0.6639\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6361 - val_loss: 0.6676\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6347 - val_loss: 0.6618\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6346 - val_loss: 0.7077\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6338 - val_loss: 0.6663\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6311 - val_loss: 0.6573\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6301 - val_loss: 0.6639\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6278 - val_loss: 0.6720\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6282 - val_loss: 0.6621\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6282 - val_loss: 0.6608\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6267 - val_loss: 0.6553\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6274 - val_loss: 0.6893\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6245 - val_loss: 0.6717\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6256 - val_loss: 0.6690\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6246 - val_loss: 0.6602\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6230 - val_loss: 0.6559\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6212 - val_loss: 0.6608\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6218 - val_loss: 0.6556\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6208 - val_loss: 0.6556\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6206 - val_loss: 0.6522\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6210 - val_loss: 0.6623\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6203 - val_loss: 0.6655\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6189 - val_loss: 0.6568\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6202 - val_loss: 0.6554\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6175 - val_loss: 0.6542\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6196 - val_loss: 0.6703\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6169 - val_loss: 0.6599\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6149 - val_loss: 0.6589\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6162 - val_loss: 0.6693\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6139 - val_loss: 0.6692\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6143 - val_loss: 0.6534\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6159 - val_loss: 0.6765\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6148 - val_loss: 0.6653\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6155 - val_loss: 0.6531\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6141 - val_loss: 0.6568\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6136 - val_loss: 0.6490\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6131 - val_loss: 0.6485\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6147 - val_loss: 0.6567\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6119 - val_loss: 0.6573\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6107 - val_loss: 0.6572\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6128 - val_loss: 0.6509\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6129 - val_loss: 0.6541\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6092 - val_loss: 0.6503\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6104 - val_loss: 0.6528\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6106 - val_loss: 0.6493\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6118 - val_loss: 0.6581\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6106 - val_loss: 0.6472\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6101 - val_loss: 0.6470\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6076 - val_loss: 0.6743\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6103 - val_loss: 0.6687\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6102 - val_loss: 0.6750\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6086 - val_loss: 0.6500\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6083 - val_loss: 0.6517\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6068 - val_loss: 0.6574\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6070 - val_loss: 0.6609\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6089 - val_loss: 0.6623\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6079 - val_loss: 0.6536\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6082 - val_loss: 0.6515\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6067 - val_loss: 0.6675\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6062 - val_loss: 0.6517\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6080 - val_loss: 0.6451\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6069 - val_loss: 0.6456\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6056 - val_loss: 0.6589\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6063 - val_loss: 0.6513\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6075 - val_loss: 0.6454\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6051 - val_loss: 0.6532\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6046 - val_loss: 0.6475\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6062 - val_loss: 0.6588\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6067 - val_loss: 0.6495\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6057 - val_loss: 0.6633\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6043 - val_loss: 0.6482\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6051 - val_loss: 0.6557\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6062 - val_loss: 0.6559\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6051 - val_loss: 0.6611\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6062 - val_loss: 0.6532\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6029 - val_loss: 0.6542\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6026 - val_loss: 0.6515\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6031 - val_loss: 0.6491\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6042 - val_loss: 0.6496\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6030 - val_loss: 0.6534\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6036 - val_loss: 0.6490\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6041 - val_loss: 0.6557\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6039 - val_loss: 0.6464\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6041 - val_loss: 0.6462\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6024 - val_loss: 0.6486\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6025 - val_loss: 0.6486\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6034 - val_loss: 0.6490\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6027 - val_loss: 0.6502\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6026 - val_loss: 0.6489\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6035 - val_loss: 0.6493\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6028 - val_loss: 0.6686\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6039 - val_loss: 0.6512\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6032 - val_loss: 0.6448\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6031 - val_loss: 0.6559\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6029 - val_loss: 0.6545\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6038 - val_loss: 0.6518\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6029 - val_loss: 0.6746\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6013 - val_loss: 0.6515\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6013 - val_loss: 0.6607\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6019 - val_loss: 0.6483\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6027 - val_loss: 0.6523\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6014 - val_loss: 0.6528\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6017 - val_loss: 0.6549\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6021 - val_loss: 0.6555\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6006 - val_loss: 0.6451\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.5996 - val_loss: 0.6545\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6013 - val_loss: 0.6530\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6030 - val_loss: 0.6489\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.5992 - val_loss: 0.6510\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6014 - val_loss: 0.6727\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.5994 - val_loss: 0.6606\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6013 - val_loss: 0.6454\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6011 - val_loss: 0.6507\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.5992 - val_loss: 0.6440\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6010 - val_loss: 0.6566\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.5999 - val_loss: 0.6539\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.5988 - val_loss: 0.6675\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6002 - val_loss: 0.6531\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 195us/step - loss: 5.9945 - val_loss: 0.7992\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 164us/step - loss: 0.7744 - val_loss: 0.7685\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 163us/step - loss: 0.7645 - val_loss: 0.7532\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 163us/step - loss: 0.7587 - val_loss: 0.7472\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 164us/step - loss: 0.7542 - val_loss: 0.7460\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 165us/step - loss: 0.7469 - val_loss: 0.7368\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.7387 - val_loss: 0.7353\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.7351 - val_loss: 0.7229\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.7264 - val_loss: 0.7216\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.7204 - val_loss: 0.7054\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.7136 - val_loss: 0.7029\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.7102 - val_loss: 0.7017\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.7038 - val_loss: 0.6974\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6993 - val_loss: 0.6977\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6943 - val_loss: 0.6928\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6912 - val_loss: 0.6847\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6865 - val_loss: 0.6841\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6830 - val_loss: 0.6816\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6802 - val_loss: 0.6914\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6759 - val_loss: 0.6846\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6730 - val_loss: 0.6736\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6728 - val_loss: 0.6757\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6681 - val_loss: 0.6794\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6663 - val_loss: 0.6738\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6643 - val_loss: 0.6709\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6644 - val_loss: 0.6815\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6585 - val_loss: 0.6625\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6578 - val_loss: 0.6891\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6566 - val_loss: 0.6573\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6543 - val_loss: 0.6603\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6542 - val_loss: 0.6769\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6507 - val_loss: 0.6546\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6515 - val_loss: 0.6580\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6491 - val_loss: 0.6586\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6492 - val_loss: 0.6612\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6456 - val_loss: 0.6554\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6450 - val_loss: 0.6607\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6428 - val_loss: 0.6576\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6429 - val_loss: 0.6733\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6443 - val_loss: 0.7105\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6412 - val_loss: 0.6526\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6378 - val_loss: 0.6529\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6377 - val_loss: 0.6506\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6373 - val_loss: 0.6564\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6376 - val_loss: 0.6525\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6332 - val_loss: 0.6552\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6338 - val_loss: 0.6544\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6306 - val_loss: 0.6517\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6304 - val_loss: 0.6529\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6304 - val_loss: 0.6474\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6295 - val_loss: 0.6603\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6301 - val_loss: 0.6435\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6288 - val_loss: 0.6471\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6281 - val_loss: 0.6565\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6260 - val_loss: 0.6500\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6243 - val_loss: 0.6488\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6258 - val_loss: 0.6442\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6236 - val_loss: 0.6609\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6224 - val_loss: 0.6442\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6218 - val_loss: 0.6433\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6213 - val_loss: 0.6486\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6202 - val_loss: 0.6546\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6175 - val_loss: 0.6698\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6192 - val_loss: 0.6402\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6167 - val_loss: 0.6461\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6169 - val_loss: 0.6478\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6171 - val_loss: 0.6462\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6145 - val_loss: 0.6362\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6135 - val_loss: 0.6421\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6144 - val_loss: 0.6436\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6129 - val_loss: 0.6394\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6138 - val_loss: 0.6555\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6141 - val_loss: 0.6347\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6116 - val_loss: 0.6535\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6108 - val_loss: 0.6358\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6111 - val_loss: 0.6411\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6110 - val_loss: 0.6528\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6082 - val_loss: 0.6333\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6108 - val_loss: 0.6354\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6075 - val_loss: 0.6343\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6075 - val_loss: 0.6401\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6086 - val_loss: 0.6419\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6079 - val_loss: 0.6483\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6070 - val_loss: 0.6395\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6078 - val_loss: 0.6414\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6048 - val_loss: 0.6354\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6047 - val_loss: 0.6381\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6064 - val_loss: 0.6308\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.6030 - val_loss: 0.6378\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6043 - val_loss: 0.6343\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6031 - val_loss: 0.6543\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6024 - val_loss: 0.6334\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6018 - val_loss: 0.6372\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6027 - val_loss: 0.6489\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6036 - val_loss: 0.6314\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6028 - val_loss: 0.6383\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6020 - val_loss: 0.6356\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6009 - val_loss: 0.6359\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6011 - val_loss: 0.6347\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6014 - val_loss: 0.6307\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6026 - val_loss: 0.6317\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6004 - val_loss: 0.6301\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.6013 - val_loss: 0.6687\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5997 - val_loss: 0.6349\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.6001 - val_loss: 0.6382\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5983 - val_loss: 0.6342\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5990 - val_loss: 0.6348\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5984 - val_loss: 0.6539\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.5999 - val_loss: 0.6288\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5999 - val_loss: 0.6391\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5960 - val_loss: 0.6376\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5964 - val_loss: 0.6322\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5969 - val_loss: 0.6362\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5973 - val_loss: 0.6330\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5989 - val_loss: 0.6357\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5958 - val_loss: 0.6270\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5978 - val_loss: 0.6315\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5969 - val_loss: 0.6293\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5957 - val_loss: 0.6412\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5946 - val_loss: 0.6503\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5945 - val_loss: 0.6346\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5959 - val_loss: 0.6293\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5930 - val_loss: 0.6259\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5950 - val_loss: 0.6321\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5960 - val_loss: 0.6312\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5918 - val_loss: 0.6251\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5938 - val_loss: 0.6616\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5924 - val_loss: 0.6328\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5925 - val_loss: 0.6338\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5921 - val_loss: 0.6500\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5932 - val_loss: 0.6440\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5899 - val_loss: 0.6323\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5918 - val_loss: 0.6274\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5903 - val_loss: 0.6421\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5901 - val_loss: 0.6274\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5912 - val_loss: 0.6335\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5893 - val_loss: 0.6257\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5920 - val_loss: 0.6395\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5920 - val_loss: 0.6241\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5900 - val_loss: 0.6296\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5907 - val_loss: 0.6282\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5885 - val_loss: 0.6323\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5887 - val_loss: 0.6276\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5890 - val_loss: 0.6430\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5885 - val_loss: 0.6396\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5893 - val_loss: 0.6286\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5897 - val_loss: 0.6284\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.5871 - val_loss: 0.6418\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.5891 - val_loss: 0.6219\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.5874 - val_loss: 0.6289\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 197us/step - loss: 5.9139 - val_loss: 0.7773\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7770 - val_loss: 0.7453\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7653 - val_loss: 0.7350\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7577 - val_loss: 0.7355\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7518 - val_loss: 0.7354\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7448 - val_loss: 0.7286\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7400 - val_loss: 0.7246\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7330 - val_loss: 0.7240\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7275 - val_loss: 0.7257\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.7206 - val_loss: 0.7116\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7155 - val_loss: 0.7206\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7129 - val_loss: 0.7031\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.7058 - val_loss: 0.7202\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.7040 - val_loss: 0.7169\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6992 - val_loss: 0.7004\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6960 - val_loss: 0.6989\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6936 - val_loss: 0.6990\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6882 - val_loss: 0.6897\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6856 - val_loss: 0.6844\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6842 - val_loss: 0.6819\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6814 - val_loss: 0.6815\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6776 - val_loss: 0.6829\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6770 - val_loss: 0.6856\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6736 - val_loss: 0.6863\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6714 - val_loss: 0.6753\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6661 - val_loss: 0.6801\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6664 - val_loss: 0.6760\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6631 - val_loss: 0.6724\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6595 - val_loss: 0.6677\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6584 - val_loss: 0.6831\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6563 - val_loss: 0.6723\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6552 - val_loss: 0.6821\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6546 - val_loss: 0.6766\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6524 - val_loss: 0.6649\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6494 - val_loss: 0.6721\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6509 - val_loss: 0.6906\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6502 - val_loss: 0.6654\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6487 - val_loss: 0.6590\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6442 - val_loss: 0.6611\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6443 - val_loss: 0.6618\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6439 - val_loss: 0.6625\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6429 - val_loss: 0.6564\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6432 - val_loss: 0.6524\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6407 - val_loss: 0.6623\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6394 - val_loss: 0.6535\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6372 - val_loss: 0.6669\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6404 - val_loss: 0.6674\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6368 - val_loss: 0.6950\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6361 - val_loss: 0.6542\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6374 - val_loss: 0.6503\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6347 - val_loss: 0.6576\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6318 - val_loss: 0.6542\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6323 - val_loss: 0.6786\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6321 - val_loss: 0.6565\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6311 - val_loss: 0.6585\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6308 - val_loss: 0.6592\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6316 - val_loss: 0.6713\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6298 - val_loss: 0.6554\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6315 - val_loss: 0.6672\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6278 - val_loss: 0.6513\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6322 - val_loss: 0.6601\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6284 - val_loss: 0.6520\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6276 - val_loss: 0.6570\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6280 - val_loss: 0.6531\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6246 - val_loss: 0.6478\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6236 - val_loss: 0.6556\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6261 - val_loss: 0.6495\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6233 - val_loss: 0.6497\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6241 - val_loss: 0.6485\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6243 - val_loss: 0.6572\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6231 - val_loss: 0.6519\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6234 - val_loss: 0.6490\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6240 - val_loss: 0.6497\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6225 - val_loss: 0.6529\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6227 - val_loss: 0.6466\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6241 - val_loss: 0.6592\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6204 - val_loss: 0.6519\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6225 - val_loss: 0.6492\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6219 - val_loss: 0.6529\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6196 - val_loss: 0.6533\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6195 - val_loss: 0.6465\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6197 - val_loss: 0.7047\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6184 - val_loss: 0.6616\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6185 - val_loss: 0.6470\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6188 - val_loss: 0.6676\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6191 - val_loss: 0.6655\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6183 - val_loss: 0.6516\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6161 - val_loss: 0.6609\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6179 - val_loss: 0.6615\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6180 - val_loss: 0.6478\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6165 - val_loss: 0.6491\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6185 - val_loss: 0.6494\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6165 - val_loss: 0.6555\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6150 - val_loss: 0.6484\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6162 - val_loss: 0.6604\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6165 - val_loss: 0.6533\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6168 - val_loss: 0.6676\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6167 - val_loss: 0.6529\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6157 - val_loss: 0.6555\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6131 - val_loss: 0.6435\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6143 - val_loss: 0.6472\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6159 - val_loss: 0.6464\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.6129 - val_loss: 0.6502\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6134 - val_loss: 0.6508\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6163 - val_loss: 0.6451\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6145 - val_loss: 0.6782\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6113 - val_loss: 0.6607\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6149 - val_loss: 0.6463\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6133 - val_loss: 0.6465\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6118 - val_loss: 0.6800\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6117 - val_loss: 0.6513\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6110 - val_loss: 0.6572\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6121 - val_loss: 0.6794\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6111 - val_loss: 0.6481\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6122 - val_loss: 0.6565\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6111 - val_loss: 0.6630\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 178us/step - loss: 0.6110 - val_loss: 0.6487\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6111 - val_loss: 0.6566\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6110 - val_loss: 0.6451\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6131 - val_loss: 0.6605\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6101 - val_loss: 0.6491\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6100 - val_loss: 0.6452\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6093 - val_loss: 0.6433\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6106 - val_loss: 0.6425\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6080 - val_loss: 0.6529\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6101 - val_loss: 0.6687\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6093 - val_loss: 0.6452\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6101 - val_loss: 0.6502\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6084 - val_loss: 0.6586\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6096 - val_loss: 0.6656\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6106 - val_loss: 0.6475\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6086 - val_loss: 0.6465\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6074 - val_loss: 0.6431\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6104 - val_loss: 0.6537\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6088 - val_loss: 0.6449\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6068 - val_loss: 0.6459\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6094 - val_loss: 0.6533\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6081 - val_loss: 0.6497\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6084 - val_loss: 0.6466\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6074 - val_loss: 0.6507\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6083 - val_loss: 0.6515\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6073 - val_loss: 0.6605\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6079 - val_loss: 0.6455\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6087 - val_loss: 0.6472\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6074 - val_loss: 0.6438\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6077 - val_loss: 0.6577\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6062 - val_loss: 0.6474\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6071 - val_loss: 0.6482\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6061 - val_loss: 0.6633\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6054 - val_loss: 0.6473\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/150\n",
      "12800/12800 [==============================] - 3s 201us/step - loss: 6.0223 - val_loss: 0.8004\n",
      "Epoch 2/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.7722 - val_loss: 0.7658\n",
      "Epoch 3/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.7585 - val_loss: 0.7533\n",
      "Epoch 4/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.7551 - val_loss: 0.7471\n",
      "Epoch 5/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.7499 - val_loss: 0.7413\n",
      "Epoch 6/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.7438 - val_loss: 0.7418\n",
      "Epoch 7/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7383 - val_loss: 0.7755\n",
      "Epoch 8/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7333 - val_loss: 0.7373\n",
      "Epoch 9/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7234 - val_loss: 0.7172\n",
      "Epoch 10/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7188 - val_loss: 0.7195\n",
      "Epoch 11/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.7104 - val_loss: 0.7208\n",
      "Epoch 12/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.7062 - val_loss: 0.7015\n",
      "Epoch 13/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6992 - val_loss: 0.7199\n",
      "Epoch 14/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6932 - val_loss: 0.7104\n",
      "Epoch 15/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6868 - val_loss: 0.6911\n",
      "Epoch 16/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6839 - val_loss: 0.7256\n",
      "Epoch 17/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6798 - val_loss: 0.6802\n",
      "Epoch 18/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6771 - val_loss: 0.6926\n",
      "Epoch 19/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6705 - val_loss: 0.6764\n",
      "Epoch 20/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6666 - val_loss: 0.6758\n",
      "Epoch 21/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6649 - val_loss: 0.6785\n",
      "Epoch 22/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6582 - val_loss: 0.7072\n",
      "Epoch 23/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6579 - val_loss: 0.6632\n",
      "Epoch 24/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6548 - val_loss: 0.6674\n",
      "Epoch 25/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6527 - val_loss: 0.6661\n",
      "Epoch 26/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6502 - val_loss: 0.6645\n",
      "Epoch 27/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6487 - val_loss: 0.6956\n",
      "Epoch 28/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6456 - val_loss: 0.6649\n",
      "Epoch 29/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6446 - val_loss: 0.6749\n",
      "Epoch 30/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6439 - val_loss: 0.6631\n",
      "Epoch 31/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6422 - val_loss: 0.6617\n",
      "Epoch 32/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6392 - val_loss: 0.6634\n",
      "Epoch 33/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6398 - val_loss: 0.6568\n",
      "Epoch 34/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6400 - val_loss: 0.6690\n",
      "Epoch 35/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6369 - val_loss: 0.6571\n",
      "Epoch 36/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6347 - val_loss: 0.6566\n",
      "Epoch 37/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6329 - val_loss: 0.6570\n",
      "Epoch 38/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6341 - val_loss: 0.6747\n",
      "Epoch 39/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6340 - val_loss: 0.6675\n",
      "Epoch 40/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6319 - val_loss: 0.6620\n",
      "Epoch 41/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6312 - val_loss: 0.6558\n",
      "Epoch 42/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6282 - val_loss: 0.6652\n",
      "Epoch 43/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6248 - val_loss: 0.6556\n",
      "Epoch 44/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6285 - val_loss: 0.6534\n",
      "Epoch 45/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6289 - val_loss: 0.6565\n",
      "Epoch 46/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6261 - val_loss: 0.6660\n",
      "Epoch 47/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6263 - val_loss: 0.6491\n",
      "Epoch 48/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6244 - val_loss: 0.6512\n",
      "Epoch 49/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6235 - val_loss: 0.6537\n",
      "Epoch 50/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6233 - val_loss: 0.6531\n",
      "Epoch 51/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6231 - val_loss: 0.6506\n",
      "Epoch 52/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6244 - val_loss: 0.6504\n",
      "Epoch 53/150\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 0.6210 - val_loss: 0.6597\n",
      "Epoch 54/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6219 - val_loss: 0.6581\n",
      "Epoch 55/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6207 - val_loss: 0.6537\n",
      "Epoch 56/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6201 - val_loss: 0.6553\n",
      "Epoch 57/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6189 - val_loss: 0.6492\n",
      "Epoch 58/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6184 - val_loss: 0.6497\n",
      "Epoch 59/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6171 - val_loss: 0.6600\n",
      "Epoch 60/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6186 - val_loss: 0.6666\n",
      "Epoch 61/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6163 - val_loss: 0.6466\n",
      "Epoch 62/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6177 - val_loss: 0.6508\n",
      "Epoch 63/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6179 - val_loss: 0.6512\n",
      "Epoch 64/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6172 - val_loss: 0.6493\n",
      "Epoch 65/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6169 - val_loss: 0.6739\n",
      "Epoch 66/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6156 - val_loss: 0.6602\n",
      "Epoch 67/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6167 - val_loss: 0.6510\n",
      "Epoch 68/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6157 - val_loss: 0.6474\n",
      "Epoch 69/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6152 - val_loss: 0.6509\n",
      "Epoch 70/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6159 - val_loss: 0.6487\n",
      "Epoch 71/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6145 - val_loss: 0.6583\n",
      "Epoch 72/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6129 - val_loss: 0.6723\n",
      "Epoch 73/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6142 - val_loss: 0.6675\n",
      "Epoch 74/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6116 - val_loss: 0.6502\n",
      "Epoch 75/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6123 - val_loss: 0.6471\n",
      "Epoch 76/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6107 - val_loss: 0.6442\n",
      "Epoch 77/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6128 - val_loss: 0.6440\n",
      "Epoch 78/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6137 - val_loss: 0.6451\n",
      "Epoch 79/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6095 - val_loss: 0.6526\n",
      "Epoch 80/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6109 - val_loss: 0.6474\n",
      "Epoch 81/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6106 - val_loss: 0.6453\n",
      "Epoch 82/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6117 - val_loss: 0.6494\n",
      "Epoch 83/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6097 - val_loss: 0.6495\n",
      "Epoch 84/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6105 - val_loss: 0.6502\n",
      "Epoch 85/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6108 - val_loss: 0.6501\n",
      "Epoch 86/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6107 - val_loss: 0.6466\n",
      "Epoch 87/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6085 - val_loss: 0.6507\n",
      "Epoch 88/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6083 - val_loss: 0.6543\n",
      "Epoch 89/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6109 - val_loss: 0.6470\n",
      "Epoch 90/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6098 - val_loss: 0.6429\n",
      "Epoch 91/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6085 - val_loss: 0.6430\n",
      "Epoch 92/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6102 - val_loss: 0.6436\n",
      "Epoch 93/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6087 - val_loss: 0.6530\n",
      "Epoch 94/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6099 - val_loss: 0.6678\n",
      "Epoch 95/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6101 - val_loss: 0.6450\n",
      "Epoch 96/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6058 - val_loss: 0.6428\n",
      "Epoch 97/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6085 - val_loss: 0.6509\n",
      "Epoch 98/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6079 - val_loss: 0.6545\n",
      "Epoch 99/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6070 - val_loss: 0.6559\n",
      "Epoch 100/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6081 - val_loss: 0.6834\n",
      "Epoch 101/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6062 - val_loss: 0.6541\n",
      "Epoch 102/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6063 - val_loss: 0.6519\n",
      "Epoch 103/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6056 - val_loss: 0.6472\n",
      "Epoch 104/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6068 - val_loss: 0.6451\n",
      "Epoch 105/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6055 - val_loss: 0.6434\n",
      "Epoch 106/150\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.6038 - val_loss: 0.6561\n",
      "Epoch 107/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6038 - val_loss: 0.6617\n",
      "Epoch 108/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6060 - val_loss: 0.6520\n",
      "Epoch 109/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6052 - val_loss: 0.6423\n",
      "Epoch 110/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6040 - val_loss: 0.6490\n",
      "Epoch 111/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6046 - val_loss: 0.6421\n",
      "Epoch 112/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6077 - val_loss: 0.6498\n",
      "Epoch 113/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6058 - val_loss: 0.6659\n",
      "Epoch 114/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6059 - val_loss: 0.6440\n",
      "Epoch 115/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6035 - val_loss: 0.6580\n",
      "Epoch 116/150\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.6050 - val_loss: 0.6502\n",
      "Epoch 117/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6043 - val_loss: 0.6467\n",
      "Epoch 118/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6037 - val_loss: 0.6470\n",
      "Epoch 119/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6036 - val_loss: 0.6476\n",
      "Epoch 120/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6033 - val_loss: 0.6509\n",
      "Epoch 121/150\n",
      "12800/12800 [==============================] - 2s 178us/step - loss: 0.6054 - val_loss: 0.6455\n",
      "Epoch 122/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6029 - val_loss: 0.6509\n",
      "Epoch 123/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6020 - val_loss: 0.6467\n",
      "Epoch 124/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6029 - val_loss: 0.6647\n",
      "Epoch 125/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6023 - val_loss: 0.6416\n",
      "Epoch 126/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6027 - val_loss: 0.6545\n",
      "Epoch 127/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6025 - val_loss: 0.6393\n",
      "Epoch 128/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6022 - val_loss: 0.6487\n",
      "Epoch 129/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6022 - val_loss: 0.6489\n",
      "Epoch 130/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6023 - val_loss: 0.6414\n",
      "Epoch 131/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6014 - val_loss: 0.6448\n",
      "Epoch 132/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6018 - val_loss: 0.6836\n",
      "Epoch 133/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6020 - val_loss: 0.6595\n",
      "Epoch 134/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6025 - val_loss: 0.6543\n",
      "Epoch 135/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6020 - val_loss: 0.6600\n",
      "Epoch 136/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6024 - val_loss: 0.6496\n",
      "Epoch 137/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6000 - val_loss: 0.6419\n",
      "Epoch 138/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6014 - val_loss: 0.6423\n",
      "Epoch 139/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6014 - val_loss: 0.6468\n",
      "Epoch 140/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.5992 - val_loss: 0.6459\n",
      "Epoch 141/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.5999 - val_loss: 0.6416\n",
      "Epoch 142/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6006 - val_loss: 0.6475\n",
      "Epoch 143/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6024 - val_loss: 0.6401\n",
      "Epoch 144/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6031 - val_loss: 0.6438\n",
      "Epoch 145/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6017 - val_loss: 0.6467\n",
      "Epoch 146/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6006 - val_loss: 0.6395\n",
      "Epoch 147/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6011 - val_loss: 0.6540\n",
      "Epoch 148/150\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.6003 - val_loss: 0.6469\n",
      "Epoch 149/150\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.6013 - val_loss: 0.6449\n",
      "Epoch 150/150\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.6017 - val_loss: 0.6392\n"
     ]
    }
   ],
   "source": [
    "# To save time: always apply PCA for now\n",
    "apply_pca = True\n",
    "apply_iqr = [False, True]\n",
    "n_samples = [5000, 20000] #[5000, 20000]\n",
    "n_iter = 4 #5\n",
    "\n",
    "noiqr_5000 = []\n",
    "noiqr_20000 = []\n",
    "iqr_5000 = []\n",
    "iqr_20000 = []\n",
    "\n",
    "for b in apply_iqr:\n",
    "    for n in n_samples:\n",
    "        for i in range(n_iter):\n",
    "            # load each time a different set (kind of cross-val)\n",
    "            X_train, X_test, y_train, y_test = load_data_train_test(n, X_tot, y_tot, iqr=b)\n",
    "            \n",
    "            # PCA\n",
    "            if apply_pca:\n",
    "                n_pcs = 80\n",
    "                X_train, X_test = do_PCA(X_train, X_test, n_pcs)\n",
    "                #nb_input_neurons = n\n",
    "            \n",
    "            # NN\n",
    "            k_ann = k_models.model_6(X_train, 'mean_absolute_error') \n",
    "            k_ann.fit(X_train, y_train, epochs=150, batch_size=10, validation_split=0.2)\n",
    "            \n",
    "            y_hat = k_ann.predict(X_test)\n",
    "            mse_nn, mae_nn = compute_score(y_test, y_hat)\n",
    "            \n",
    "            # Store result\n",
    "            # Case: iqr, 5000 samples\n",
    "            if (b and (n == n_samples[0])):\n",
    "                iqr_5000.append(mse_nn)\n",
    "            elif ((not b) and (n == n_samples[0])):\n",
    "                noiqr_5000.append(mse_nn)\n",
    "            elif (b and (n == n_samples[1])):\n",
    "                iqr_20000.append(mse_nn)\n",
    "            else:\n",
    "                noiqr_20000.append(mse_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stats_iqr_vs_n_2.pkl', 'wb') as f:\n",
    "    pickle.dump([noiqr_5000, noiqr_20000, iqr_5000, iqr_20000], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Figures\n",
    "<a id='figures'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Distributions\n",
    "<a id='fig_dist'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-5, 65), Text(0.5, 0, 'Shielding')]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "sns.set(font_scale=1) \n",
    "plt.subplot(2,2,1)\n",
    "plt.title('Shielding distribution \\n (w/o outlier removal)', fontsize=15)\n",
    "sns.distplot(y_tot).set(xlim=(-5,65),ylim=(0,0.14))\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('Shielding distribution \\n (after applying IQR)', fontsize=15)\n",
    "sns.distplot(y_filtered).set(xlim=(-5,65),ylim=(0,0.14))\n",
    "plt.subplot(2,2,3)\n",
    "b1 = sns.boxplot(x=y_tot, linewidth=1.5).set(xlabel='Shielding', xlim=(-5,65))\n",
    "#b1.set_xlabel(\"Shielding\",fontsize=10)\n",
    "plt.subplot(2,2,4)\n",
    "sns.boxplot(x=y_filtered, linewidth=1.5).set(xlabel='Shielding', xlim=(-5,65))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"iqr.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 IQR\n",
    "<a id='fig_iqr'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noiqr_5000</th>\n",
       "      <th>noiqr_20000</th>\n",
       "      <th>iqr_5000</th>\n",
       "      <th>iqr_20000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.966410</td>\n",
       "      <td>1.164088</td>\n",
       "      <td>0.891626</td>\n",
       "      <td>0.832680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.286497</td>\n",
       "      <td>1.257255</td>\n",
       "      <td>0.887988</td>\n",
       "      <td>0.744232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.757331</td>\n",
       "      <td>0.852787</td>\n",
       "      <td>1.002124</td>\n",
       "      <td>0.833200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.238798</td>\n",
       "      <td>1.663109</td>\n",
       "      <td>1.009964</td>\n",
       "      <td>0.722377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   noiqr_5000  noiqr_20000  iqr_5000  iqr_20000\n",
       "0    1.966410     1.164088  0.891626   0.832680\n",
       "1    1.286497     1.257255  0.887988   0.744232\n",
       "2    1.757331     0.852787  1.002124   0.833200\n",
       "3    1.238798     1.663109  1.009964   0.722377"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iqr = pd.DataFrame({\n",
    "    'noiqr_5000': noiqr_5000,\n",
    "    'noiqr_20000': noiqr_20000,\n",
    "    'iqr_5000': iqr_5000,\n",
    "    'iqr_20000': iqr_20000\n",
    "})\n",
    "df_iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>iqr</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.966410</td>\n",
       "      <td>no</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.286497</td>\n",
       "      <td>no</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.757331</td>\n",
       "      <td>no</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.238798</td>\n",
       "      <td>no</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.164088</td>\n",
       "      <td>no</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.257255</td>\n",
       "      <td>no</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.852787</td>\n",
       "      <td>no</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.663109</td>\n",
       "      <td>no</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.891626</td>\n",
       "      <td>yes</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.887988</td>\n",
       "      <td>yes</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.002124</td>\n",
       "      <td>yes</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.009964</td>\n",
       "      <td>yes</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.832680</td>\n",
       "      <td>yes</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.744232</td>\n",
       "      <td>yes</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.833200</td>\n",
       "      <td>yes</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.722377</td>\n",
       "      <td>yes</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mse  iqr      n\n",
       "0   1.966410   no   5000\n",
       "1   1.286497   no   5000\n",
       "2   1.757331   no   5000\n",
       "3   1.238798   no   5000\n",
       "4   1.164088   no  20000\n",
       "5   1.257255   no  20000\n",
       "6   0.852787   no  20000\n",
       "7   1.663109   no  20000\n",
       "8   0.891626  yes   5000\n",
       "9   0.887988  yes   5000\n",
       "10  1.002124  yes   5000\n",
       "11  1.009964  yes   5000\n",
       "12  0.832680  yes  20000\n",
       "13  0.744232  yes  20000\n",
       "14  0.833200  yes  20000\n",
       "15  0.722377  yes  20000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_iqr_n = noiqr_5000 + noiqr_20000 + iqr_5000 + iqr_20000\n",
    "\n",
    "iqrs_labels = ['no','no','no','no','no','no','no','no','yes','yes','yes','yes','yes','yes','yes','yes']\n",
    "n_labels = [5000,5000,5000,5000,20000,20000,20000,20000,5000,5000,5000,5000,20000,20000,20000,20000]\n",
    "\n",
    "my_df = pd.DataFrame({\n",
    "    'mse': tot_iqr_n,\n",
    "    'iqr': iqrs_labels,\n",
    "    'n': n_labels\n",
    "})\n",
    "my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iqr_vs_n, _ = plt.subplots(figsize=(10,8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "ax = sns.boxplot(data=my_df, x='iqr', y='mse', hue='n', linewidth=1.)\n",
    "ax.set_xticklabels(['No IQR', 'IQR'], fontsize=20)\n",
    "ax.set_xlabel('', fontsize=20)\n",
    "ax.set_ylabel('MSE', fontsize=20)\n",
    "\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='20')\n",
    "plt.setp(ax.get_legend().get_title(), fontsize='28')\n",
    "\n",
    "for patch in ax.artists:\n",
    "    r, g, b, a = patch.get_facecolor()\n",
    "    patch.set_facecolor((r, g, b, .75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_vs_n.savefig(\"iqr_vs_n_2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Feature distributions\n",
    "<a id='fig_feature_dist'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_number = [12711, 1457, 4502]\n",
    "\n",
    "feature_0 = X_tot[:, feature_number[0]]\n",
    "feature_1 = X_tot[:, feature_number[1]]\n",
    "feature_2 = X_tot[:, feature_number[2]]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(14,5))\n",
    "\n",
    "# Divide the figure into a 2x1 grid, and give me the first section\n",
    "ax_1 = fig.add_subplot(131)\n",
    "\n",
    "# Divide the figure into a 2x1 grid, and give me the second section\n",
    "ax_2 = fig.add_subplot(132)\n",
    "ax_3 = fig.add_subplot(133)\n",
    "\n",
    "\n",
    "sns.distplot(feature_0, ax=ax_1, kde=False, hist_kws=dict(edgecolor=\"w\", linewidth=1))\n",
    "\n",
    "ax_1.axes.set_title(\"Feature {}\".format(str(feature_number[0]), fontsize=20))\n",
    "ax_1.set_xlabel(\"\", fontsize=17)\n",
    "ax_1.set_ylabel(\"Count\",fontsize=17)\n",
    "\n",
    "sns.distplot(feature_1, ax=ax_2, kde=False, hist_kws=dict(edgecolor=\"w\", linewidth=1))\n",
    "\n",
    "ax_2.axes.set_title(\"Feature {}\".format(str(feature_number[1]), fontsize=20))\n",
    "ax_2.set_xlabel(\"Feature value\", fontsize=17)\n",
    "ax_2.set_ylabel(\"\", fontsize=17)\n",
    "                    \n",
    "sns.distplot(feature_2, ax=ax_3, kde=False, hist_kws=dict(edgecolor=\"w\", linewidth=1))\n",
    "\n",
    "ax_3.axes.set_title(\"Feature {}\".format(str(feature_number[2]), fontsize=20))\n",
    "ax_3.set_xlabel(\"\", fontsize=17)\n",
    "ax_3.set_ylabel(\"\", fontsize=17)\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note NN\n",
    "- Simple model is the best (1 hidden of 100 neurons)\n",
    "- For now dropout does not help\n",
    "- Increasing the number of hidden neurons to 200 does not help\n",
    "- Decreasing the number of hidden neurons to 50 shows problem of cenvergence\n",
    "- Increasing the number of samples show little or no improvements (filter with **IQR** and **minmaxscaler** before and/or **PCA**)\n",
    "- Test Dropout in PyTorch (specify when training and when NOT training)!\n",
    "- Again even with batch norm, the simplest model with one hidden seems the best\n",
    "- Keras: Adam better for model 3 with 100 neurons in hidden layer (sinon does not converge with SGD). Can also try with opt='adam' (...)\n",
    "- **OK definitely best with 100 hidden neurons and adam as optimizer**\n",
    "- Now check with PCA, min_max scaling\n",
    "- Increasing the number of epochs seems a good idea\n",
    "- IQR helps (ouf)!\n",
    "- Increasing the number of samples helps also (ouf)!\n",
    "- Not sure if PCA useful with Keras models\n",
    "- Model_6 still the best at this point. Try with DEEP net (lots of 'small' layers)?\n",
    "\n",
    "> About PCA\n",
    "- Shall we normalize before?\n",
    "- How does the PCA method from sklearn work? Normalize before?\n",
    "- When trying without PCA > apply a SCALER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
