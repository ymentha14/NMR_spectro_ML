{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification ML project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import SVG\n",
    "SVG(filename='../data/pipeline.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logistic as log\n",
    "import split as spl\n",
    "import least_squares as lst\n",
    "import helpers as hlp\n",
    "import pre_processing as pre\n",
    "import vizu as viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "DATA_TEST_PATH = '../data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd # cannot use external libraries, just pandas for data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y, tX, ids = hlp.load_csv_data(DATA_TRAIN_PATH)\n",
    "_, tX_test, ids_test = hlp.load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tX.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATA_TRAIN_PATH)\n",
    "test_data = pd.read_csv(DATA_TEST_PATH)\n",
    "dic = {'s':1,'b':-1}\n",
    "data.Prediction = data.Prediction.map(dic)\n",
    "test_data.Prediction = test_data.Prediction.map(dic)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = data.isin([-999]).any(axis = 1)\n",
    "print(len(data[mask]))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The vast majoriy of our data has -999 values: we'd better handle it carefully_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.replace(to_replace = -999,value = np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace_val = np.nan\n",
    "#tX = np.where(tX == -999,replace_val,tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.nanstd(tX,axis = 0)\n",
    "mean = np.nanmean(tX,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train set size: {} samples x {} features'.format(pd.DataFrame(tX).shape[0], pd.DataFrame(tX).shape[1]))\n",
    "print('Test set size: {} samples x {} features'.format(test_data.shape[0], pd.DataFrame(tX).shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#totrash before submit: we use pandas to know to which index PRI_jet_num does correspond.\n",
    "np.where(data.columns.values == \"PRI_jet_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trains = spl.split_categorical_data(tX,22,labels = y,split = True)\n",
    "data_tests = spl.split_categorical_data(tX_test,22,split = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0\n",
    "stdev = 0\n",
    "clean_data_trains = []\n",
    "clean_data_tests = []\n",
    "degre_polys = [12,12,13]\n",
    "for i,((x_train,y_train),(x_test,test_indx)) in enumerate(zip(data_trains,data_tests)):\n",
    "    x_train,x_test = pre.clean_variance(x_train,x_test)\n",
    "    \n",
    "    x_train = pre.clean_value(x_train,-999,np.nan)\n",
    "    x_test = pre.clean_value(x_test,-999,np.nan)\n",
    "    \n",
    "    \"\"\"\n",
    "    pre.PCA_visualize(x_train,label = i)\n",
    "    \n",
    "    mean,eigvecs,eigvals = pre.get_PCA(x_train)\n",
    "    x_test = x_test - mean\n",
    "    \n",
    "    x_train = pre.reduce_PCA(eigvecs,x_train,10)\n",
    "    x_test = pre.reduce_PCA(eigvecs,x_test,10)\n",
    "    \"\"\"\n",
    "    \n",
    "    x_train,mean,stdev =  pre.standardize_data(x_train)\n",
    "    x_test,_,_ = pre.standardize_data(x_test, mean,stdev)\n",
    "    \n",
    "    x_train = pre.clean_value(x_train,np.nan,0,inplace = True)\n",
    "    x_test = pre.clean_value(x_test,np.nan,0,inplace = True)\n",
    "    \n",
    "    \n",
    "    x_train = pre.build_poly(x_train, degre_polys[i])\n",
    "    x_test = pre.build_poly(x_test,degre_polys[i])\n",
    "    x_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
    "    x_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]\n",
    "    \n",
    "    clean_data_trains.append((x_train,y_train))\n",
    "    clean_data_tests.append((x_test,test_indx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Ridge regression avec optimization de lamdbas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We now need to standardize the function so that they all take the same type of parameters as inputs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_w = np.random.rand(clean_data_tests[0][0].shape[1])\n",
    "maxiters = 100\n",
    "gamma = 0.01\n",
    "\n",
    "#method 1\n",
    "meth1 = lambda  y, x: lst.ridge_regression(y,x,5.17E-5)\n",
    "\n",
    "if (len(clean_data_tests) > 1):\n",
    "    init_w2 = np.random.rand(clean_data_tests[1][0].shape[1])\n",
    "    #method 2\n",
    "    #reg_log_reg = lambda y,x : log.reg_logistic_regression(y, x, lambda_, init_w2, maxiters, gamma)\n",
    "    #meth2 = lambda  y, x: log.logistic_regression(y,x,init_w2,5,gamma)\n",
    "    meth2 = lambda y,x : lst.ridge_regression(y,x,0.0013)\n",
    "\n",
    "    init_w3 = np.random.rand(clean_data_tests[2][0].shape[1])\n",
    "    #method 3\n",
    "    lambda_ = 0.1\n",
    "    meth3 = lambda y, x: lst.ridge_regression(y,x,0.001389)\n",
    "    #log_reg3 = lambda  y, x: log.logistic_regression(y,x,init_w3,5,gamma)\n",
    "\n",
    "methods = [meth1,meth2,meth3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_At this point we try the different models defined in the cell above: to do so run the cell below, and check the obtained accuracies._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kfold for the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_group_means = []\n",
    "accuracies_group_stds = []\n",
    "cutoffs_group = []\n",
    "used_metric = hlp.f1\n",
    "for round_,((x_train,y_train),meth) in enumerate(zip(clean_data_trains,methods)):\n",
    "    print(\"#################################\")\n",
    "    print(\"**********treating the {i}th group of data:**************\".format(i = round_+1))\n",
    "    accuracies, accu_stds,opt_cutoffs = spl.k_fold_cv(y_train,x_train,2,meth,metric = used_metric)\n",
    "    accuracies_group_means.append(accuracies)\n",
    "    accuracies_group_stds.append(accu_stds)\n",
    "    cutoffs_group.append(opt_cutoffs)\n",
    "print(\"\\n done! Obtained :\" + hlp.dico[used_metric],[np.mean(i) for i in accuracies_group_means])\n",
    "print(\"ideal cutoffs for these groups-methods pairs:\",[np.mean(i) for i in cutoffs_group])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kfold for the decision threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(-1,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = np.linspace(-2,2,20)\n",
    "metric_trains = []\n",
    "metric_tests = []\n",
    "opt_cutoffs = []\n",
    "for round_,((x_train,y_train),meth) in enumerate(zip(clean_data_trains,methods)):\n",
    "    print(\"#################################\")\n",
    "    print(\"**********treating the {i}th group of data:**************\".format(i = round_+1))\n",
    "    metric_train,metric_test,opt_cutoff = spl.k_fold_cutoff(y_train,x_train,4,meth,cutoffs,metric = hlp.accuracy)\n",
    "    metric_trains.append(metric_train)\n",
    "    metric_tests.append(metric_test)\n",
    "    opt_cutoffs.append(opt_cutoff)\n",
    "print(\"Obtained average metric on the test sets: \",[np.mean(i) for i in metric_tests])\n",
    "print(\"Obtained average metric on the train sets: \",[np.mean(i) for i in metric_trains])\n",
    "#one cutoff per fold, 3 groups ==> k * 3 cutoffs\n",
    "print(\"Optimal obtained cutoffs: (k * 3)\",opt_cutoffs)\n",
    "\n",
    "#best cutoffs for accuracy as a metric: [[-0.10526315789473695, -0.10526315789473695, 2.0, 0.10526315789473673], [0.10526315789473673, 0.10526315789473673, 0.10526315789473673, 0.10526315789473673], [0.10526315789473673, 0.10526315789473673, 0.10526315789473673, 0.10526315789473673]]\n",
    "#best cutoffs for f1 as a metric:  [[-0.3157894736842106, -0.3157894736842106, -0.3157894736842106, -0.3157894736842106], [-0.10526315789473695, -0.10526315789473695, -0.10526315789473695, -0.10526315789473695], [-0.10526315789473695, -0.10526315789473695, -0.10526315789473695, -0.10526315789473695]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ridge: for every models test different lambdas and degrees\n",
    "degrees = np.arange(1, 3)\n",
    "lambdas = np.logspace(-5, 0, 2)\n",
    "\n",
    "accuracies_tot = []\n",
    "\n",
    "for idx_subset, (x_train, y_train) in enumerate(clean_data_trains):\n",
    "    print('##### START SUBSET {} #####'.format(idx_subset))\n",
    "    accuracies = np.zeros((len(lambdas), len(degrees)))\n",
    "    for idx_deg, deg in enumerate(degrees):\n",
    "        x_poly = pre.build_poly(x_train, deg)\n",
    "        \n",
    "        for idx_lambda, lambda_ in enumerate(lambdas):\n",
    "            ridge = lambda y, x: lst.ridge_regression(y,x,lambda_)\n",
    "            _, k_accuracies_test = spl.k_fold_cv(y_train, x_poly, 2, ridge)\n",
    "            \n",
    "            # update table\n",
    "            accuracies[idx_lambda][idx_deg] = np.mean(k_accuracies_test)\n",
    "    \n",
    "    accuracies_tot.append(accuracies)\n",
    "    print('##### END SUBSET {} #####'.format(idx_subset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save accuracies\n",
    "import pickle\n",
    "\n",
    "with open('acc_ridge.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump(accuracies_tot, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.subplots(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "fig = sns.heatmap(accuracies_tot[0])\n",
    "fig.set_yticklabels(np.round(lambdas, 5), rotation=60)\n",
    "fig.set_xticklabels(degrees)\n",
    "fig.set_xlabel('degree')\n",
    "fig.set_ylabel('lambda')\n",
    "fig.set_title('Accuracy')\n",
    "plt.subplot(1,3,2)\n",
    "fig = sns.heatmap(accuracies_tot[1])\n",
    "fig.set_xticklabels(degrees)\n",
    "fig.set_xlabel('degree')\n",
    "fig.set_ylabel('lambda')\n",
    "fig.set_title('Accuracy')\n",
    "plt.subplot(1,3,3)\n",
    "fig = sns.heatmap(accuracies_tot[2])\n",
    "fig.set_xticklabels(degrees)\n",
    "fig.set_xlabel('degree')\n",
    "fig.set_ylabel('lambda')\n",
    "fig.set_title('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y: ↓ (lambdas), x: → (degree)\n",
    "for nb, acc in enumerate(accuracies_tot):\n",
    "    print('SUBSET {}'.format(nb))\n",
    "    ymax = np.asscalar(np.where(acc == np.max(acc))[0])\n",
    "    xmax = np.asscalar(np.where(acc == np.max(acc))[1])\n",
    "    \n",
    "    print('Best degree for subset {}: {}'.format(nb, degrees[xmax]))\n",
    "    print('Best lambda for subset {}: {}'.format(nb, lambdas[ymax]))\n",
    "    print('Accuracy: {}'.format(acc[ymax][xmax]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-5, 0, 5)\n",
    "plt.figure(1, figsize=(15, 11))\n",
    "for idx_subset, (x_train, y_train) in enumerate(clean_data_trains):\n",
    "    accuracy_train = np.zeros((len(lambdas)))\n",
    "    accuracy_test = np.zeros((len(lambdas)))\n",
    "    \n",
    "    for idx_lambda, lambda_ in enumerate(lambdas):\n",
    "            ridge = lambda y, x: lst.ridge_regression(y,x,lambda_)\n",
    "            k_accuracies_train, k_accuracies_test = spl.k_fold_cv(y_train, x_train, 4, ridge, hlp.accuracy)\n",
    "            plt.subplot(3,2,idx_lambda+1)\n",
    "            plt.boxplot(k_accuracies_train, positions = [idx_lambda])\n",
    "            plt.boxplot(k_accuracies_test, positions = [idx_lambda])\n",
    "            plt.xlabel(\"lambda\")\n",
    "            plt.ylabel(\"testing rmse\")\n",
    "            plt.title(\"cross validation for categorical subset{i}\".format(i=idx_subset))\n",
    "            # update table\n",
    "            accuracy_train[idx_lambda] = np.mean(k_accuracies_train)\n",
    "            accuracy_test[idx_lambda] = np.mean(k_accuracies_test)\n",
    "    viz.cross_validation_visualization(lambdas, accuracy_train, accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We now interpolate the data thanks to the model defined 2 cells higher..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submit = np.zeros(len(tX_test))\n",
    "assert(len(tX_test) == sum([i[0].shape[0] for i in clean_data_tests]))\n",
    "for (x_test,y_indx),(x_train,y_train),meth in zip(clean_data_tests,clean_data_trains,methods):\n",
    "    w_fin,loss = meth(y_train,x_train)\n",
    "    y_test = x_test @ w_fin\n",
    "    y_test = [-1 if i < 0 else 1.0 for i in y_test]\n",
    "    y_submit[y_indx] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_And finally save the results to csv._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlp.create_csv_submission(ids_test,y_submit,\"anakin.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put your useful trash here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trash random dataframe\n",
    "np.random.seed(2)\n",
    "df = pd.DataFrame(np.random.randint(-1002,-995,size =(3,4)), columns=list('ABCD'))\n",
    "df.replace(to_replace = -999,value = np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Yann\n",
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
