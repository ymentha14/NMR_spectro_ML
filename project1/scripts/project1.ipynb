{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd # cannot use external libraries, just pandas for data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "print(tX.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_met_phi</th>\n",
       "      <th>PRI_met_sumet</th>\n",
       "      <th>PRI_jet_num</th>\n",
       "      <th>PRI_jet_leading_pt</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>s</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>b</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>b</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>b</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>b</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id Prediction  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  \\\n",
       "0  100000          s       138.470                       51.655        97.827   \n",
       "1  100001          b       160.937                       68.768       103.235   \n",
       "2  100002          b      -999.000                      162.172       125.953   \n",
       "3  100003          b       143.905                       81.417        80.943   \n",
       "4  100004          b       175.864                       16.915       134.805   \n",
       "\n",
       "   DER_pt_h  DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0    27.980                  0.91           124.711                2.666   \n",
       "1    48.146               -999.00          -999.000             -999.000   \n",
       "2    35.635               -999.00          -999.000             -999.000   \n",
       "3     0.414               -999.00          -999.000             -999.000   \n",
       "4    16.405               -999.00          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep       ...        PRI_met_phi  PRI_met_sumet  \\\n",
       "0               3.064       ...             -0.277        258.733   \n",
       "1               3.473       ...             -1.916        164.546   \n",
       "2               3.148       ...             -2.186        260.414   \n",
       "3               3.310       ...              0.060         86.062   \n",
       "4               3.891       ...             -0.871         53.131   \n",
       "\n",
       "   PRI_jet_num  PRI_jet_leading_pt  PRI_jet_leading_eta  PRI_jet_leading_phi  \\\n",
       "0            2              67.435                2.150                0.444   \n",
       "1            1              46.226                0.725                1.158   \n",
       "2            1              44.251                2.053               -2.028   \n",
       "3            0            -999.000             -999.000             -999.000   \n",
       "4            0            -999.000             -999.000             -999.000   \n",
       "\n",
       "   PRI_jet_subleading_pt  PRI_jet_subleading_eta  PRI_jet_subleading_phi  \\\n",
       "0                 46.062                    1.24                  -2.475   \n",
       "1               -999.000                 -999.00                -999.000   \n",
       "2               -999.000                 -999.00                -999.000   \n",
       "3               -999.000                 -999.00                -999.000   \n",
       "4               -999.000                 -999.00                -999.000   \n",
       "\n",
       "   PRI_jet_all_pt  \n",
       "0         113.497  \n",
       "1          46.226  \n",
       "2          44.251  \n",
       "3           0.000  \n",
       "4           0.000  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "train = pd.read_csv(DATA_TRAIN_PATH)\n",
    "test = pd.read_csv(DATA_TEST_PATH)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 250000 samples x 30 features\n",
      "Test set size: 568238 samples x 30 features\n"
     ]
    }
   ],
   "source": [
    "print('Train set size: {} samples x {} features'.format(pd.DataFrame(tX).shape[0], pd.DataFrame(tX).shape[1]))\n",
    "print('Test set size: {} samples x {} features'.format(test.shape[0], pd.DataFrame(tX).shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250000 entries, 0 to 249999\n",
      "Data columns (total 32 columns):\n",
      "Id                             250000 non-null int64\n",
      "Prediction                     250000 non-null object\n",
      "DER_mass_MMC                   250000 non-null float64\n",
      "DER_mass_transverse_met_lep    250000 non-null float64\n",
      "DER_mass_vis                   250000 non-null float64\n",
      "DER_pt_h                       250000 non-null float64\n",
      "DER_deltaeta_jet_jet           250000 non-null float64\n",
      "DER_mass_jet_jet               250000 non-null float64\n",
      "DER_prodeta_jet_jet            250000 non-null float64\n",
      "DER_deltar_tau_lep             250000 non-null float64\n",
      "DER_pt_tot                     250000 non-null float64\n",
      "DER_sum_pt                     250000 non-null float64\n",
      "DER_pt_ratio_lep_tau           250000 non-null float64\n",
      "DER_met_phi_centrality         250000 non-null float64\n",
      "DER_lep_eta_centrality         250000 non-null float64\n",
      "PRI_tau_pt                     250000 non-null float64\n",
      "PRI_tau_eta                    250000 non-null float64\n",
      "PRI_tau_phi                    250000 non-null float64\n",
      "PRI_lep_pt                     250000 non-null float64\n",
      "PRI_lep_eta                    250000 non-null float64\n",
      "PRI_lep_phi                    250000 non-null float64\n",
      "PRI_met                        250000 non-null float64\n",
      "PRI_met_phi                    250000 non-null float64\n",
      "PRI_met_sumet                  250000 non-null float64\n",
      "PRI_jet_num                    250000 non-null int64\n",
      "PRI_jet_leading_pt             250000 non-null float64\n",
      "PRI_jet_leading_eta            250000 non-null float64\n",
      "PRI_jet_leading_phi            250000 non-null float64\n",
      "PRI_jet_subleading_pt          250000 non-null float64\n",
      "PRI_jet_subleading_eta         250000 non-null float64\n",
      "PRI_jet_subleading_phi         250000 non-null float64\n",
      "PRI_jet_all_pt                 250000 non-null float64\n",
      "dtypes: float64(29), int64(2), object(1)\n",
      "memory usage: 61.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks clean!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_met_phi</th>\n",
       "      <th>PRI_met_sumet</th>\n",
       "      <th>PRI_jet_num</th>\n",
       "      <th>PRI_jet_leading_pt</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>224999.500000</td>\n",
       "      <td>-49.023079</td>\n",
       "      <td>49.239819</td>\n",
       "      <td>81.181982</td>\n",
       "      <td>57.895962</td>\n",
       "      <td>-708.420675</td>\n",
       "      <td>-601.237051</td>\n",
       "      <td>-709.356603</td>\n",
       "      <td>2.373100</td>\n",
       "      <td>18.917332</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010119</td>\n",
       "      <td>209.797178</td>\n",
       "      <td>0.979176</td>\n",
       "      <td>-348.329567</td>\n",
       "      <td>-399.254314</td>\n",
       "      <td>-399.259788</td>\n",
       "      <td>-692.381204</td>\n",
       "      <td>-709.121609</td>\n",
       "      <td>-709.118631</td>\n",
       "      <td>73.064591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>72168.927986</td>\n",
       "      <td>406.345647</td>\n",
       "      <td>35.344886</td>\n",
       "      <td>40.828691</td>\n",
       "      <td>63.655682</td>\n",
       "      <td>454.480565</td>\n",
       "      <td>657.972302</td>\n",
       "      <td>453.019877</td>\n",
       "      <td>0.782911</td>\n",
       "      <td>22.273494</td>\n",
       "      <td>...</td>\n",
       "      <td>1.812223</td>\n",
       "      <td>126.499506</td>\n",
       "      <td>0.977426</td>\n",
       "      <td>532.962789</td>\n",
       "      <td>489.338286</td>\n",
       "      <td>489.333883</td>\n",
       "      <td>479.875496</td>\n",
       "      <td>453.384624</td>\n",
       "      <td>453.389017</td>\n",
       "      <td>98.015662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.329000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.142000</td>\n",
       "      <td>13.678000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>162499.750000</td>\n",
       "      <td>78.100750</td>\n",
       "      <td>19.241000</td>\n",
       "      <td>59.388750</td>\n",
       "      <td>14.068750</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>1.810000</td>\n",
       "      <td>2.841000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.575000</td>\n",
       "      <td>123.017500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>224999.500000</td>\n",
       "      <td>105.012000</td>\n",
       "      <td>46.524000</td>\n",
       "      <td>73.752000</td>\n",
       "      <td>38.467500</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>2.491500</td>\n",
       "      <td>12.315500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024000</td>\n",
       "      <td>179.739000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>38.960000</td>\n",
       "      <td>-1.872000</td>\n",
       "      <td>-2.093000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>40.512500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>287499.250000</td>\n",
       "      <td>130.606250</td>\n",
       "      <td>73.598000</td>\n",
       "      <td>92.259000</td>\n",
       "      <td>79.169000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>83.446000</td>\n",
       "      <td>-4.593000</td>\n",
       "      <td>2.961000</td>\n",
       "      <td>27.591000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.561000</td>\n",
       "      <td>263.379250</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>75.349000</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>0.503000</td>\n",
       "      <td>33.703000</td>\n",
       "      <td>-2.457000</td>\n",
       "      <td>-2.275000</td>\n",
       "      <td>109.933750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>349999.000000</td>\n",
       "      <td>1192.026000</td>\n",
       "      <td>690.075000</td>\n",
       "      <td>1349.351000</td>\n",
       "      <td>2834.999000</td>\n",
       "      <td>8.503000</td>\n",
       "      <td>4974.979000</td>\n",
       "      <td>16.690000</td>\n",
       "      <td>5.684000</td>\n",
       "      <td>2834.999000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.142000</td>\n",
       "      <td>2003.976000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1120.573000</td>\n",
       "      <td>4.499000</td>\n",
       "      <td>3.141000</td>\n",
       "      <td>721.456000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>3.142000</td>\n",
       "      <td>1633.433000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Id   DER_mass_MMC  DER_mass_transverse_met_lep  \\\n",
       "count  250000.000000  250000.000000                250000.000000   \n",
       "mean   224999.500000     -49.023079                    49.239819   \n",
       "std     72168.927986     406.345647                    35.344886   \n",
       "min    100000.000000    -999.000000                     0.000000   \n",
       "25%    162499.750000      78.100750                    19.241000   \n",
       "50%    224999.500000     105.012000                    46.524000   \n",
       "75%    287499.250000     130.606250                    73.598000   \n",
       "max    349999.000000    1192.026000                   690.075000   \n",
       "\n",
       "        DER_mass_vis       DER_pt_h  DER_deltaeta_jet_jet  DER_mass_jet_jet  \\\n",
       "count  250000.000000  250000.000000         250000.000000     250000.000000   \n",
       "mean       81.181982      57.895962           -708.420675       -601.237051   \n",
       "std        40.828691      63.655682            454.480565        657.972302   \n",
       "min         6.329000       0.000000           -999.000000       -999.000000   \n",
       "25%        59.388750      14.068750           -999.000000       -999.000000   \n",
       "50%        73.752000      38.467500           -999.000000       -999.000000   \n",
       "75%        92.259000      79.169000              0.490000         83.446000   \n",
       "max      1349.351000    2834.999000              8.503000       4974.979000   \n",
       "\n",
       "       DER_prodeta_jet_jet  DER_deltar_tau_lep     DER_pt_tot       ...        \\\n",
       "count        250000.000000       250000.000000  250000.000000       ...         \n",
       "mean           -709.356603            2.373100      18.917332       ...         \n",
       "std             453.019877            0.782911      22.273494       ...         \n",
       "min            -999.000000            0.208000       0.000000       ...         \n",
       "25%            -999.000000            1.810000       2.841000       ...         \n",
       "50%            -999.000000            2.491500      12.315500       ...         \n",
       "75%              -4.593000            2.961000      27.591000       ...         \n",
       "max              16.690000            5.684000    2834.999000       ...         \n",
       "\n",
       "         PRI_met_phi  PRI_met_sumet    PRI_jet_num  PRI_jet_leading_pt  \\\n",
       "count  250000.000000  250000.000000  250000.000000       250000.000000   \n",
       "mean       -0.010119     209.797178       0.979176         -348.329567   \n",
       "std         1.812223     126.499506       0.977426          532.962789   \n",
       "min        -3.142000      13.678000       0.000000         -999.000000   \n",
       "25%        -1.575000     123.017500       0.000000         -999.000000   \n",
       "50%        -0.024000     179.739000       1.000000           38.960000   \n",
       "75%         1.561000     263.379250       2.000000           75.349000   \n",
       "max         3.142000    2003.976000       3.000000         1120.573000   \n",
       "\n",
       "       PRI_jet_leading_eta  PRI_jet_leading_phi  PRI_jet_subleading_pt  \\\n",
       "count        250000.000000        250000.000000          250000.000000   \n",
       "mean           -399.254314          -399.259788            -692.381204   \n",
       "std             489.338286           489.333883             479.875496   \n",
       "min            -999.000000          -999.000000            -999.000000   \n",
       "25%            -999.000000          -999.000000            -999.000000   \n",
       "50%              -1.872000            -2.093000            -999.000000   \n",
       "75%               0.433000             0.503000              33.703000   \n",
       "max               4.499000             3.141000             721.456000   \n",
       "\n",
       "       PRI_jet_subleading_eta  PRI_jet_subleading_phi  PRI_jet_all_pt  \n",
       "count           250000.000000           250000.000000   250000.000000  \n",
       "mean              -709.121609             -709.118631       73.064591  \n",
       "std                453.384624              453.389017       98.015662  \n",
       "min               -999.000000             -999.000000        0.000000  \n",
       "25%               -999.000000             -999.000000        0.000000  \n",
       "50%               -999.000000             -999.000000       40.512500  \n",
       "75%                 -2.457000               -2.275000      109.933750  \n",
       "max                  4.500000                3.142000     1633.433000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.hist(bins=50, figsize=(20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we'redealing with large arrays, we will use the batch iter method in order to avoid memory access problems\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y, tx, w):\n",
    "    '''Compute MSE loss.'''\n",
    "    e = y - tx.dot(w)\n",
    "    mse = (1/2) * np.mean(e**2)\n",
    "    return mse\n",
    "\n",
    "def calculate_mse(e):\n",
    "    \"\"\"Calculate the mse for vector e.\"\"\"\n",
    "    return 1/2*np.mean(e**2)\n",
    "\n",
    "\n",
    "def calculate_mae(e):\n",
    "    \"\"\"Calculate the mae for vector e.\"\"\"\n",
    "    return np.mean(np.abs(e))\n",
    "\n",
    "\n",
    "def compute_loss_ls(y, tx, w):\n",
    "    \"\"\"\n",
    "    Calculate the least-square loss using mse.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return calculate_mse(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_ls(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Linear regression using gradient descent.\"\"\"\n",
    "    w = initial_w\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        # gradient\n",
    "        gradient = compute_gradient_ls(y, tx, w)\n",
    "        \n",
    "        # loss\n",
    "        loss = compute_loss_ls(y, tx, w)\n",
    "        \n",
    "        # update rule\n",
    "        w -= gamma * gradient\n",
    "        \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, maxiters, gamma):\n",
    "    \"\"\"Linear regression using gradient descent (GD).\"\"\"\n",
    "    eps = 10E-5\n",
    "    assert(y.shape[0] == tx.shape[0])\n",
    "    w = initial_w\n",
    "    maxreached = False\n",
    "    N = len(y)\n",
    "    for i in range(0, maxiters+1):\n",
    "        loss = np.linalg.norm(tx @ w - y)**2 / len(y)\n",
    "        if (loss < eps):\n",
    "            break\n",
    "        grad = tx.T @ (tx @ w -  y)/N\n",
    "        w -= gamma * grad\n",
    "    if (i == maxiters):\n",
    "        print('Number of maxiterations reached.')    \n",
    "    return (w,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.64005098 -4.74073768  0.49662478 -0.64677607]\n",
      " [-0.79632198 -1.69665179 -2.95351366  1.19270966]\n",
      " [-2.00345326 -2.33172725  1.21133833  0.29142094]\n",
      " [-3.65420055  0.13578121 -3.15560134  2.85335148]\n",
      " [ 3.53975293 -0.05763163  3.46561485 -4.20354523]\n",
      " [ 0.0524609  -4.34713496 -0.71877672 -4.03469084]\n",
      " [-3.72840028  0.96745309 -2.73987999 -3.93054316]\n",
      " [-2.79693793 -1.50173715 -0.32212515 -2.98256774]\n",
      " [ 1.40406725 -0.16930164  0.0523672  -1.13107349]\n",
      " [ 2.93637454  0.80004179 -3.37701401  2.00752347]] \n",
      "\n",
      "The real coeffs are  [0.96455108 0.50000836 0.88952006 0.34161365] \n",
      "\n",
      "The found coeffs are  [0.966196   0.49574387 0.88923322 0.34396779]  and the error is  9.858645811462567e-05\n"
     ]
    }
   ],
   "source": [
    "#Test for Gradient descent\n",
    "np.random.seed(2)\n",
    "dim = 4\n",
    "test_X = 10*(np.random.rand(10,dim) -0.5)\n",
    "wtrue = np.random.rand(dim)\n",
    "print(test_X,'\\n')\n",
    "print('The real coeffs are ',wtrue,'\\n')\n",
    "test_y = test_X @ wtrue\n",
    "w0 = np.random.rand(dim)\n",
    "wfin = least_squares_GD(test_y,test_X,w0,100,0.03)\n",
    "print('The found coeffs are ',wfin[0], ' and the error is ', wfin[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.966196  , 0.49574387, 0.88923322, 0.34396779]), 9.858645811462567e-05)\n"
     ]
    }
   ],
   "source": [
    "print(wfin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"\n",
    "    Linear regression using SGD.\n",
    "    Compute MSE loss\n",
    "    \"\"\"\n",
    "    w = initial_w\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=1, num_batches=1):\n",
    "            # gradient\n",
    "            gradient = compute_gradient_ls(y_batch, tx_batch, w)\n",
    "           \n",
    "            # update rule\n",
    "            w -= gamma * gradient\n",
    "            \n",
    "            # loss\n",
    "            loss = compute_loss_ls(y_batch, tx_batch, w)\n",
    "            \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"\n",
    "    Least squares regression using normal equations.\n",
    "    Note: better use solve. Problems occur when using np.linalg.inv()\n",
    "    np.linalg.solve(a,b): solve problems of type ax=b\n",
    "    \"\"\"\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    \n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = compute_loss_ls(y, tx, w)\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"Ridge regression using normal equations.\"\"\"\n",
    "    N = tx.shape[0]\n",
    "    D = tx.shape[1]\n",
    "    a = tx.T.dot(tx) + 2*N*lambda_*np.identity(D)\n",
    "    b = tx.T.dot(y)\n",
    "    \n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = compute_loss_ls(y, tx, w)\n",
    "        \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(s):\n",
    "    \"\"\"apply sigmoid function.\"\"\"\n",
    "    return np.exp(s) / (1 + np.exp(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_lr(y, tx, w):\n",
    "    \"\"\"\n",
    "    Compute the loss of the logistic regression\n",
    "    by negative log likelihood.\n",
    "    \"\"\"\n",
    "    prediction = sigmoid(tx.dot(w))\n",
    "    loss = y.T.dot(np.log(prediction)) + (1 - y).T.dot(np.log(1 - prediction))\n",
    "    return np.squeeze(-loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_lr(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss for the logistic regression.\"\"\"\n",
    "    prediction = sigmoid(tx.dot(w))\n",
    "    gradient = tx.T.dot(prediction - y)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GD version\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Logistic regression using gradient descent or SGD.\"\"\"\n",
    "    # Note: we don't have to keep track of the weithts and losses\n",
    "    w = initial_w\n",
    "    \n",
    "    # as we assume that the constant term is contained in x\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), tx]\n",
    "    \n",
    "    # gradient descent\n",
    "    for i in range(max_iters):\n",
    "        # loss\n",
    "        loss = compute_loss_lr(y, tx, w)\n",
    "\n",
    "        # gradient\n",
    "        gradient = compute_gradient_lr(y, tx, w)\n",
    "        \n",
    "        # update rule\n",
    "        w -= gamma * gradient\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD version\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Logistic regression using gradient descent or SGD.\"\"\"   \n",
    "    w = initial_w\n",
    "    \n",
    "    # as we assume that the constant term is contained in x\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), tx]\n",
    "    \n",
    "    # stochastic gradient descent\n",
    "    for i in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=1, num_batches=1): \n",
    "            # loss\n",
    "            loss = compute_loss_lr(y_batch, tx_batch, w)\n",
    "\n",
    "            # gradient\n",
    "            gradient = compute_gradient_lr(y_batch, tx_batch, w)\n",
    "\n",
    "            # update rule\n",
    "            w -= gamma * gradient\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GD version\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    \"\"\"Regularized logistic regression using gradient descent or SGD.\"\"\"\n",
    "    # same as before except that we have the additional penalty term\n",
    "    w = initial_w\n",
    "    \n",
    "    # constant term contained in x\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), tx]\n",
    "    \n",
    "    # gradient descent\n",
    "    for i in range(max_iters):\n",
    "        # loss\n",
    "        loss =  compute_loss_lr + lambda_/2 *  np.squeeze(w.T.dot(w))\n",
    "        \n",
    "        # gradient\n",
    "        gradient =  compute_gradient_lr + lambda_ * w\n",
    "        \n",
    "        # update rule\n",
    "        w -= gamma * gradient\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD version\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    \"\"\"Regularized logistic regression using gradient descent or SGD.\"\"\"  \n",
    "    w = initial_w\n",
    "    \n",
    "    # constant term contained in x\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), tx]\n",
    "    \n",
    "    # gradient descent\n",
    "    for i in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=1, num_batches=1): \n",
    "            # loss\n",
    "            loss = compute_loss_lr(y_batch, tx_batch, w) + lambda_/2 *  np.squeeze(w.T.dot(w))\n",
    "\n",
    "            # gradient\n",
    "            gradient = compute_gradient_lr(y_batch, x_batch, w) + lambda_ * w\n",
    "\n",
    "            # update rule\n",
    "            w -= gamma * gradient\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other useful methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, ratio, seed=12):\n",
    "    \"\"\"Split the X and y sets into a training and a validation/testing one.\"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "\n",
    "    permutations = np.random.permutation(len(X))\n",
    "    x = X[permutations]\n",
    "    y = y[permutations]\n",
    "    \n",
    "    idx = int(np.floor(ratio * len(X)))\n",
    "    x_train = x[:idx]\n",
    "    y_train = y[:idx]\n",
    "    x_test = x[idx:]\n",
    "    y_test = y[idx:]\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    \"\"\"\n",
    "    Compute the accuracy between the real values (y) \n",
    "    and the predicted ones (y_hat).\n",
    "    Note: for binary predictions with values {0, 1} only.\n",
    "    \"\"\"\n",
    "    return np.sum(y == y_hat) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv(y, X, k):\n",
    "    \"\"\"\n",
    "    k-fold cross-validation for model selection.\n",
    "    \n",
    "    y: labels\n",
    "    X: features matrix\n",
    "    k: number of folds\n",
    "    \n",
    "    output: mean accuracy and std across the k-folds\n",
    "    \"\"\"\n",
    "    \n",
    "    # Before partitionning the data we shuffle it randomly\n",
    "    permutations = np.random.permutation(len(y))\n",
    "    X = X[permutations]\n",
    "    y = y[permutations]\n",
    "    \n",
    "    # array to store the outcome accuracy  of the different folds\n",
    "    accuracies = np.zeros(k)\n",
    "    \n",
    "    # Indexes for the k different intervals\n",
    "    idxs = np.linspace(0, len(y), k+1, dtype=int) # +1 for 'upper bound'\n",
    "     \n",
    "    for i in range(nb_folds):\n",
    "        X_te = X[idxs[i]:idxs[i+1]] #test indexes\n",
    "        y_te = y[idxs[i]:idxs[i+1]]\n",
    "        \n",
    "        X_tr = np.delete(X, list(np.arange(idxs[i], idxs[i+1])), axis=0)\n",
    "        y_tr = np.delete(y, list(np.arange(idxs[i], idxs[i+1])), axis=0)\n",
    "        \n",
    "        ## TODO: implement the classifier part \n",
    "        # 1. fit data\n",
    "        # 2. compute y_hat\n",
    "        # 3. compute accuracy\n",
    "    \n",
    "    return np.mean(accuracies), np.std(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ... Mais jpeeeeense alors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Template for plotting graph in 3D later.\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "x = np.linspace(-1,1,10)\n",
    "y = np.linspace(-1,1,10)\n",
    "xp,yp = np.meshgrid(x,y)\n",
    "z = xp**2 + yp**2\n",
    "surf = ax.plot_surface(xp,yp,z)\n",
    "k = np.rand()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
